{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169fbe1e",
   "metadata": {},
   "source": [
    "# Agentic RAG (Retrieval-Augmented Generation) in Colab\n",
    "\n",
    "This notebook demonstrates how to build and experiment with **Agentic RAG** systems ‚Äî  \n",
    "a more advanced form of traditional RAG that combines **retrieval** with **agent-like reasoning and decision-making**.\n",
    "\n",
    "\n",
    "## What is Agentic RAG?\n",
    "- **Traditional RAG**: Enhances LLMs with external knowledge by retrieving documents and feeding them into the model.  \n",
    "- **Agentic RAG**: Goes beyond retrieval by allowing the model to:\n",
    "  - Decide **what to retrieve** and **when to retrieve**.  \n",
    "  - Use **tools and reasoning loops**.  \n",
    "  - Plan multi-step actions to answer complex queries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cd7931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39011eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f7e41",
   "metadata": {},
   "source": [
    "# langchain Tutorial docs retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9045d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_langchain=[\n",
    "    'https://python.langchain.com/docs/tutorials/',\n",
    "    'https://python.langchain.com/docs/tutorials/llm_chain/',\n",
    "    'https://python.langchain.com/docs/tutorials/retrievers/',\n",
    "    'https://python.langchain.com/docs/tutorials/classification/',\n",
    "    'https://python.langchain.com/docs/tutorials/extraction/',\n",
    "    'https://python.langchain.com/docs/tutorials/chatbot/',\n",
    "    'https://python.langchain.com/docs/tutorials/agents/',\n",
    "    'https://python.langchain.com/docs/tutorials/rag/',\n",
    "    'https://python.langchain.com/docs/tutorials/qa_chat_history/',\n",
    "    'https://python.langchain.com/docs/tutorials/sql_qa/',\n",
    "    'https://python.langchain.com/docs/tutorials/summarization/',\n",
    "    'https://python.langchain.com/docs/tutorials/graph/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81164dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_langchain=[WebBaseLoader(url).load() for url in urls_langchain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ed9b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Tutorials | ü¶úÔ∏èüîó LangChain', 'description': 'New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTutorials | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsOn this pageTutorials\\nNew to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\\nGet started\\u200b\\nFamiliarize yourself with LangChain\\'s open-source components by building simple applications.\\nIf you\\'re looking to get started with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our supported integrations.\\n\\nChat models and prompts: Build a simple LLM application with prompt templates and chat models.\\nSemantic search: Build a semantic search engine over a PDF with document loaders, embedding models, and vector stores.\\nClassification: Classify text into categories or labels using chat models with structured outputs.\\nExtraction: Extract structured data from text and other unstructured media using chat models and few-shot examples.\\n\\nRefer to the how-to guides for more detail on using all LangChain components.\\nOrchestration\\u200b\\nGet started using LangGraph to assemble LangChain components into full-featured applications.\\n\\nChatbots: Build a chatbot that incorporates memory.\\nAgents: Build an agent that interacts with external tools.\\nRetrieval Augmented Generation (RAG) Part 1: Build an application that uses your own documents to inform its responses.\\nRetrieval Augmented Generation (RAG) Part 2: Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\\nQuestion-Answering with SQL: Build a question-answering system that executes SQL queries to inform its responses.\\nSummarization: Generate summaries of (potentially long) texts.\\nQuestion-Answering with Graph Databases: Build a question-answering system that queries a graph database to inform its responses.\\n\\nLangSmith\\u200b\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse LangSmith tutorials here.\\nEvaluation\\u200b\\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\\n\\nEvaluate your LLM application\\nEdit this pagePreviousIntroductionNextBuild a Question Answering application over a Graph DatabaseGet startedOrchestrationLangSmithEvaluationCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates\\nIn this quickstart we\\'ll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it\\'s just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you\\'ll have a high level overview of:\\n\\n\\nUsing language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet\\'s dive in!\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"export LANGSMITH_PROJECT=\"default\" # or any other project name\\nOr, if in a notebook, you can set them with:\\nimport getpassimport ostry:    # load environment variables from .env file (requires `python-dotenv`)    from dotenv import load_dotenv    load_dotenv()except ImportError:    passos.environ[\"LANGSMITH_TRACING\"] = \"true\"if \"LANGSMITH_API_KEY\" not in os.environ:    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(        prompt=\"Enter your LangSmith API key (optional): \"    )if \"LANGSMITH_PROJECT\" not in os.environ:    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(        prompt=\\'Enter your LangSmith Project Name (default = \"default\"): \\'    )    if not os.environ.get(\"LANGSMITH_PROJECT\"):        os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\\nUsing Language Models\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.\\n\\n\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(content=\"Translate the following from English into Italian\"),    HumanMessage(content=\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage\\nAIMessage(content=\\'Ciao!\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 3, \\'prompt_tokens\\': 20, \\'total_tokens\\': 23, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-32654a56-627c-40e1-a141-ad9350bbfd3e-0\\', usage_metadata={\\'input_tokens\\': 20, \\'output_tokens\\': 3, \\'total_tokens\\': 23, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\ntipIf we\\'ve enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\\nNote that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:\\n\\nlanguage: The language to translate text into\\ntext: The text to translate\\n\\nfrom langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])\\nWe can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\\nThat\\'s it! In this tutorial you\\'ve learned how to create your first simple LLM application. You\\'ve learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we\\'ve got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we\\'ve got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\\n\\nChat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pagePreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/retrievers/', 'title': 'Build a semantic search engine | ü¶úÔ∏èüîó LangChain', 'description': \"This tutorial will familiarize you with LangChain's document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here).\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a semantic search engine | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a semantic search engineOn this pageBuild a semantic search engine\\nThis tutorial will familiarize you with LangChain\\'s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here).\\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.\\nConcepts\\u200b\\nThis guide focuses on retrieval of text data. We will cover the following concepts:\\n\\nDocuments and document loaders;\\nText splitters;\\nEmbeddings;\\nVector stores and retrievers.\\n\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nThis tutorial requires the langchain-community and pypdf packages:\\n\\nPipCondapip install langchain-community pypdfconda install langchain-community pypdf -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nDocuments and Document Loaders\\u200b\\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\\n\\npage_content: a string representing the content;\\nmetadata: a dict containing arbitrary metadata;\\nid: (optional) a string identifier for the document.\\n\\nThe metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\\nWe can generate sample documents when desired:\\nfrom langchain_core.documents import Documentdocuments = [    Document(        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",        metadata={\"source\": \"mammal-pets-doc\"},    ),    Document(        page_content=\"Cats are independent pets that often enjoy their own space.\",        metadata={\"source\": \"mammal-pets-doc\"},    ),]API Reference:Document\\nHowever, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.\\nLoading documents\\u200b\\nLet\\'s load a PDF into a sequence of Document objects. There is a sample PDF in the LangChain repo here -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders. Let\\'s select PyPDFLoader, which is fairly lightweight.\\nfrom langchain_community.document_loaders import PyPDFLoaderfile_path = \"../example_data/nke-10k-2023.pdf\"loader = PyPDFLoader(file_path)docs = loader.load()print(len(docs))\\n107\\ntipSee this guide for more detail on PDF document loaders.\\nPyPDFLoader loads one Document object per PDF page. For each, we can easily access:\\n\\nThe string content of the page;\\nMetadata containing the file name and page number.\\n\\nprint(f\"{docs[0].page_content[:200]}\\\\n\")print(docs[0].metadata)\\nTable of ContentsUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)‚òë ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934FO{\\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'page\\': 0}\\nSplitting\\u200b\\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\\nwith 200 characters of overlap between chunks. The overlap helps\\nmitigate the possibility of separating a statement from important\\ncontext related to it. We use the\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nWe set add_start_index=True so that the character index where each\\nsplit Document starts within the initial Document is preserved as\\nmetadata attribute ‚Äústart_index‚Äù.\\nSee this guide for more detail about working with PDFs, including how to extract text from specific sections and images.\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000, chunk_overlap=200, add_start_index=True)all_splits = text_splitter.split_documents(docs)len(all_splits)\\n514\\nEmbeddings\\u200b\\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\\nLangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let\\'s select a model:\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nvector_1 = embeddings.embed_query(all_splits[0].page_content)vector_2 = embeddings.embed_query(all_splits[1].page_content)assert len(vector_1) == len(vector_2)print(f\"Generated vectors of length {len(vector_1)}\\\\n\")print(vector_1[:10])\\nGenerated vectors of length 1536[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\\nVector stores\\u200b\\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\\nLangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\\'s select a vector store:\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nHaving instantiated our vector store, we can now index the documents.\\nids = vector_store.add_documents(documents=all_splits)\\nNote that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.\\nOnce we\\'ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\\n\\nSynchronously and asynchronously;\\nBy string query and by vector;\\nWith and without returning similarity scores;\\nBy similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).\\n\\nThe methods will generally include a list of Document objects in their outputs.\\nUsage\\u200b\\nEmbeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\\nReturn documents based on similarity to a string query:\\nresults = vector_store.similarity_search(    \"How many distribution centers does Nike have in the US?\")print(results[0])\\npage_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:U.S. RETAIL STORES NUMBERNIKE Brand factory stores 213 NIKE Brand in-line stores (including employee-only stores) 74 Converse stores (including factory stores) 82 TOTAL 369 In the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.2023 FORM 10-K 2\\' metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}\\nAsync query:\\nresults = await vector_store.asimilarity_search(\"When was Nike incorporated?\")print(results[0])\\npage_content=\\'Table of ContentsPART IITEM 1. BUSINESSGENERALNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE isthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail storesand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\' metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\nReturn scores:\\n# Note that providers implement different scores; the score here# is a distance metric that varies inversely with similarity.results = vector_store.similarity_search_with_score(\"What was Nike\\'s revenue in 2023?\")doc, score = results[0]print(f\"Score: {score}\\\\n\")print(doc)\\nScore: 0.23699893057346344page_content=\\'Table of ContentsFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTSThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:FISCAL 2023 COMPARED TO FISCAL 2022‚Ä¢NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.The increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,2 and 1 percentage points to NIKE, Inc. Revenues, respectively.‚Ä¢NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. Thisincrease was primarily due to higher revenues in Men\\'s, the Jordan Brand, Women\\'s and Kids\\' which grew 17%, 35%,11% and 10%, respectively, on a wholesaleequivalent basis.\\' metadata={\\'page\\': 35, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\nReturn documents based on similarity to an embedded query:\\nembedding = embeddings.embed_query(\"How were Nike\\'s margins impacted in 2023?\")results = vector_store.similarity_search_by_vector(embedding)print(results[0])\\npage_content=\\'Table of ContentsGROSS MARGINFISCAL 2023 COMPARED TO FISCAL 2022For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:*Wholesale equivalentThe decrease in gross margin for fiscal 2023 was primarily due to:‚Ä¢Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well asproduct mix;‚Ä¢Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity inthe prior period resulting from lower available inventory supply;‚Ä¢Unfavorable changes in net foreign currency exchange rates, including hedges; and‚Ä¢Lower off-price margin, on a wholesale equivalent basis.This was partially offset by:\\' metadata={\\'page\\': 36, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\nLearn more:\\n\\nAPI reference\\nHow-to guide\\nIntegration-specific docs\\n\\nRetrievers\\u200b\\nLangChain VectorStore objects do not subclass Runnable. LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\\nWe can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method:\\nfrom typing import Listfrom langchain_core.documents import Documentfrom langchain_core.runnables import chain@chaindef retriever(query: str) -> List[Document]:    return vector_store.similarity_search(query, k=1)retriever.batch(    [        \"How many distribution centers does Nike have in the US?\",        \"When was Nike incorporated?\",    ],)API Reference:Document | chain\\n[[Document(metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}, page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\\\nU.S. RETAIL STORES NUMBER\\\\nNIKE Brand factory stores 213 \\\\nNIKE Brand in-line stores (including employee-only stores) 74 \\\\nConverse stores (including factory stores) 82 \\\\nTOTAL 369 \\\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\\\n2023 FORM 10-K 2\\')], [Document(metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}, page_content=\\'Table of Contents\\\\nPART I\\\\nITEM 1. BUSINESS\\\\nGENERAL\\\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\')]]\\nVectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\\nretriever = vector_store.as_retriever(    search_type=\"similarity\",    search_kwargs={\"k\": 1},)retriever.batch(    [        \"How many distribution centers does Nike have in the US?\",        \"When was Nike incorporated?\",    ],)\\n[[Document(metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}, page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\\\nU.S. RETAIL STORES NUMBER\\\\nNIKE Brand factory stores 213 \\\\nNIKE Brand in-line stores (including employee-only stores) 74 \\\\nConverse stores (including factory stores) 82 \\\\nTOTAL 369 \\\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\\\n2023 FORM 10-K 2\\')], [Document(metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}, page_content=\\'Table of Contents\\\\nPART I\\\\nITEM 1. BUSINESS\\\\nGENERAL\\\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\')]]\\nVectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\". We can use the latter to threshold documents output by the retriever by similarity score.\\nRetrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial.\\nLearn more:\\u200b\\nRetrieval strategies can be rich and complex. For example:\\n\\nWe can infer hard rules and filters from a query (e.g., \"using documents published after 2020\");\\nWe can return documents that are linked to the retrieved context in some way (e.g., via some document taxonomy);\\nWe can generate multiple embeddings for each unit of context;\\nWe can ensemble results from multiple retrievers;\\nWe can assign weights to documents, e.g., to weigh recent documents higher.\\n\\nThe retrievers section of the how-to guides covers these and other built-in retrieval strategies.\\nIt is also straightforward to extend the BaseRetriever class in order to implement custom retrievers. See our how-to guide here.\\nNext steps\\u200b\\nYou\\'ve now seen how to build a semantic search engine over a PDF document.\\nFor more on document loaders:\\n\\nConceptual guide\\nHow-to guides\\nAvailable integrations\\n\\nFor more on embeddings:\\n\\nConceptual guide\\nHow-to guides\\nAvailable integrations\\n\\nFor more on vector stores:\\n\\nConceptual guide\\nHow-to guides\\nAvailable integrations\\n\\nFor more on RAG, see:\\n\\nBuild a Retrieval Augmented Generation (RAG) App\\nRelated how-to guides\\nEdit this pagePreviousBuild a Retrieval Augmented Generation (RAG) App: Part 1NextBuild a Question/Answering system over SQL dataConceptsSetupJupyter NotebookInstallationLangSmithDocuments and Document LoadersLoading documentsSplittingEmbeddingsVector storesUsageRetrieversLearn more:Next stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/classification/', 'title': 'Tagging | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTagging | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsTaggingOn this page\\nClassify Text into Labels\\nTagging means labeling a document with classes such as:\\n\\nSentiment\\nLanguage\\nStyle (formal, informal etc.)\\nCovered topics\\nPolitical tendency\\n\\n\\nOverview\\u200b\\nTagging has a few components:\\n\\nfunction: Like extraction, tagging uses functions to specify how the model should tag a document\\nschema: defines how we want to tag the document\\n\\nQuickstart\\u200b\\nLet\\'s see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We\\'ll use the with_structured_output method supported by OpenAI models.\\npip install -U langchain-core\\nWe\\'ll need to load a chat model:\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s specify a Pydantic model with a few properties and their expected type in our schema.\\nfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldtagging_prompt = ChatPromptTemplate.from_template(    \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the \\'Classification\\' function.Passage:{input}\"\"\")class Classification(BaseModel):    sentiment: str = Field(description=\"The sentiment of the text\")    aggressiveness: int = Field(        description=\"How aggressive the text is on a scale from 1 to 10\"    )    language: str = Field(description=\"The language the text is written in\")# Structured LLMstructured_llm = llm.with_structured_output(Classification)API Reference:ChatPromptTemplate\\ninp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response\\nClassification(sentiment=\\'positive\\', aggressiveness=1, language=\\'Spanish\\')\\nIf we want dictionary output, we can just call .model_dump()\\ninp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response.model_dump()\\n{\\'sentiment\\': \\'angry\\', \\'aggressiveness\\': 8, \\'language\\': \\'Spanish\\'}\\nAs we can see in the examples, it correctly interprets what we want.\\nThe results vary so that we may get, for example, sentiments in different languages (\\'positive\\', \\'enojado\\' etc.).\\nWe will see how to control these results in the next section.\\nFiner control\\u200b\\nCareful schema definition gives us more control over the model\\'s output.\\nSpecifically, we can define:\\n\\nPossible values for each property\\nDescription to make sure that the model understands the property\\nRequired properties to be returned\\n\\nLet\\'s redeclare our Pydantic model to control for each of the previously mentioned aspects using enums:\\nclass Classification(BaseModel):    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])    aggressiveness: int = Field(        ...,        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",        enum=[1, 2, 3, 4, 5],    )    language: str = Field(        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]    )\\ntagging_prompt = ChatPromptTemplate.from_template(    \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the \\'Classification\\' function.Passage:{input}\"\"\")structured_llm = llm.with_structured_output(Classification)\\nNow the answers will be restricted in a way we expect!\\ninp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\\nClassification(sentiment=\\'happy\\', aggressiveness=1, language=\\'spanish\\')\\ninp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\\nClassification(sentiment=\\'sad\\', aggressiveness=4, language=\\'spanish\\')\\ninp = \"Weather is ok here, I can go outside without much more than a coat\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\\nClassification(sentiment=\\'happy\\', aggressiveness=1, language=\\'english\\')\\nThe LangSmith trace lets us peek under the hood:\\n\\nGoing deeper\\u200b\\n\\nYou can use the metadata tagger document transformer to extract metadata from a LangChain Document.\\nThis covers the same basic functionality as the tagging chain, only applied to a LangChain Document.\\nEdit this pagePreviousBuild an AgentNextBuild a Retrieval Augmented Generation (RAG) App: Part 1OverviewQuickstartFiner controlGoing deeperCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/extraction/', 'title': 'Build an Extraction Chain | ü¶úÔ∏èüîó LangChain', 'description': 'In this tutorial, we will use tool-calling features of chat models to extract structured information from unstructured text. We will also demonstrate how to use few-shot prompting in this context to improve performance.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild an Extraction Chain | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild an Extraction ChainOn this pageBuild an Extraction Chain\\nIn this tutorial, we will use tool-calling features of chat models to extract structured information from unstructured text. We will also demonstrate how to use few-shot prompting in this context to improve performance.\\nimportantThis tutorial requires langchain-core>=0.3.20 and will only work with models that support tool calling.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install --upgrade langchain-coreconda install langchain-core -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nThe Schema\\u200b\\nFirst, we need to describe what information we want to extract from the text.\\nWe\\'ll use Pydantic to define an example schema  to extract personal information.\\nfrom typing import Optionalfrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    # ^ Doc-string for the entity Person.    # This doc-string is sent to the LLM as the description of the schema Person,    # and it can help to improve extraction results.    # Note that:    # 1. Each field is an `optional` -- this allows the model to decline to extract it!    # 2. Each field has a `description` -- this description is used by the LLM.    # Having a good description can help improve extraction results.    name: Optional[str] = Field(default=None, description=\"The name of the person\")    hair_color: Optional[str] = Field(        default=None, description=\"The color of the person\\'s hair if known\"    )    height_in_meters: Optional[str] = Field(        default=None, description=\"Height measured in meters\"    )\\nThere are two best practices when defining schema:\\n\\nDocument the attributes and the schema itself: This information is sent to the LLM and is used to improve the quality of information extraction.\\nDo not force the LLM to make up information! Above we used Optional for the attributes allowing the LLM to output None if it doesn\\'t know the answer.\\n\\nimportantFor best performance, document the schema well and make sure the model isn\\'t forced to return results if there\\'s no information to be extracted in the text.\\nThe Extractor\\u200b\\nLet\\'s create an information extractor using the schema we defined above.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder# Define a custom prompt to provide instructions and any additional context.# 1) You can add examples into the prompt template to improve extraction quality# 2) Introduce additional parameters to take context into account (e.g., include metadata#    about the document from which the text was extracted.)prompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are an expert extraction algorithm. \"            \"Only extract relevant information from the text. \"            \"If you do not know the value of an attribute asked to extract, \"            \"return null for the attribute\\'s value.\",        ),        # Please see the how-to about improving performance with        # reference examples.        # MessagesPlaceholder(\\'examples\\'),        (\"human\", \"{text}\"),    ])API Reference:ChatPromptTemplate | MessagesPlaceholder\\nWe need to use a model that supports function/tool calling.\\nPlease review the documentation for all models that can be used with this API.\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nstructured_llm = llm.with_structured_output(schema=Person)\\nLet\\'s test it out:\\ntext = \"Alan Smith is 6 feet tall and has blond hair.\"prompt = prompt_template.invoke({\"text\": text})structured_llm.invoke(prompt)\\nPerson(name=\\'Alan Smith\\', hair_color=\\'blond\\', height_in_meters=\\'1.83\\')\\nimportantExtraction is Generative ü§ØLLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters\\neven though it was provided in feet!\\nWe can see the LangSmith trace here. Note that the chat model portion of the trace reveals the exact sequence of messages sent to the model, tools invoked, and other metadata.\\nMultiple Entities\\u200b\\nIn most cases, you should be extracting a list of entities rather than a single entity.\\nThis can be easily achieved using pydantic by nesting models inside one another.\\nfrom typing import List, Optionalfrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    # ^ Doc-string for the entity Person.    # This doc-string is sent to the LLM as the description of the schema Person,    # and it can help to improve extraction results.    # Note that:    # 1. Each field is an `optional` -- this allows the model to decline to extract it!    # 2. Each field has a `description` -- this description is used by the LLM.    # Having a good description can help improve extraction results.    name: Optional[str] = Field(default=None, description=\"The name of the person\")    hair_color: Optional[str] = Field(        default=None, description=\"The color of the person\\'s hair if known\"    )    height_in_meters: Optional[str] = Field(        default=None, description=\"Height measured in meters\"    )class Data(BaseModel):    \"\"\"Extracted data about people.\"\"\"    # Creates a model so that we can extract multiple entities.    people: List[Person]\\nimportantExtraction results might not be perfect here. Read on to see how to use Reference Examples to improve the quality of extraction, and check out our extraction how-to guides for more detail.\\nstructured_llm = llm.with_structured_output(schema=Data)text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"prompt = prompt_template.invoke({\"text\": text})structured_llm.invoke(prompt)\\nData(people=[Person(name=\\'Jeff\\', hair_color=\\'black\\', height_in_meters=\\'1.83\\'), Person(name=\\'Anna\\', hair_color=\\'black\\', height_in_meters=None)])\\ntipWhen the schema accommodates the extraction of multiple entities, it also allows the model to extract no entities if no relevant information\\nis in the text by providing an empty list.This is usually a good thing! It allows specifying required attributes on an entity without necessarily forcing the model to detect this entity.\\nWe can see the LangSmith trace here.\\nReference examples\\u200b\\nThe behavior of LLM applications can be steered using few-shot prompting. For chat models, this can take the form of a sequence of pairs of input and response messages demonstrating desired behaviors.\\nFor example, we can convey the meaning of a symbol with alternating user and assistant messages:\\nmessages = [    {\"role\": \"user\", \"content\": \"2 ü¶ú 2\"},    {\"role\": \"assistant\", \"content\": \"4\"},    {\"role\": \"user\", \"content\": \"2 ü¶ú 3\"},    {\"role\": \"assistant\", \"content\": \"5\"},    {\"role\": \"user\", \"content\": \"3 ü¶ú 4\"},]response = llm.invoke(messages)print(response.content)\\n7\\nStructured output often uses tool calling under-the-hood. This typically involves the generation of AI messages containing tool calls, as well as tool messages containing the results of tool calls. What should a sequence of messages look like in this case?\\nDifferent chat model providers impose different requirements for valid message sequences. Some will accept a (repeating) message sequence of the form:\\n\\nUser message\\nAI message with tool call\\nTool message with result\\n\\nOthers require a final AI message containing some sort of response.\\nLangChain includes a utility function tool_example_to_messages that will generate a valid sequence for most model providers. It simplifies the generation of structured few-shot examples by just requiring Pydantic representations of the corresponding tool calls.\\nLet\\'s try this out. We can convert pairs of input strings and desired Pydantic objects to a sequence of messages that can be provided to a chat model. Under the hood, LangChain will format the tool calls to each provider\\'s required format.\\nNote: this version of tool_example_to_messages requires langchain-core>=0.3.20.\\nfrom langchain_core.utils.function_calling import tool_example_to_messagesexamples = [    (        \"The ocean is vast and blue. It\\'s more than 20,000 feet deep.\",        Data(people=[]),    ),    (        \"Fiona traveled far from France to Spain.\",        Data(people=[Person(name=\"Fiona\", height_in_meters=None, hair_color=None)]),    ),]messages = []for txt, tool_call in examples:    if tool_call.people:        # This final message is optional for some providers        ai_response = \"Detected people.\"    else:        ai_response = \"Detected no people.\"    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))API Reference:tool_example_to_messages\\nInspecting the result, we see these two example pairs generated eight messages:\\nfor message in messages:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================The ocean is vast and blue. It\\'s more than 20,000 feet deep.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  Data (d8f2e054-7fb9-417f-b28f-0447a775b2c3) Call ID: d8f2e054-7fb9-417f-b28f-0447a775b2c3  Args:    people: []=================================\\x1b[1m Tool Message \\x1b[0m=================================You have correctly called this tool.==================================\\x1b[1m Ai Message \\x1b[0m==================================Detected no people.================================\\x1b[1m Human Message \\x1b[0m=================================Fiona traveled far from France to Spain.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  Data (0178939e-a4b1-4d2a-a93e-b87f665cdfd6) Call ID: 0178939e-a4b1-4d2a-a93e-b87f665cdfd6  Args:    people: [{\\'name\\': \\'Fiona\\', \\'hair_color\\': None, \\'height_in_meters\\': None}]=================================\\x1b[1m Tool Message \\x1b[0m=================================You have correctly called this tool.==================================\\x1b[1m Ai Message \\x1b[0m==================================Detected people.\\nLet\\'s compare performance with and without these messages. For example, let\\'s pass a message for which we intend no people to be extracted:\\nmessage_no_extraction = {    \"role\": \"user\",    \"content\": \"The solar system is large, but earth has only 1 moon.\",}structured_llm = llm.with_structured_output(schema=Data)structured_llm.invoke([message_no_extraction])\\nData(people=[Person(name=\\'Earth\\', hair_color=\\'None\\', height_in_meters=\\'0.00\\')])\\nIn this example, the model is liable to erroneously generate records of people.\\nBecause our few-shot examples contain examples of \"negatives\", we encourage the model to behave correctly in this case:\\nstructured_llm.invoke(messages + [message_no_extraction])\\nData(people=[])\\ntipThe LangSmith trace for the run reveals the exact sequence of messages sent to the chat model, tool calls generated, latency, token counts, and other metadata.\\nSee this guide for more detail on extraction workflows with reference examples, including how to incorporate prompt templates and customize the generation of example messages.\\nNext steps\\u200b\\nNow that you understand the basics of extraction with LangChain, you\\'re ready to proceed to the rest of the how-to guides:\\n\\nAdd Examples: More detail on using reference examples to improve performance.\\nHandle Long Text: What should you do if the text does not fit into the context window of the LLM?\\nUse a Parsing Approach: Use a prompt based approach to extract with models that do not support tool/function calling.\\nEdit this pagePreviousBuild a Retrieval Augmented Generation (RAG) App: Part 2NextBuild an AgentSetupJupyter NotebookInstallationLangSmithThe SchemaThe ExtractorMultiple EntitiesReference examplesNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | ü¶úÔ∏èüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Chatbot | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a ChatbotOn this pageBuild a Chatbot\\nnoteThis tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nOverview\\u200b\\nWe\\'ll go over an example of how to design and implement an LLM-powered chatbot.\\nThis chatbot will be able to have a conversation and remember previous interactions with a chat model.\\nNote that this chatbot that we build will only use the language model to have a conversation.\\nThere are several other related concepts that you may be looking for:\\n\\nConversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nThis tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nFor this tutorial we will need langchain-core and langgraph. This guide requires langgraph >= 0.2.28.\\n\\nPipCondapip install langchain-core langgraph>0.2.27conda install langchain-core langgraph>0.2.27 -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, (you\\'ll need to create an API key from the Settings -> API Keys page on the LangSmith website), make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nQuickstart\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hi! I\\'m Bob\")])API Reference:HumanMessage\\nAIMessage(content=\\'Hi Bob! How can I assist you today?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 10, \\'prompt_tokens\\': 11, \\'total_tokens\\': 21, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 10, \\'total_tokens\\': 21, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nThe model on its own does not have any concept of state. For example, if you ask a followup question:\\nmodel.invoke([HumanMessage(content=\"What\\'s my name?\")])\\nAIMessage(content=\"I\\'m sorry, but I don\\'t have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 34, \\'prompt_tokens\\': 11, \\'total_tokens\\': 45, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-a2d13a18-7022-4784-b54f-f85c097d1075-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 34, \\'total_tokens\\': 45, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nLet\\'s take a look at the example LangSmith trace\\nWe can see that it doesn\\'t take the previous conversation turn into context, and cannot answer the question.\\nThis makes for a terrible chatbot experience!\\nTo get around this, we need to pass the entire conversation history into the model. Let\\'s see what happens when we do that:\\nfrom langchain_core.messages import AIMessagemodel.invoke(    [        HumanMessage(content=\"Hi! I\\'m Bob\"),        AIMessage(content=\"Hello Bob! How can I assist you today?\"),        HumanMessage(content=\"What\\'s my name?\"),    ])API Reference:AIMessage\\nAIMessage(content=\\'Your name is Bob! How can I help you today, Bob?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 14, \\'prompt_tokens\\': 33, \\'total_tokens\\': 47, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-34bcccb3-446e-42f2-b1de-52c09936c02c-0\\', usage_metadata={\\'input_tokens\\': 33, \\'output_tokens\\': 14, \\'total_tokens\\': 47, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nAnd now we can see that we get a good response!\\nThis is the basic idea underpinning a chatbot\\'s ability to interact conversationally.\\nSo how do we best implement this?\\nMessage persistence\\u200b\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\nfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define a new graphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    response = model.invoke(state[\"messages\"])    return {\"messages\": response}# Define the (single) node in the graphworkflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)# Add memorymemory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:MemorySaver | StateGraph\\nWe now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like:\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\\nThis enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\\nWe can then invoke the application:\\nquery = \"Hi! I\\'m Bob.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()  # output contains all messages in state\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Hi Bob! How can I assist you today?\\nquery = \"What\\'s my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob! How can I help you today, Bob?\\nGreat! Our chatbot now remembers things about us. If we change the config to reference a different thread_id, we can see that it starts the conversation fresh.\\nconfig = {\"configurable\": {\"thread_id\": \"abc234\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================I\\'m sorry, but I don\\'t have access to personal information about you unless you\\'ve shared it in this conversation. How can I assist you today?\\nHowever, we can always go back to the original conversation (since we are persisting it in a database)\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob. What would you like to discuss today?\\nThis is how we can support a chatbot having conversations with many users!\\ntipFor async support, update the call_model node to be an async function and use .ainvoke when invoking the application:# Async function for node:async def call_model(state: MessagesState):    response = await model.ainvoke(state[\"messages\"])    return {\"messages\": response}# Define graph as before:workflow = StateGraph(state_schema=MessagesState)workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)app = workflow.compile(checkpointer=MemorySaver())# Async invocation:output = await app.ainvoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\nRight now, all we\\'ve done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.\\nPrompt templates\\u200b\\nPrompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let\\'s now make that a bit more complicated. First, let\\'s add in a system message with some custom instructions (but still taking messages as input). Next, we\\'ll add in more input besides just the messages.\\nTo add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You talk like a pirate. Answer all questions to the best of your ability.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])API Reference:ChatPromptTemplate | MessagesPlaceholder\\nWe can now update our application to incorporate this template:\\nworkflow = StateGraph(state_schema=MessagesState)def call_model(state: MessagesState):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": response}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nWe invoke the application in the same way:\\nconfig = {\"configurable\": {\"thread_id\": \"abc345\"}}query = \"Hi! I\\'m Jim.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ahoy there, Jim! What brings ye to these waters today? Be ye seekin\\' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!\\nAwesome! Let\\'s now make our prompt a little bit more complicated. Let\\'s assume that the prompt template now looks something like this:\\nprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])\\nNote that we have added a new language input to the prompt. Our application now has two parameters-- the input messages and language. We should update our application\\'s state to reflect this:\\nfrom typing import Sequencefrom langchain_core.messages import BaseMessagefrom langgraph.graph.message import add_messagesfrom typing_extensions import Annotated, TypedDictclass State(TypedDict):    messages: Annotated[Sequence[BaseMessage], add_messages]    language: strworkflow = StateGraph(state_schema=State)def call_model(state: State):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:BaseMessage | add_messages\\nconfig = {\"configurable\": {\"thread_id\": \"abc456\"}}query = \"Hi! I\\'m Bob.\"language = \"Spanish\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================¬°Hola, Bob! ¬øC√≥mo puedo ayudarte hoy?\\nNote that the entire state is persisted, so we can omit parameters like language if no changes are desired:\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Tu nombre es Bob. ¬øHay algo m√°s en lo que pueda ayudarte?\\nTo help you understand what\\'s happening internally, check out this LangSmith trace.\\nManaging Conversation History\\u200b\\nOne important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\\nImportantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\\nWe can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the Message History class.\\nLangChain comes with a few built-in helpers for managing a list of messages. In this case we\\'ll use the trim_messages helper to reduce how many messages we\\'re sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:\\nfrom langchain_core.messages import SystemMessage, trim_messagestrimmer = trim_messages(    max_tokens=65,    strategy=\"last\",    token_counter=model,    include_system=True,    allow_partial=False,    start_on=\"human\",)messages = [    SystemMessage(content=\"you\\'re a good assistant\"),    HumanMessage(content=\"hi! I\\'m bob\"),    AIMessage(content=\"hi!\"),    HumanMessage(content=\"I like vanilla ice cream\"),    AIMessage(content=\"nice\"),    HumanMessage(content=\"whats 2 + 2\"),    AIMessage(content=\"4\"),    HumanMessage(content=\"thanks\"),    AIMessage(content=\"no problem!\"),    HumanMessage(content=\"having fun?\"),    AIMessage(content=\"yes!\"),]trimmer.invoke(messages)API Reference:SystemMessage | trim_messages\\n[SystemMessage(content=\"you\\'re a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'whats 2 + 2\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'4\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'thanks\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'no problem!\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'having fun?\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'yes!\\', additional_kwargs={}, response_metadata={})]\\nTo  use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\\nworkflow = StateGraph(state_schema=State)def call_model(state: State):    print(f\"Messages before trimming: {len(state[\\'messages\\'])}\")    trimmed_messages = trimmer.invoke(state[\"messages\"])    print(f\"Messages after trimming: {len(trimmed_messages)}\")    print(\"Remaining messages:\")    for msg in trimmed_messages:        print(f\"  {type(msg).__name__}: {msg.content}\")    prompt = prompt_template.invoke(        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}    )    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nNow if we try asking the model our name, it won\\'t know it since we trimmed that part of the chat history. (By defining our trim stragegy as \\'last\\', we are only keeping the most recent messages that fit within the max_tokens.)\\nconfig = {\"configurable\": {\"thread_id\": \"abc567\"}}query = \"What is my name?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\nMessages before trimming: 12Messages after trimming: 8Remaining messages:  SystemMessage: you\\'re a good assistant  HumanMessage: whats 2 + 2  AIMessage: 4  HumanMessage: thanks  AIMessage: no problem!  HumanMessage: having fun?  AIMessage: yes!  HumanMessage: What is my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================I don\\'t know your name. If you\\'d like to share it, feel free!\\nBut if we ask about information that is within the last few messages, it remembers:\\nconfig = {\"configurable\": {\"thread_id\": \"abc678\"}}query = \"What math problem was asked?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\nMessages before trimming: 12Messages after trimming: 8Remaining messages:  SystemMessage: you\\'re a good assistant  HumanMessage: whats 2 + 2  AIMessage: 4  HumanMessage: thanks  AIMessage: no problem!  HumanMessage: having fun?  AIMessage: yes!  HumanMessage: What math problem was asked?==================================\\x1b[1m Ai Message \\x1b[0m==================================The math problem that was asked was \"what\\'s 2 + 2.\"\\nIf you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace.\\nStreaming\\u200b\\nNow we\\'ve got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\\nIt\\'s actually super easy to do this!\\nBy default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead:\\nconfig = {\"configurable\": {\"thread_id\": \"abc789\"}}query = \"Hi I\\'m Todd, please tell me a joke.\"language = \"English\"input_messages = [HumanMessage(query)]for chunk, metadata in app.stream(    {\"messages\": input_messages, \"language\": language},    config,    stream_mode=\"messages\",):    if isinstance(chunk, AIMessage):  # Filter to just model responses        print(chunk.content, end=\"|\")\\n|Hi| Todd|!| Here|‚Äôs| a| joke| for| you|:|Why| don\\'t| scientists| trust| atoms|?|Because| they| make| up| everything|!||\\nNext Steps\\u200b\\nNow that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:\\n\\nConversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nIf you want to dive deeper on specifics, some things worth checking out are:\\n\\nStreaming: streaming is crucial for chat applications\\nHow to add message history: for a deeper dive into all things related to message history\\nHow to manage large message history: more techniques for managing a large chat history\\nLangGraph main docs: for more detail on building with LangGraph\\nEdit this pagePreviousBuild a simple LLM application with chat models and prompt templatesNextBuild a Retrieval Augmented Generation (RAG) App: Part 2OverviewSetupJupyter NotebookInstallationLangSmithQuickstartMessage persistencePrompt templatesManaging Conversation HistoryStreamingNext StepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/agents/', 'title': 'Build an Agent | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain supports the creation of agents, or systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild an Agent | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild an AgentOn this pageBuild an Agent\\nLangChain supports the creation of agents, or systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action.\\nAfter executing actions, the results can be fed back into the LLM to determine whether more actions are needed, or whether it is okay to finish. This is often achieved via tool-calling.\\nIn this tutorial we will build an agent that can interact with a search engine. You will be able to ask this agent questions, watch it call the search tool, and have conversations with it.\\nEnd-to-end agent\\u200b\\nThe code snippet below represents a fully functional agent that uses an LLM to decide which tools to use. It is equipped with a generic search tool. It has conversational memory - meaning that it can be used as a multi-turn chatbot.\\nIn the rest of the guide, we will walk through the individual components and what each part does - but if you want to just grab some code and get started, feel free to use this!\\n# Import relevant functionalityfrom langchain.chat_models import init_chat_modelfrom langchain_tavily import TavilySearchfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.prebuilt import create_react_agent# Create the agentmemory = MemorySaver()model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")search = TavilySearch(max_results=2)tools = [search]agent_executor = create_react_agent(model, tools, checkpointer=memory)API Reference:MemorySaver | create_react_agent\\n# Use the agentconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_message = {    \"role\": \"user\",    \"content\": \"Hi, I\\'m Bob and I live in SF.\",}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi, I\\'m Bob and I live in SF.==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello Bob! I notice you\\'ve introduced yourself and mentioned you live in SF (San Francisco), but you haven\\'t asked a specific question or made a request that requires the use of any tools. Is there something specific you\\'d like to know about San Francisco or any other topic? I\\'d be happy to help you find information using the available search tools.\\ninput_message = {    \"role\": \"user\",    \"content\": \"What\\'s the weather where I live?\",}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s the weather where I live?==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \\'Let me search for current weather information in San Francisco.\\', \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_011kSdheoJp8THURoLmeLtZo\\', \\'input\\': {\\'query\\': \\'current weather San Francisco CA\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_011kSdheoJp8THURoLmeLtZo) Call ID: toolu_011kSdheoJp8THURoLmeLtZo  Args:    query: current weather San Francisco CA=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco CA\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.944705, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.86441374, \"raw_content\": null}], \"response_time\": 2.34}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9 milesThis is quite typical weather for San Francisco, with the characteristic fog that the city is known for. Would you like to know anything else about the weather or San Francisco in general?\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect interactive environments for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc), and observing these cases is a great way to better understand building with LLMs.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n%pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nTavily\\u200b\\nWe will be using Tavily (a search engine) as a tool.\\nIn order to use it, you will need to get and set an API key:\\nexport TAVILY_API_KEY=\"...\"\\nOr, if in a notebook, you can set it with:\\nimport getpassimport osos.environ[\"TAVILY_API_KEY\"] = getpass.getpass()\\nDefine tools\\u200b\\nWe first need to create the tools we want to use. Our main tool of choice will be Tavily - a search engine. We can use the dedicated langchain-tavily integration package to easily use Tavily search engine as tool with LangChain.\\nfrom langchain_tavily import TavilySearchsearch = TavilySearch(max_results=2)search_results = search.invoke(\"What is the weather in SF\")print(search_results)# If we want, we can create other tools.# Once we have all the tools we want, we can put them in a list that we will reference later.tools = [search]\\n{\\'query\\': \\'What is the weather in SF\\', \\'follow_up_questions\\': None, \\'answer\\': None, \\'images\\': [], \\'results\\': [{\\'title\\': \\'Weather in San Francisco, CA\\', \\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \\'score\\': 0.9185379, \\'raw_content\\': None}, {\\'title\\': \\'Weather in San Francisco in June 2025\\', \\'url\\': \\'https://world-weather.info/forecast/usa/san_francisco/june-2025/\\', \\'content\\': \"Weather in San Francisco in June 2025 (California) - Detailed Weather Forecast for a Month *   Weather in San Francisco Weather in San Francisco in June 2025 *   1 +63¬∞ +55¬∞ *   2 +66¬∞ +54¬∞ *   3 +66¬∞ +55¬∞ *   4 +66¬∞ +54¬∞ *   5 +66¬∞ +55¬∞ *   6 +66¬∞ +57¬∞ *   7 +64¬∞ +55¬∞ *   8 +63¬∞ +55¬∞ *   9 +63¬∞ +54¬∞ *   10 +59¬∞ +54¬∞ *   11 +59¬∞ +54¬∞ *   12 +61¬∞ +54¬∞ Weather in Washington, D.C.**+68¬∞** Sacramento**+81¬∞** Pleasanton**+72¬∞** Redwood City**+68¬∞** San Leandro**+61¬∞** San Mateo**+64¬∞** San Rafael**+70¬∞** San Ramon**+64¬∞** South San Francisco**+61¬∞** Daly City**+59¬∞** Wilder**+66¬∞** Woodacre**+70¬∞** world\\'s temperature today Colchani day+50¬∞F night+16¬∞F Az Zubayr day+124¬∞F night+93¬∞F Weather forecast on your site Install _San Francisco_ +61¬∞ Temperature units\", \\'score\\': 0.7978881, \\'raw_content\\': None}], \\'response_time\\': 2.62}\\ntipIn many applications, you may want to define custom tools. LangChain supports custom\\ntool creation via Python functions and other means. Refer to the\\nHow to create tools guide for details.\\nUsing Language Models\\u200b\\nNext, let\\'s learn how to use a language model to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nYou can call the language model by passing in a list of messages. By default, the response is a content string.\\nquery = \"Hi!\"response = model.invoke([{\"role\": \"user\", \"content\": query}])response.text()\\n\\'Hello! How can I help you today?\\'\\nWe can now see what it is like to enable this model to do tool calling. In order to enable that we use .bind_tools to give the language model knowledge of these tools\\nmodel_with_tools = model.bind_tools(tools)\\nWe can now call the model. Let\\'s first call it with a normal message, and see how it responds. We can look at both the content field as well as the tool_calls field.\\nquery = \"Hi!\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\\\n\")print(f\"Tool calls: {response.tool_calls}\")\\nMessage content: Hello! I\\'m here to help you. I have access to a powerful search tool that can help answer questions and find information about various topics. What would you like to know about?Feel free to ask any question or request information, and I\\'ll do my best to assist you using the available tools.Tool calls: []\\nNow, let\\'s try calling it with some input that would expect a tool to be called.\\nquery = \"Search for the weather in SF\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\\\n\")print(f\"Tool calls: {response.tool_calls}\")\\nMessage content: I\\'ll help you search for information about the weather in San Francisco.Tool calls: [{\\'name\\': \\'tavily_search\\', \\'args\\': {\\'query\\': \\'current weather San Francisco\\'}, \\'id\\': \\'toolu_015gdPn1jbB2Z21DmN2RAnti\\', \\'type\\': \\'tool_call\\'}]\\nWe can see that there\\'s now no text content, but there is a tool call! It wants us to call the Tavily Search tool.\\nThis isn\\'t calling that tool yet - it\\'s just telling us to. In order to actually call it, we\\'ll want to create our agent.\\nCreate the agent\\u200b\\nNow that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent.\\nCurrently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.\\nNow, we can initialize the agent with the LLM and the tools.\\nNote that we are passing in the model, not model_with_tools. That is because create_react_agent will call .bind_tools for us under the hood.\\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(model, tools)API Reference:create_react_agent\\nRun the agent\\u200b\\nWe can now run the agent with a few queries! Note that for now, these are all stateless queries (it won\\'t remember previous interactions). Note that the agent will return the final state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs).\\nFirst up, let\\'s see how it responds when there\\'s no need to call a tool:\\ninput_message = {\"role\": \"user\", \"content\": \"Hi!\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi!==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello! I\\'m here to help you with your questions using the available search tools. Please feel free to ask any question, and I\\'ll do my best to find relevant and accurate information for you.\\nIn order to see exactly what is happening under the hood (and to make sure it\\'s not calling a tool) we can take a look at the LangSmith trace\\nLet\\'s now try it out on an example where it should be invoking the tool\\ninput_message = {\"role\": \"user\", \"content\": \"Search for the weather in SF\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Search for the weather in SF==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \"I\\'ll help you search for weather information in San Francisco. Let me use the search engine to find current weather conditions.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01WWcXGnArosybujpKzdmARZ\\', \\'input\\': {\\'query\\': \\'current weather San Francisco SF\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_01WWcXGnArosybujpKzdmARZ) Call ID: toolu_01WWcXGnArosybujpKzdmARZ  Args:    query: current weather San Francisco SF=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco SF\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.885373, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.8830044, \"raw_content\": null}], \"response_time\": 2.6}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Conditions: Foggy- Wind: 4.0 mph from the SW- Humidity: 86%- Visibility: 9.0 milesThe weather appears to be typical for San Francisco, with morning fog and mild temperatures. The \"feels like\" temperature is 52.4¬∞F (11.3¬∞C).\\nWe can check out the LangSmith trace to make sure it\\'s calling the search tool effectively.\\nStreaming Messages\\u200b\\nWe\\'ve seen how the agent can be called with .invoke to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\\nfor step in agent_executor.stream({\"messages\": [input_message]}, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Search for the weather in SF==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \"I\\'ll help you search for information about the weather in San Francisco.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01DCPnJES53Fcr7YWnZ47kDG\\', \\'input\\': {\\'query\\': \\'current weather San Francisco\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_01DCPnJES53Fcr7YWnZ47kDG) Call ID: toolu_01DCPnJES53Fcr7YWnZ47kDG  Args:    query: current weather San Francisco=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168506, \\'localtime\\': \\'2025-06-17 06:55\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.9542825, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.8638634, \"raw_content\": null}], \"response_time\": 2.57}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9.0 miles- Feels like: 52.4¬∞F (11.3¬∞C)This is quite typical weather for San Francisco, which is known for its fog, especially during the morning hours. The city\\'s proximity to the ocean and unique geographical features often result in mild temperatures and foggy conditions.\\nStreaming tokens\\u200b\\nIn addition to streaming back messages, it is also useful to stream back tokens.\\nWe can do this by specifying stream_mode=\"messages\".\\n::: note\\nBelow we use message.text(), which requires langchain-core>=0.3.37.\\n:::\\nfor step, metadata in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"messages\"):    if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()):        print(text, end=\"|\")\\nI|\\'ll help you search for information| about the weather in San Francisco.|Base|d on the search results, here|\\'s the current weather in| San Francisco:-| Temperature: 53.1¬∞F (|11.7¬∞C)-| Condition: Foggy- Wind:| 4.0 mph from| the Southwest- Humidity|: 86%|- Visibility: 9|.0 miles- Pressure: |30.02 in|HgThe weather| is characteristic of San Francisco, with| foggy conditions and mild temperatures|. The \"feels like\" temperature is slightly| lower at 52.4|¬∞F (11.|3¬∞C)| due to the wind chill effect|.|\\nAdding in memory\\u200b\\nAs mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a thread_id when invoking the agent (so it knows which thread/conversation to resume from).\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()API Reference:MemorySaver\\nagent_executor = create_react_agent(model, tools, checkpointer=memory)config = {\"configurable\": {\"thread_id\": \"abc123\"}}\\ninput_message = {\"role\": \"user\", \"content\": \"Hi, I\\'m Bob!\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi, I\\'m Bob!==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello Bob! I\\'m an AI assistant who can help you search for information using specialized search tools. Is there anything specific you\\'d like to know about or search for? I\\'m happy to help you find accurate and up-to-date information on various topics.\\ninput_message = {\"role\": \"user\", \"content\": \"What\\'s my name?\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob, as you introduced yourself earlier. I can remember information shared within our conversation without needing to search for it.\\nExample LangSmith trace\\nIf you want to start a new conversation, all you have to do is change the thread_id used\\nconfig = {\"configurable\": {\"thread_id\": \"xyz123\"}}input_message = {\"role\": \"user\", \"content\": \"What\\'s my name?\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================I apologize, but I don\\'t have access to any tools that would tell me your name. I can only assist you with searching for publicly available information using the tavily_search function. I don\\'t have access to personal information about users. If you\\'d like to tell me your name, I\\'ll be happy to address you by it.\\nConclusion\\u200b\\nThat\\'s a wrap! In this quick start we covered how to create a simple agent.\\nWe\\'ve then shown how to stream back a response - not only with the intermediate steps, but also tokens!\\nWe\\'ve also added in memory so you can have a conversation with them.\\nAgents are a complex topic with lots to learn!\\nFor more information on Agents, please check out the LangGraph documentation. This has it\\'s own set of concepts, tutorials, and how-to guides.Edit this pagePreviousBuild an Extraction ChainNextTaggingEnd-to-end agentSetupJupyter NotebookInstallationLangSmithTavilyDefine toolsUsing Language ModelsCreate the agentRun the agentStreaming MessagesStreaming tokensAdding in memoryConclusionCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 1On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 1\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis is a multi-part tutorial:\\n\\nPart 1 (this guide) introduces RAG and walks through a minimal implementation.\\nPart 2 extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\\n\\nThis tutorial will show how to build a simple Q&A application\\nover a text data source. Along the way we‚Äôll go over a typical Q&A\\narchitecture and highlight additional resources for more advanced Q&A techniques. We‚Äôll also see\\nhow LangSmith can help us trace and understand our application.\\nLangSmith will become increasingly helpful as our application grows in\\ncomplexity.\\nIf you\\'re already familiar with basic retrieval, you might also be interested in\\nthis high-level overview of different retrieval techniques.\\nNote: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.\\nOverview\\u200b\\nA typical RAG application has two main components:\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\\nRetrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\nNote: the indexing portion of this tutorial will largely follow the semantic search tutorial.\\nThe most common full sequence from raw data to answer looks like:\\nIndexing\\u200b\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\\'t fit in a model\\'s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\nRetrieval and generation\\u200b\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nOnce we\\'ve indexed our data, we will use LangGraph as our orchestration framework to implement the retrieval and generation steps.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nThis tutorial requires these langchain dependencies:\\n\\nPipConda%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraphconda install langchain-text-splitters langchain-community langgraph -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nComponents\\u200b\\nWe will need to select three components from LangChain\\'s suite of integrations.\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nPreview\\u200b\\nIn this guide we‚Äôll build an app that answers questions about the website\\'s content. The specific website we will use is the LLM Powered Autonomous\\nAgents blog post\\nby Lilian Weng, which allows us to ask questions about the contents of\\nthe post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~50\\nlines of code.\\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Index chunks_ = vector_store.add_documents(documents=all_splits)# Define prompt for question-answering# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    context: List[Document]    answer: str# Define application stepsdef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:Document | StateGraph\\nresponse = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(response[\"answer\"])\\nTask Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model\\'s thinking process.\\nCheck out the LangSmith\\ntrace.\\nDetailed walkthrough\\u200b\\nLet‚Äôs go through the above code step-by-step to really understand what‚Äôs\\ngoing on.\\n1. Indexing\\u200b\\nnoteThis section is an abbreviated version of the content in the semantic search tutorial.\\nIf you\\'re comfortable with document loaders, embeddings, and vector stores,\\nfeel free to skip to the next section on retrieval and generation.\\nLoading documents\\u200b\\nWe need to first load the blog post contents. We can use\\nDocumentLoaders\\nfor this, which are objects that load in data from a source and return a\\nlist of\\nDocument\\nobjects.\\nIn this case we‚Äôll use the\\nWebBaseLoader,\\nwhich uses urllib to load HTML from web URLs and BeautifulSoup to\\nparse it to text. We can customize the HTML -> text parsing by passing\\nin parameters into the BeautifulSoup parser via bs_kwargs (see\\nBeautifulSoup\\ndocs).\\nIn this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or\\n‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nimport bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()assert len(docs) == 1print(f\"Total characters: {len(docs[0].page_content)}\")\\nTotal characters: 43131\\nprint(docs[0].page_content[:500])\\n      LLM Powered Autonomous Agents    Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In\\nGo deeper\\u200b\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nDocs:\\nDetailed documentation on how to use DocumentLoaders.\\nIntegrations: 160+\\nintegrations to choose from.\\nInterface:\\nAPI reference for the base interface.\\n\\nSplitting documents\\u200b\\nOur loaded document is over 42k characters which is too long to fit\\ninto the context window of many models. Even for those models that could\\nfit the full post in their context window, models can struggle to find\\ninformation in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and\\nvector storage. This should help us retrieve only the most relevant parts\\nof the blog post at run time.\\nAs in the semantic search tutorial, we use a\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000,  # chunk size (characters)    chunk_overlap=200,  # chunk overlap (characters)    add_start_index=True,  # track index in original document)all_splits = text_splitter.split_documents(docs)print(f\"Split blog post into {len(all_splits)} sub-documents.\")\\nSplit blog post into 66 sub-documents.\\nGo deeper\\u200b\\nTextSplitter: Object that splits a list of Documents into smaller\\nchunks. Subclass of DocumentTransformers.\\n\\nLearn more about splitting text using different methods by reading the how-to docs\\nCode (py or js)\\nScientific papers\\nInterface: API reference for the base interface.\\n\\nDocumentTransformer: Object that performs a transformation on a list\\nof Document objects.\\n\\nDocs: Detailed documentation on how to use DocumentTransformers\\nIntegrations\\nInterface: API reference for the base interface.\\n\\nStoring documents\\u200b\\nNow we need to index our 66 text chunks so that we can search over them\\nat runtime. Following the semantic search tutorial,\\nour approach is to embed the contents of each document split and insert these embeddings\\ninto a vector store. Given an input query, we can then use\\nvector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command\\nusing the vector store and embeddings model selected at the start of the tutorial.\\ndocument_ids = vector_store.add_documents(documents=all_splits)print(document_ids[:3])\\n[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\nGo deeper\\u200b\\nEmbeddings: Wrapper around a text embedding model, used for converting\\ntext to embeddings.\\n\\nDocs: Detailed documentation on how to use embeddings.\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and\\nquerying embeddings.\\n\\nDocs: Detailed documentation on how to use vector stores.\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point\\nwe have a query-able vector store containing the chunked contents of our\\nblog post. Given a user question, we should ideally be able to return\\nthe snippets of the blog post that answer the question.\\n2. Retrieval and Generation\\u200b\\nNow let‚Äôs write the actual application logic. We want to create a simple\\napplication that takes a user question, searches for documents relevant\\nto that question, passes the retrieved documents and initial question to\\na model, and returns an answer.\\nFor generation, we will use the chat model selected at the start of the tutorial.\\nWe‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub\\n(here).\\nfrom langchain import hub# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke(    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}).to_messages()assert len(example_messages) == 1print(example_messages[0].content)\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.Question: (question goes here) Context: (context goes here) Answer:\\nWe\\'ll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\\n\\nWe can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.\\nWe get streamlined deployments via LangGraph Platform.\\nLangSmith will automatically trace the steps of our application together.\\nWe can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes.\\n\\nTo use LangGraph, we need to define three things:\\n\\nThe state of our application;\\nThe nodes of our application (i.e., application steps);\\nThe \"control flow\" of our application (e.g., the ordering of the steps).\\n\\nState:\\u200b\\nThe state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\\nFor a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer:\\nfrom langchain_core.documents import Documentfrom typing_extensions import List, TypedDictclass State(TypedDict):    question: str    context: List[Document]    answer: strAPI Reference:Document\\nNodes (application steps)\\u200b\\nLet\\'s start with a simple sequence of two steps: retrieval and generation.\\ndef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}\\nOur retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.\\nControl flow\\u200b\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the retrieval and generation steps into a single sequence.\\nfrom langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:StateGraph\\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nDo I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:question = \"...\"retrieved_docs = vector_store.similarity_search(question)docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)prompt = prompt.invoke({\"question\": question, \"context\": docs_content})answer = llm.invoke(prompt)The benefits of LangGraph include:\\nSupport for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\\nAutomatic support for tracing via LangSmith and deployments via LangGraph Platform;\\nSupport for persistence, human-in-the-loop, and other features.\\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in Part 2 of the tutorial, LangGraph\\'s management and persistence of state simplifies these applications enormously.\\nUsage\\u200b\\nLet\\'s test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\\nInvoke:\\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f\"Context: {result[\\'context\\']}\\\\n\\\\n\")print(f\"Answer: {result[\\'answer\\']}\")\\nContext: [Document(id=\\'a42dc78b-8f76-472a-9e25-180508af74f3\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}, page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\'), Document(id=\\'c0e45887-d0b0-483d-821a-bb5d8316d51d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}, page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\'), Document(id=\\'4cc7f318-35f5-440f-a4a4-145b5f0b918d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 29630}, page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\'), Document(id=\\'f621ade4-9b0d-471f-a522-44eb5feeba0c\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]Answer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs.\\nStream steps:\\nfor step in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"):    print(f\"{step}\\\\n\\\\n----------------\\\\n\")\\n{\\'retrieve\\': {\\'context\\': [Document(id=\\'a42dc78b-8f76-472a-9e25-180508af74f3\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}, page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\'), Document(id=\\'c0e45887-d0b0-483d-821a-bb5d8316d51d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}, page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\'), Document(id=\\'4cc7f318-35f5-440f-a4a4-145b5f0b918d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 29630}, page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\'), Document(id=\\'f621ade4-9b0d-471f-a522-44eb5feeba0c\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}----------------{\\'generate\\': {\\'answer\\': \\'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.\\'}}----------------\\nStream tokens:\\nfor message, metadata in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"):    print(message.content, end=\"|\")\\n|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|\\'s| reasoning| process|.||\\ntipFor async invocations, use:result = await graph.ainvoke(...)andasync for step in graph.astream(...):\\nReturning sources\\u200b\\nNote that by storing the retrieved context in the state of the graph, we recover sources for the model\\'s generated answer in the \"context\" field of the state. See this guide on returning sources for more detail.\\nGo deeper\\u200b\\nChat models take in a sequence of messages and return a message.\\n\\nDocs\\nIntegrations: 25+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nCustomizing the prompt\\nAs shown above, we can load prompts (e.g., this RAG\\nprompt) from the prompt\\nhub. The prompt can also be easily customized. For example:\\nfrom langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)API Reference:PromptTemplate\\nQuery analysis\\u200b\\nSo far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:\\n\\nIn addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\");\\nThe model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.\\n\\nQuery analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let\\'s add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.\\ntotal_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"all_splits[0].metadata\\n{\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 8, \\'section\\': \\'beginning\\'}\\nWe will need to update the documents in our vector store. We will use a simple InMemoryVectorStore for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store integration documentation for relevant features of your chosen vector store.\\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)API Reference:InMemoryVectorStore\\nLet\\'s next define a schema for our search query. We will use structured output for this purpose. Here we define a query as containing a string query and a document section (either \"beginning\", \"middle\", or \"end\"), but this can be defined however you like.\\nfrom typing import Literalfrom typing_extensions import Annotatedclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]\\nFinally, we add a step to our LangGraph application to generate a query from the user\\'s raw input:\\nclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()\\nFull Code:from typing import Literalimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import Annotated, List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Update metadata (illustration purposes)total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"# Index chunksvector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)# Define schema for searchclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()API Reference:Document | InMemoryVectorStore | StateGraph\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nWe can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.\\nfor step in graph.stream(    {\"question\": \"What does the end of the post say about Task Decomposition?\"},    stream_mode=\"updates\",):    print(f\"{step}\\\\n\\\\n----------------\\\\n\")\\n{\\'analyze_query\\': {\\'query\\': {\\'query\\': \\'Task Decomposition\\', \\'section\\': \\'end\\'}}}----------------{\\'retrieve\\': {\\'context\\': [Document(id=\\'d6cef137-e1e8-4ddc-91dc-b62bd33c6020\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 39221, \\'section\\': \\'end\\'}, page_content=\\'Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\\\n\\\\n\\\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\'), Document(id=\\'d1834ae1-eb6a-43d7-a023-08dfa5028799\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 39086, \\'section\\': \\'end\\'}, page_content=\\'}\\\\n]\\\\nChallenges#\\\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\'), Document(id=\\'ca7f06e4-2c2e-4788-9a81-2418d82213d9\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 32942, \\'section\\': \\'end\\'}, page_content=\\'}\\\\n]\\\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\\\nSystem message:\\'), Document(id=\\'1fcc2736-30f4-4ef6-90f2-c64af92118cb\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 35127, \\'section\\': \\'end\\'}, page_content=\\'\"content\": \"You will get instructions for code to write.\\\\\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\\\\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\\\`\\\\`\\\\`LANG\\\\\\\\nCODE\\\\\\\\n\\\\`\\\\`\\\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\"entrypoint\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease\\')]}}----------------{\\'generate\\': {\\'answer\\': \\'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.\\'}}----------------\\nIn both the streamed steps and the LangSmith trace, we can now observe the structured query that was fed into the retrieval step.\\nQuery Analysis is a rich problem with a wide range of approaches. Refer to the how-to guides for more examples.\\nNext steps\\u200b\\nWe\\'ve covered the steps to build a basic Q&A app over data:\\n\\nLoading data with a Document Loader\\nChunking the indexed data with a Text Splitter to make it more easily usable by a model\\nEmbedding the data and storing the data in a vectorstore\\nRetrieving the previously stored chunks in response to incoming questions\\nGenerating an answer using the retrieved chunks as context.\\n\\nIn Part 2 of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.\\nFurther reading:\\n\\nReturn sources: Learn how to return source documents\\nStreaming: Learn how to stream outputs and intermediate steps\\nAdd chat history: Learn how to add chat history to your app\\nRetrieval conceptual guide: A high-level overview of specific retrieval techniques\\nEdit this pagePreviousTaggingNextBuild a semantic search engineOverviewIndexingRetrieval and generationSetupJupyter NotebookInstallationLangSmithComponentsPreviewDetailed walkthrough1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationQuery analysisNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain', 'description': 'In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 2On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 2\\nIn many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\\nThis is the second part of a multi-part tutorial:\\n\\nPart 1 introduces RAG and walks through a minimal implementation.\\nPart 2 (this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\\n\\nHere we focus on adding logic for incorporating historical messages. This involves the management of a chat history.\\nWe will cover two approaches:\\n\\nChains, in which we execute at most one retrieval step;\\nAgents, in which we give an LLM discretion to execute multiple retrieval steps.\\n\\nnoteThe methods presented here leverage tool-calling capabilities in modern chat models. See this page for a table of models supporting tool calling features.\\nFor the external knowledge source, we will use the same LLM Powered Autonomous Agents blog post by Lilian Weng from the Part 1 of the RAG tutorial.\\nSetup\\u200b\\nComponents\\u200b\\nWe will need to select three components from LangChain\\'s suite of integrations.\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nDependencies\\u200b\\nIn addition, we\\'ll use the following packages:\\n%%capture --no-stderr%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"if not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nChains\\u200b\\nLet\\'s first revisit the vector store we built in Part 1, which indexes an LLM Powered Autonomous Agents blog post by Lilian Weng.\\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)API Reference:Document\\n# Index chunks_ = vector_store.add_documents(documents=all_splits)\\nIn the Part 1 of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of messages. In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via tool messages. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have\\n\\nUser input as a HumanMessage;\\nVector store query as an AIMessage with tool calls;\\nRetrieved documents as a ToolMessage;\\nFinal response as a AIMessage.\\n\\nThis model for state is so versatile that LangGraph offers a built-in version for convenience:\\nfrom langgraph.graph import MessagesState, StateGraphgraph_builder = StateGraph(MessagesState)API Reference:StateGraph\\nLeveraging tool-calling to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:\\n\\nHuman: \"What is Task Decomposition?\"\\nAI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\\nHuman: \"What are common ways of doing it?\"\\n\\nIn this scenario, a model could generate a query such as \"common approaches to task decomposition\". Tool-calling facilitates this naturally. As in the query analysis section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user).\\nLet\\'s turn our retrieval step into a tool:\\nfrom langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def retrieve(query: str):    \"\"\"Retrieve information related to a query.\"\"\"    retrieved_docs = vector_store.similarity_search(query, k=2)    serialized = \"\\\\n\\\\n\".join(        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")        for doc in retrieved_docs    )    return serialized, retrieved_docsAPI Reference:tool\\nSee this guide for more detail on creating tools.\\nOur graph will consist of three nodes:\\n\\nA node that fields the user input, either generating a query for the retriever or responding directly;\\nA node for the retriever tool that executes the retrieval step;\\nA node that generates the final response using the retrieved context.\\n\\nWe build them below. Note that we leverage another pre-built LangGraph component, ToolNode, that executes the tool and adds the result as a ToolMessage to the state.\\nfrom langchain_core.messages import SystemMessagefrom langgraph.prebuilt import ToolNode# Step 1: Generate an AIMessage that may include a tool-call to be sent.def query_or_respond(state: MessagesState):    \"\"\"Generate tool call for retrieval or respond.\"\"\"    llm_with_tools = llm.bind_tools([retrieve])    response = llm_with_tools.invoke(state[\"messages\"])    # MessagesState appends messages to state instead of overwriting    return {\"messages\": [response]}# Step 2: Execute the retrieval.tools = ToolNode([retrieve])# Step 3: Generate a response using the retrieved content.def generate(state: MessagesState):    \"\"\"Generate answer.\"\"\"    # Get generated ToolMessages    recent_tool_messages = []    for message in reversed(state[\"messages\"]):        if message.type == \"tool\":            recent_tool_messages.append(message)        else:            break    tool_messages = recent_tool_messages[::-1]    # Format into prompt    docs_content = \"\\\\n\\\\n\".join(doc.content for doc in tool_messages)    system_message_content = (        \"You are an assistant for question-answering tasks. \"        \"Use the following pieces of retrieved context to answer \"        \"the question. If you don\\'t know the answer, say that you \"        \"don\\'t know. Use three sentences maximum and keep the \"        \"answer concise.\"        \"\\\\n\\\\n\"        f\"{docs_content}\"    )    conversation_messages = [        message        for message in state[\"messages\"]        if message.type in (\"human\", \"system\")        or (message.type == \"ai\" and not message.tool_calls)    ]    prompt = [SystemMessage(system_message_content)] + conversation_messages    # Run    response = llm.invoke(prompt)    return {\"messages\": [response]}API Reference:SystemMessage | ToolNode\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the steps into a sequence. We also allow the first query_or_respond step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\\nfrom langgraph.graph import ENDfrom langgraph.prebuilt import ToolNode, tools_conditiongraph_builder.add_node(query_or_respond)graph_builder.add_node(tools)graph_builder.add_node(generate)graph_builder.set_entry_point(\"query_or_respond\")graph_builder.add_conditional_edges(    \"query_or_respond\",    tools_condition,    {END: END, \"tools\": \"tools\"},)graph_builder.add_edge(\"tools\", \"generate\")graph_builder.add_edge(\"generate\", END)graph = graph_builder.compile()API Reference:ToolNode | tools_condition\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nLet\\'s test our application.\\nNote that it responds appropriately to messages that do not require an additional retrieval step:\\ninput_message = \"Hello\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hello==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello! How can I assist you today?\\nAnd when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:\\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is Task Decomposition?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_dLjB3rkMoxZZxwUGXi33UBeh) Call ID: call_dLjB3rkMoxZZxwUGXi33UBeh  Args:    query: Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\\x1b[1m Ai Message \\x1b[0m==================================Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often involves techniques like Chain of Thought (CoT), which encourages models to think step by step, enhancing performance on complex tasks. This approach allows for a clearer understanding of the task and aids in structuring the problem-solving process.\\nCheck out the LangSmith trace here.\\nStateful management of chat history\\u200b\\nnoteThis section of the tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nIn production, the Q&A application will usually persist the chat history into a database, and be able to read and update it appropriately.\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nTo manage multiple conversational turns and threads, all we have to do is specify a checkpointer when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\nFor a detailed walkthrough of how to manage message history, head to the How to add message history (memory) guide.\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory)# Specify an ID for the threadconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}API Reference:MemorySaver\\nWe can now invoke similar to before:\\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is Task Decomposition?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_JZb6GLD812bW2mQsJ5EJQDnN) Call ID: call_JZb6GLD812bW2mQsJ5EJQDnN  Args:    query: Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\\x1b[1m Ai Message \\x1b[0m==================================Task Decomposition is a technique used to break down complicated tasks into smaller, manageable steps. It involves using methods like Chain of Thought (CoT) prompting, which encourages the model to think step by step, enhancing performance on complex tasks. This process helps to clarify the model\\'s reasoning and makes it easier to tackle difficult problems.\\ninput_message = \"Can you look up some common ways of doing it?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Can you look up some common ways of doing it?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_kjRI4Y5cJOiB73yvd7dmb6ux) Call ID: call_kjRI4Y5cJOiB73yvd7dmb6ux  Args:    query: common methods of task decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================Common ways of performing Task Decomposition include: (1) using Large Language Models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", (2) employing task-specific instructions such as \"Write a story outline\" for specific tasks, and (3) incorporating human inputs to guide the decomposition process.\\nNote that the query generated by the model in the second question incorporates the conversational context.\\nThe LangSmith trace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step.\\nAgents\\u200b\\nAgents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.\\nBelow we assemble a minimal RAG agent. Using LangGraph\\'s pre-built ReAct agent constructor, we can do this in one line.\\ntipCheck out LangGraph\\'s Agentic RAG tutorial for more advanced formulations.\\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)API Reference:create_react_agent\\nLet\\'s inspect the graph:\\ndisplay(Image(agent_executor.get_graph().draw_mermaid_png()))\\n\\nThe key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.\\nLet\\'s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nconfig = {\"configurable\": {\"thread_id\": \"def234\"}}input_message = (    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"    \"Once you get the answer, look up common extensions of that method.\")for event in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    event[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is the standard method for Task Decomposition?Once you get the answer, look up common extensions of that method.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_Y3YaIzL71B83Cjqa8d2G0O8N) Call ID: call_Y3YaIzL71B83Cjqa8d2G0O8N  Args:    query: standard method for Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_2JntP1x4XQMWwgVpYurE12ff) Call ID: call_2JntP1x4XQMWwgVpYurE12ff  Args:    query: common extensions of Task Decomposition methods=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================The standard method for task decomposition involves using techniques such as Chain of Thought (CoT), where a model is instructed to \"think step by step\" to break down complex tasks into smaller, more manageable components. This approach enhances model performance by allowing for more thorough reasoning and planning. Task decomposition can be accomplished through various means, including:1. Simple prompting (e.g., asking for steps to achieve a goal).2. Task-specific instructions (e.g., asking for a story outline).3. Human inputs to guide the decomposition process.### Common Extensions of Task Decomposition Methods:1. **Tree of Thoughts**: This extension builds on CoT by not only decomposing the problem into thought steps but also generating multiple thoughts at each step, creating a tree structure. The search process can employ breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.These extensions aim to enhance reasoning capabilities and improve the effectiveness of task decomposition in various contexts.\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nNext steps\\u200b\\nWe\\'ve covered the steps to build a basic conversational Q&A application:\\n\\nWe used chains to build a predictable application that generates at most one query per user input;\\nWe used agents to build an application that can iterate on a sequence of queries.\\n\\nTo explore different types of retrievers and retrieval strategies, visit the retrievers section of the how-to guides.\\nFor a detailed walkthrough of LangChain\\'s conversation memory abstractions, visit the How to add message history (memory) guide.\\nTo learn more about agents, check out the conceptual guide and LangGraph agent architectures page.Edit this pagePreviousBuild a ChatbotNextBuild an Extraction ChainSetupComponentsDependenciesLangSmithChainsStateful management of chat historyAgentsNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/sql_qa/', 'title': 'Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Question/Answering system over SQL dataOn this pageBuild a Question/Answering system over SQL data\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nTools\\nAgents\\nLangGraph\\n\\nEnabling a LLM system to query structured data can be qualitatively different from unstructured text data. Whereas in the latter it is common to generate text that can be searched against a vector database, the approach for structured data is often for the LLM to write and execute queries in a DSL, such as SQL. In this guide we\\'ll go over the basic ways to create a Q&A system over tabular data in databases. We will cover implementations using both chains and agents. These systems will allow us to ask a question about the data in a database and get back a natural language answer. The main difference between the two is that our agent can query the database in a loop as many times as it needs to answer the question.\\n‚ö†Ô∏è Security note ‚ö†Ô∏è\\u200b\\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent\\'s needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see here.\\nArchitecture\\u200b\\nAt a high-level, the steps of these systems are:\\n\\nConvert question to SQL query: Model converts user input to a SQL query.\\nExecute SQL query: Execute the query.\\nAnswer the question: Model responds to user input using the query results.\\n\\nNote that querying data in CSVs can follow a similar approach. See our how-to guide on question-answering over CSV data for more detail.\\n\\nSetup\\u200b\\nFirst, get required packages and set environment variables:\\n%%capture --no-stderr%pip install --upgrade --quiet langchain-community langgraph\\n# Comment out the below to opt-out of using LangSmith in this notebook. Not required.if not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()    os.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nSample data\\u200b\\nThe below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow these installation steps to create Chinook.db in the same directory as this notebook. You can also download and build the database via the command line:\\ncurl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db\\nNow, Chinook.db is in our directory and we can interface with it using the SQLAlchemy-driven SQLDatabase class:\\nfrom langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")print(db.dialect)print(db.get_usable_table_names())db.run(\"SELECT * FROM Artist LIMIT 10;\")\\nsqlite[\\'Album\\', \\'Artist\\', \\'Customer\\', \\'Employee\\', \\'Genre\\', \\'Invoice\\', \\'InvoiceLine\\', \\'MediaType\\', \\'Playlist\\', \\'PlaylistTrack\\', \\'Track\\']\\n\"[(1, \\'AC/DC\\'), (2, \\'Accept\\'), (3, \\'Aerosmith\\'), (4, \\'Alanis Morissette\\'), (5, \\'Alice In Chains\\'), (6, \\'Ant√¥nio Carlos Jobim\\'), (7, \\'Apocalyptica\\'), (8, \\'Audioslave\\'), (9, \\'BackBeat\\'), (10, \\'Billy Cobham\\')]\"\\nGreat! We\\'ve got a SQL database that we can query. Now let\\'s try hooking it up to an LLM.\\nChains\\u200b\\nChains are compositions of predictable steps. In LangGraph, we can represent a chain via simple sequence of nodes. Let\\'s create a sequence of steps that, given a question, does the following:\\n\\nconverts the question into a SQL query;\\nexecutes the query;\\nuses the result to answer the original question.\\n\\nThere are scenarios not supported by this arrangement. For example, this system will execute a SQL query for any user input-- even \"hello\". Importantly, as we\\'ll see below, some questions require more than one query to answer. We will address these scenarios in the Agents section.\\nApplication state\\u200b\\nThe LangGraph state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\\nFor this application, we can just keep track of the input question, generated query, query result, and generated answer:\\nfrom typing_extensions import TypedDictclass State(TypedDict):    question: str    query: str    result: str    answer: str\\nNow we just need functions that operate on this state and populate its contents.\\nConvert question to SQL query\\u200b\\nThe first step is to take the user input and convert it to a SQL query. To reliably obtain SQL queries (absent markdown formatting and explanations or clarifications), we will make use of LangChain\\'s structured output abstraction.\\nLet\\'s select a chat model for our application:\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s provide some instructions for our model:\\nfrom langchain_core.prompts import ChatPromptTemplatesystem_message = \"\"\"Given an input question, create a syntactically correct {dialect} query torun to help find the answer. Unless the user specifies in his question aspecific number of examples they wish to obtain, always limit your query toat most {top_k} results. You can order the results by a relevant column toreturn the most interesting examples in the database.Never query for all the columns from a specific table, only ask for a thefew relevant columns given the question.Pay attention to use only the column names that you can see in the schemadescription. Be careful to not query for columns that do not exist. Also,pay attention to which column is in which table.Only use the following tables:{table_info}\"\"\"user_prompt = \"Question: {input}\"query_prompt_template = ChatPromptTemplate(    [(\"system\", system_message), (\"user\", user_prompt)])for message in query_prompt_template.messages:    message.pretty_print()API Reference:ChatPromptTemplate\\n================================\\x1b[1m System Message \\x1b[0m================================Given an input question, create a syntactically correct \\x1b[33;1m\\x1b[1;3m{dialect}\\x1b[0m query torun to help find the answer. Unless the user specifies in his question aspecific number of examples they wish to obtain, always limit your query toat most \\x1b[33;1m\\x1b[1;3m{top_k}\\x1b[0m results. You can order the results by a relevant column toreturn the most interesting examples in the database.Never query for all the columns from a specific table, only ask for a thefew relevant columns given the question.Pay attention to use only the column names that you can see in the schemadescription. Be careful to not query for columns that do not exist. Also,pay attention to which column is in which table.Only use the following tables:\\x1b[33;1m\\x1b[1;3m{table_info}\\x1b[0m================================\\x1b[1m Human Message \\x1b[0m=================================Question: \\x1b[33;1m\\x1b[1;3m{input}\\x1b[0m\\nThe prompt includes several parameters we will need to populate, such as the SQL dialect and table schemas. LangChain\\'s SQLDatabase object includes methods to help with this. Our write_query step will just populate these parameters and prompt a model to generate the SQL query:\\nfrom typing_extensions import Annotatedclass QueryOutput(TypedDict):    \"\"\"Generated SQL query.\"\"\"    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]def write_query(state: State):    \"\"\"Generate SQL query to fetch information.\"\"\"    prompt = query_prompt_template.invoke(        {            \"dialect\": db.dialect,            \"top_k\": 10,            \"table_info\": db.get_table_info(),            \"input\": state[\"question\"],        }    )    structured_llm = llm.with_structured_output(QueryOutput)    result = structured_llm.invoke(prompt)    return {\"query\": result[\"query\"]}\\nLet\\'s test it out:\\nwrite_query({\"question\": \"How many Employees are there?\"})\\n{\\'query\\': \\'SELECT COUNT(*) as employee_count FROM Employee;\\'}\\nExecute query\\u200b\\nThis is the most dangerous part of creating a SQL chain. Consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution (see below).\\nTo execute the query, we will load a tool from langchain-community. Our execute_query node will just wrap this tool:\\nfrom langchain_community.tools.sql_database.tool import QuerySQLDatabaseTooldef execute_query(state: State):    \"\"\"Execute SQL query.\"\"\"    execute_query_tool = QuerySQLDatabaseTool(db=db)    return {\"result\": execute_query_tool.invoke(state[\"query\"])}\\nTesting this step:\\nexecute_query({\"query\": \"SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;\"})\\n{\\'result\\': \\'[(8,)]\\'}\\nGenerate answer\\u200b\\nFinally, our last step generates an answer to the question given the information pulled from the database:\\ndef generate_answer(state: State):    \"\"\"Answer question using retrieved information as context.\"\"\"    prompt = (        \"Given the following user question, corresponding SQL query, \"        \"and SQL result, answer the user question.\\\\n\\\\n\"        f\"Question: {state[\\'question\\']}\\\\n\"        f\"SQL Query: {state[\\'query\\']}\\\\n\"        f\"SQL Result: {state[\\'result\\']}\"    )    response = llm.invoke(prompt)    return {\"answer\": response.content}\\nOrchestrating with LangGraph\\u200b\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the three steps into a single sequence.\\nfrom langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence(    [write_query, execute_query, generate_answer])graph_builder.add_edge(START, \"write_query\")graph = graph_builder.compile()API Reference:StateGraph\\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nLet\\'s test our application! Note that we can stream the results of individual steps:\\nfor step in graph.stream(    {\"question\": \"How many employees are there?\"}, stream_mode=\"updates\"):    print(step)\\n{\\'write_query\\': {\\'query\\': \\'SELECT COUNT(*) as employee_count FROM Employee;\\'}}{\\'execute_query\\': {\\'result\\': \\'[(8,)]\\'}}{\\'generate_answer\\': {\\'answer\\': \\'There are 8 employees in total.\\'}}\\nCheck out the LangSmith trace.\\nHuman-in-the-loop\\u200b\\nLangGraph supports a number of features that can be useful for this workflow. One of them is human-in-the-loop: we can interrupt our application before sensitive steps (such as the execution of a SQL query) for human review. This is enabled by LangGraph\\'s persistence layer, which saves run progress to your storage of choice. Below, we specify storage in-memory:\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory, interrupt_before=[\"execute_query\"])# Now that we\\'re using persistence, we need to specify a thread ID# so that we can continue the run after review.config = {\"configurable\": {\"thread_id\": \"1\"}}API Reference:MemorySaver\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nLet\\'s repeat the same run, adding in a simple yes/no approval step:\\nfor step in graph.stream(    {\"question\": \"How many employees are there?\"},    config,    stream_mode=\"updates\",):    print(step)try:    user_approval = input(\"Do you want to go to execute query? (yes/no): \")except Exception:    user_approval = \"no\"if user_approval.lower() == \"yes\":    # If approved, continue the graph execution    for step in graph.stream(None, config, stream_mode=\"updates\"):        print(step)else:    print(\"Operation cancelled by user.\")\\n{\\'write_query\\': {\\'query\\': \\'SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;\\'}}{\\'__interrupt__\\': ()}``````outputDo you want to go to execute query? (yes/no):  yes``````output{\\'execute_query\\': {\\'result\\': \\'[(8,)]\\'}}{\\'generate_answer\\': {\\'answer\\': \\'There are 8 employees.\\'}}\\nSee this LangGraph guide for more detail and examples.\\nNext steps\\u200b\\nFor more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:\\n\\nPrompting strategies: Advanced prompt engineering techniques.\\nQuery checking: Add query validation and error handling.\\nLarge databases: Techniques for working with large databases.\\n\\nAgents\\u200b\\nAgents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the query generation and execution process. Although their behavior is less predictable than the above \"chain\", they feature some advantages:\\n\\nThey can query the database as many times as needed to answer the user question.\\nThey can recover from errors by running a generated query, catching the traceback and regenerating it correctly.\\nThey can answer questions based on the databases\\' schema as well as on the databases\\' content (like describing a specific table).\\n\\nBelow we assemble a minimal SQL agent. We will equip it with a set of tools using LangChain\\'s SQLDatabaseToolkit. Using LangGraph\\'s pre-built ReAct agent constructor, we can do this in one line.\\ntipCheck out LangGraph\\'s SQL Agent Tutorial for a more advanced formulation of a SQL agent.\\nThe SQLDatabaseToolkit includes tools that can:\\n\\nCreate and execute queries\\nCheck query syntax\\nRetrieve table descriptions\\n... and more\\n\\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkittoolkit = SQLDatabaseToolkit(db=db, llm=llm)tools = toolkit.get_tools()tools\\n[QuerySQLDatabaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column \\'xxxx\\' in \\'field list\\', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), InfoSQLDatabaseTool(description=\\'Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\\', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), QuerySQLCheckerTool(description=\\'Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\\', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name=\\'gpt-4o\\', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=[\\'dialect\\', \\'query\\'], input_types={}, partial_variables={}, template=\\'\\\\n{query}\\\\nDouble check the {dialect} query above for common mistakes, including:\\\\n- Using NOT IN with NULL values\\\\n- Using UNION when UNION ALL should have been used\\\\n- Using BETWEEN for exclusive ranges\\\\n- Data type mismatch in predicates\\\\n- Properly quoting identifiers\\\\n- Using the correct number of arguments for functions\\\\n- Casting to the correct data type\\\\n- Using the proper columns for joins\\\\n\\\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\\\n\\\\nOutput the final SQL query only.\\\\n\\\\nSQL Query: \\'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name=\\'gpt-4o\\', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')), output_parser=StrOutputParser(), llm_kwargs={}))]\\nSystem Prompt\\u200b\\nWe will also want to load a system prompt for our agent. This will consist of instructions for how to behave. Note that the prompt below has several parameters, which we assign below.\\nsystem_message = \"\"\"You are an agent designed to interact with a SQL database.Given an input question, create a syntactically correct {dialect} query to run,then look at the results of the query and return the answer. Unless the userspecifies a specific number of examples they wish to obtain, always limit yourquery to at most {top_k} results.You can order the results by a relevant column to return the most interestingexamples in the database. Never query for all the columns from a specific table,only ask for the relevant columns given the question.You MUST double check your query before executing it. If you get an error whileexecuting a query, rewrite the query and try again.DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to thedatabase.To start you should ALWAYS look at the tables in the database to see what youcan query. Do NOT skip this step.Then you should query the schema of the most relevant tables.\"\"\".format(    dialect=\"SQLite\",    top_k=5,)\\nInitializing agent\\u200b\\nWe will use a prebuilt LangGraph agent to build our agent\\nfrom langchain_core.messages import HumanMessagefrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, tools, prompt=system_message)API Reference:HumanMessage | create_react_agent\\nConsider how the agent responds to the below question:\\nquestion = \"Which country\\'s customers spent the most?\"for step in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Which country\\'s customers spent the most?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_list_tables (call_tFp7HYD6sAAmCShgeqkVZH6Q) Call ID: call_tFp7HYD6sAAmCShgeqkVZH6Q  Args:=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_list_tablesAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_schema (call_KJZ1Jx6JazyDdJa0uH1UeiOz) Call ID: call_KJZ1Jx6JazyDdJa0uH1UeiOz  Args:    table_names: Customer, Invoice=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_schemaCREATE TABLE \"Customer\" (\\t\"CustomerId\" INTEGER NOT NULL, \\t\"FirstName\" NVARCHAR(40) NOT NULL, \\t\"LastName\" NVARCHAR(20) NOT NULL, \\t\"Company\" NVARCHAR(80), \\t\"Address\" NVARCHAR(70), \\t\"City\" NVARCHAR(40), \\t\"State\" NVARCHAR(40), \\t\"Country\" NVARCHAR(40), \\t\"PostalCode\" NVARCHAR(10), \\t\"Phone\" NVARCHAR(24), \\t\"Fax\" NVARCHAR(24), \\t\"Email\" NVARCHAR(60) NOT NULL, \\t\"SupportRepId\" INTEGER, \\tPRIMARY KEY (\"CustomerId\"), \\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\"))/*3 rows from Customer table:CustomerId\\tFirstName\\tLastName\\tCompany\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\tSupportRepId1\\tLu√≠s\\tGon√ßalves\\tEmbraer - Empresa Brasileira de Aeron√°utica S.A.\\tAv. Brigadeiro Faria Lima, 2170\\tS√£o Jos√© dos Campos\\tSP\\tBrazil\\t12227-000\\t+55 (12) 3923-5555\\t+55 (12) 3923-5566\\tluisg@embraer.com.br\\t32\\tLeonie\\tK√∂hler\\tNone\\tTheodor-Heuss-Stra√üe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t+49 0711 2842222\\tNone\\tleonekohler@surfeu.de\\t53\\tFran√ßois\\tTremblay\\tNone\\t1498 rue B√©langer\\tMontr√©al\\tQC\\tCanada\\tH2G 1A7\\t+1 (514) 721-4711\\tNone\\tftremblay@gmail.com\\t3*/CREATE TABLE \"Invoice\" (\\t\"InvoiceId\" INTEGER NOT NULL, \\t\"CustomerId\" INTEGER NOT NULL, \\t\"InvoiceDate\" DATETIME NOT NULL, \\t\"BillingAddress\" NVARCHAR(70), \\t\"BillingCity\" NVARCHAR(40), \\t\"BillingState\" NVARCHAR(40), \\t\"BillingCountry\" NVARCHAR(40), \\t\"BillingPostalCode\" NVARCHAR(10), \\t\"Total\" NUMERIC(10, 2) NOT NULL, \\tPRIMARY KEY (\"InvoiceId\"), \\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\"))/*3 rows from Invoice table:InvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal1\\t2\\t2021-01-01 00:00:00\\tTheodor-Heuss-Stra√üe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.982\\t4\\t2021-01-02 00:00:00\\tUllev√•lsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.963\\t8\\t2021-01-03 00:00:00\\tGr√©trystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94*/==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_query_checker (call_AQuTGbgH63u4gPgyV723yrjX) Call ID: call_AQuTGbgH63u4gPgyV723yrjX  Args:    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_query_checker\\\\`\\\\`\\\\`sqlSELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;\\\\`\\\\`\\\\`==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_query (call_B88EwU44nwwpQL5M9nlcemSU) Call ID: call_B88EwU44nwwpQL5M9nlcemSU  Args:    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_query[(\\'USA\\', 523.06)]==================================\\x1b[1m Ai Message \\x1b[0m==================================The country whose customers spent the most is the USA, with a total spending of 523.06.\\nYou can also use the LangSmith trace to visualize these steps and associated metadata.\\nNote that the agent executes multiple queries until it has the information it needs:\\n\\nList available tables;\\nRetrieves the schema for three tables;\\nQueries multiple of the tables via a join operation.\\n\\nThe agent is then able to use the result of the final query to generate an answer to the original question.\\nThe agent can similarly handle qualitative questions:\\nquestion = \"Describe the playlisttrack table\"for step in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Describe the playlisttrack table==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_list_tables (call_fMF8eTmX5TJDJjc3Mhdg52TI) Call ID: call_fMF8eTmX5TJDJjc3Mhdg52TI  Args:=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_list_tablesAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_schema (call_W8Vkk4NEodkAAIg8nexAszUH) Call ID: call_W8Vkk4NEodkAAIg8nexAszUH  Args:    table_names: PlaylistTrack=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_schemaCREATE TABLE \"PlaylistTrack\" (\\t\"PlaylistId\" INTEGER NOT NULL, \\t\"TrackId\" INTEGER NOT NULL, \\tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\"))/*3 rows from PlaylistTrack table:PlaylistId\\tTrackId1\\t34021\\t33891\\t3390*/==================================\\x1b[1m Ai Message \\x1b[0m==================================The `PlaylistTrack` table is designed to associate tracks with playlists. It has the following structure:- **PlaylistId**: An integer that serves as a foreign key referencing the `Playlist` table. It is part of the composite primary key.- **TrackId**: An integer that serves as a foreign key referencing the `Track` table. It is also part of the composite primary key.The primary key for this table is a composite key consisting of both `PlaylistId` and `TrackId`, ensuring that each track can be uniquely associated with a playlist. The table enforces referential integrity by linking to the `Track` and `Playlist` tables through foreign keys.\\nDealing with high-cardinality columns\\u200b\\nIn order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.\\nWe can achieve this by creating a vector store with all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.\\nFirst we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:\\nimport astimport redef query_as_list(db, query):    res = db.run(query)    res = [el for sub in ast.literal_eval(res) for el in sub if el]    res = [re.sub(r\"\\\\b\\\\d+\\\\b\", \"\", string).strip() for string in res]    return list(set(res))artists = query_as_list(db, \"SELECT Name FROM Artist\")albums = query_as_list(db, \"SELECT Title FROM Album\")albums[:5]\\n[\\'In Through The Out Door\\', \\'Transmission\\', \\'Battlestar Galactica (Classic), Season\\', \\'A Copland Celebration, Vol. I\\', \\'Quiet Songs\\']\\nUsing this function, we can create a retriever tool that the agent can execute at its discretion.\\nLet\\'s select an embeddings model and vector store for this step:\\nSelect an embedding model:\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nSelect a vector store:\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nWe can now construct a retrieval tool that can search over relevant proper nouns in the database:\\nfrom langchain.agents.agent_toolkits import create_retriever_tool_ = vector_store.add_texts(artists + albums)retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})description = (    \"Use to look up values to filter on. Input is an approximate spelling \"    \"of the proper noun, output is valid proper nouns. Use the noun most \"    \"similar to the search.\")retriever_tool = create_retriever_tool(    retriever,    name=\"search_proper_nouns\",    description=description,)\\nLet\\'s try it out:\\nprint(retriever_tool.invoke(\"Alice Chains\"))\\nAlice In ChainsAlanis MorissettePearl JamPearl JamAudioslave\\nThis way, if the agent determines it needs to write a filter based on an artist along the lines of \"Alice Chains\", it can first use the retriever tool to observe relevant values of a column.\\nPutting this together:\\n# Add to system messagesuffix = (    \"If you need to filter on a proper noun like a Name, you must ALWAYS first look up \"    \"the filter value using the \\'search_proper_nouns\\' tool! Do not try to \"    \"guess at the proper name - use this function to find similar ones.\")system = f\"{system_message}\\\\n\\\\n{suffix}\"tools.append(retriever_tool)agent = create_react_agent(llm, tools, prompt=system)\\nquestion = \"How many albums does alis in chain have?\"for step in agent.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================How many albums does alis in chain have?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  search_proper_nouns (call_8ryjsRPLAr79mM3Qvnq6gTOH) Call ID: call_8ryjsRPLAr79mM3Qvnq6gTOH  Args:    query: alis in chain=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: search_proper_nounsAlice In ChainsAisha DuoXisDa Lama Ao CaosA-Sides==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_list_tables (call_NJjtCpU89MBMplssjn1z0xzq) Call ID: call_NJjtCpU89MBMplssjn1z0xzq  Args:  search_proper_nouns (call_1BfrueC9koSIyi4OfMu2Ao8q) Call ID: call_1BfrueC9koSIyi4OfMu2Ao8q  Args:    query: Alice In Chains=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: search_proper_nounsAlice In ChainsPearl JamPearl JamFoo FightersSoundgarden==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_schema (call_Kn09w9jd9swcNzIZ1b5MlKID) Call ID: call_Kn09w9jd9swcNzIZ1b5MlKID  Args:    table_names: Album, Artist=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_schemaCREATE TABLE \"Album\" (\\t\"AlbumId\" INTEGER NOT NULL, \\t\"Title\" NVARCHAR(160) NOT NULL, \\t\"ArtistId\" INTEGER NOT NULL, \\tPRIMARY KEY (\"AlbumId\"), \\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\"))/*3 rows from Album table:AlbumId\\tTitle\\tArtistId1\\tFor Those About To Rock We Salute You\\t12\\tBalls to the Wall\\t23\\tRestless and Wild\\t2*/CREATE TABLE \"Artist\" (\\t\"ArtistId\" INTEGER NOT NULL, \\t\"Name\" NVARCHAR(120), \\tPRIMARY KEY (\"ArtistId\"))/*3 rows from Artist table:ArtistId\\tName1\\tAC/DC2\\tAccept3\\tAerosmith*/==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_query (call_WkHRiPcBoGN9bc58MIupRHKP) Call ID: call_WkHRiPcBoGN9bc58MIupRHKP  Args:    query: SELECT COUNT(*) FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = \\'Alice In Chains\\')=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_query[(1,)]==================================\\x1b[1m Ai Message \\x1b[0m==================================Alice In Chains has released 1 album in the database.\\nAs we can see, both in the streamed steps and in the LangSmith trace, the agent used the search_proper_nouns tool in order to check how to correctly query the database for this specific artist.Edit this pagePreviousBuild a semantic search engineNextSummarize Text‚ö†Ô∏è Security note ‚ö†Ô∏èArchitectureSetupSample dataChainsApplication stateConvert question to SQL queryExecute queryGenerate answerOrchestrating with LangGraphHuman-in-the-loopNext stepsAgentsSystem PromptInitializing agentDealing with high-cardinality columnsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/summarization/', 'title': 'Summarize Text | ü¶úÔ∏èüîó LangChain', 'description': 'This tutorial demonstrates text summarization using built-in chains and LangGraph.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nSummarize Text | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsSummarize TextOn this pageSummarize Text\\ninfoThis tutorial demonstrates text summarization using built-in chains and LangGraph.A previous version of this page showcased the legacy chains StuffDocumentsChain, MapReduceDocumentsChain, and RefineDocumentsChain. See here for information on using those abstractions and a comparison with the methods demonstrated in this tutorial.\\nSuppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content.\\nLLMs are a great tool for this given their proficiency in understanding and synthesizing text.\\nIn the context of retrieval-augmented generation, summarizing text can help distill the information in a large number of retrieved documents to provide context for a LLM.\\nIn this walkthrough we\\'ll go over how to summarize content from multiple documents using LLMs.\\n\\nConcepts\\u200b\\nConcepts we will cover are:\\n\\n\\nUsing language models.\\n\\n\\nUsing document loaders, specifically the WebBaseLoader to load content from an HTML webpage.\\n\\n\\nTwo ways to summarize or otherwise combine documents.\\n\\nStuff, which simply concatenates documents into a prompt;\\nMap-reduce, for larger sets of documents. This splits documents into batches, summarizes those, and then summarizes the summaries.\\n\\n\\n\\nShorter, targeted guides on these strategies and others, including iterative refinement, can be found in the how-to guides.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nOverview\\u200b\\nA central question for building a summarizer is how to pass your documents into the LLM\\'s context window. Two common approaches for this are:\\n\\n\\nStuff: Simply \"stuff\" all your documents into a single prompt. This is the simplest approach (see here for more on the create_stuff_documents_chain constructor, which is used for this method).\\n\\n\\nMap-reduce: Summarize each document on its own in a \"map\" step and then \"reduce\" the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method).\\n\\n\\nNote that map-reduce is especially effective when understanding of a sub-document does not rely on preceding context. For example, when summarizing a corpus of many, shorter documents. In other cases, such as summarizing a novel or body of text with an inherent sequence, iterative refinement may be more effective.\\n\\nSetup\\u200b\\nFirst set environment variables and install packages:\\n%pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4 langchain-community# Set env var OPENAI_API_KEY or load from a .env file# import dotenv# dotenv.load_dotenv()\\nimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nFirst we load in our documents. We will use WebBaseLoader to load a blog post:\\nfrom langchain_community.document_loaders import WebBaseLoaderloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")docs = loader.load()\\nLet\\'s next select a LLM:\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nStuff: summarize in a single LLM call\\u200b\\nWe can use create_stuff_documents_chain, especially if using larger context window models such as:\\n\\n128k token OpenAI gpt-4o\\n200k token Anthropic claude-3-5-sonnet-latest\\n\\nThe chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:\\nfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain.chains.llm import LLMChainfrom langchain_core.prompts import ChatPromptTemplate# Define promptprompt = ChatPromptTemplate.from_messages(    [(\"system\", \"Write a concise summary of the following:\\\\\\\\n\\\\\\\\n{context}\")])# Instantiate chainchain = create_stuff_documents_chain(llm, prompt)# Invoke chainresult = chain.invoke({\"context\": docs})print(result)API Reference:ChatPromptTemplate\\nThe article \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture that includes three main components: Planning, Memory, and Tool Use. 1. **Planning** involves task decomposition, where complex tasks are broken down into manageable subgoals, and self-reflection, allowing agents to learn from past actions to improve future performance. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for enhancing reasoning and planning.2. **Memory** is categorized into short-term and long-term memory, with mechanisms for fast retrieval using Maximum Inner Product Search (MIPS) algorithms. This allows agents to retain and recall information effectively.3. **Tool Use** enables agents to interact with external APIs and tools, enhancing their capabilities beyond the limitations of their training data. Examples include MRKL systems and frameworks like HuggingGPT, which facilitate task planning and execution.The article also addresses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes with case studies demonstrating the practical applications of these concepts in scientific discovery and interactive simulations. Overall, the article emphasizes the potential of LLMs as powerful problem solvers in autonomous agent systems.\\nStreaming\\u200b\\nNote that we can also stream the result token-by-token:\\nfor token in chain.stream({\"context\": docs}):    print(token, end=\"|\")\\n|The| article| \"|LL|M| Powered| Autonomous| Agents|\"| by| Lil|ian| W|eng| discusses| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| outlines| a| system| architecture| that| includes| three| main| components|:| Planning|,| Memory|,| and| Tool| Use|.| |1|.| **|Planning|**| involves| task| decomposition|,| where| complex| tasks| are| broken| down| into| manageable| sub|go|als|,| and| self|-ref|lection|,| allowing| agents| to| learn| from| past| actions| to| improve| future| performance|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| enhancing| reasoning| and| planning|.|2|.| **|Memory|**| is| categorized| into| short|-term| and| long|-term| memory|,| with| mechanisms| for| fast| retrieval| using| Maximum| Inner| Product| Search| (|M|IPS|)| algorithms|.| This| allows| agents| to| retain| and| recall| information| effectively|.|3|.| **|Tool| Use|**| emphasizes| the| integration| of| external| APIs| and| tools| to| extend| the| capabilities| of| L|LM|s|,| enabling| them| to| perform| tasks| beyond| their| inherent| limitations|.| Examples| include| MR|KL| systems| and| frameworks| like| Hug|ging|GPT|,| which| facilitate| task| planning| and| execution|.|The| article| also| addresses| challenges| such| as| finite| context| length|,| difficulties| in| long|-term| planning|,| and| the| reliability| of| natural| language| interfaces|.| It| concludes| with| case| studies| demonstrating| the| practical| applications| of| L|LM|-powered| agents| in| scientific| discovery| and| interactive| simulations|.| Overall|,| the| piece| illustrates| the| potential| of| L|LM|s| as| general| problem| sol|vers| and| their| evolving| role| in| autonomous| systems|.||\\nGo deeper\\u200b\\n\\nYou can easily customize the prompt.\\nYou can easily try different LLMs, (e.g., Claude) via the llm parameter.\\n\\nMap-Reduce: summarize long texts via parallelization\\u200b\\nLet\\'s unpack the map reduce approach. For this, we\\'ll first map each document to an individual summary using an LLM. Then we\\'ll reduce or consolidate those summaries into a single global summary.\\nNote that the map step is typically parallelized over the input documents.\\nLangGraph, built on top of langchain-core, supports map-reduce workflows and is well-suited to this problem:\\n\\nLangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;\\nLangGraph\\'s checkpointing supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.\\nThe LangGraph implementation is straightforward to modify and extend, as we will see below.\\n\\nMap\\u200b\\nLet\\'s first define the prompt associated with the map step. We can use the same summarization prompt as in the stuff approach, above:\\nfrom langchain_core.prompts import ChatPromptTemplatemap_prompt = ChatPromptTemplate.from_messages(    [(\"system\", \"Write a concise summary of the following:\\\\\\\\n\\\\\\\\n{context}\")])API Reference:ChatPromptTemplate\\nWe can also use the Prompt Hub to store and fetch prompts.\\nThis will work with your LangSmith API key.\\nFor example, see the map prompt here.\\nfrom langchain import hubmap_prompt = hub.pull(\"rlm/map-prompt\")\\nReduce\\u200b\\nWe also define a prompt that takes the document mapping results and reduces them into a single output.\\n# Also available via the hub: `hub.pull(\"rlm/reduce-prompt\")`reduce_template = \"\"\"The following is a set of summaries:{docs}Take these and distill it into a final, consolidated summaryof the main themes.\"\"\"reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\\nOrchestration via LangGraph\\u200b\\nBelow we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.\\nMap-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model\\'s context window size. Here we implement a recursive \"collapsing\" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.\\nFirst we chunk the blog post into smaller \"sub documents\" to be mapped:\\nfrom langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    chunk_size=1000, chunk_overlap=0)split_docs = text_splitter.split_documents(docs)print(f\"Generated {len(split_docs)} documents.\")\\nCreated a chunk of size 1003, which is longer than the specified 1000``````outputGenerated 14 documents.\\nNext, we define our graph. Note that we define an artificially low maximum token length of 1,000 tokens to illustrate the \"collapsing\" step.\\nimport operatorfrom typing import Annotated, List, Literal, TypedDictfrom langchain.chains.combine_documents.reduce import (    acollapse_docs,    split_list_of_docs,)from langchain_core.documents import Documentfrom langgraph.constants import Sendfrom langgraph.graph import END, START, StateGraphtoken_max = 1000def length_function(documents: List[Document]) -> int:    \"\"\"Get number of tokens for input contents.\"\"\"    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)# This will be the overall state of the main graph.# It will contain the input document contents, corresponding# summaries, and a final summary.class OverallState(TypedDict):    # Notice here we use the operator.add    # This is because we want combine all the summaries we generate    # from individual nodes back into one list - this is essentially    # the \"reduce\" part    contents: List[str]    summaries: Annotated[list, operator.add]    collapsed_summaries: List[Document]    final_summary: str# This will be the state of the node that we will \"map\" all# documents to in order to generate summariesclass SummaryState(TypedDict):    content: str# Here we generate a summary, given a documentasync def generate_summary(state: SummaryState):    prompt = map_prompt.invoke(state[\"content\"])    response = await llm.ainvoke(prompt)    return {\"summaries\": [response.content]}# Here we define the logic to map out over the documents# We will use this an edge in the graphdef map_summaries(state: OverallState):    # We will return a list of `Send` objects    # Each `Send` object consists of the name of a node in the graph    # as well as the state to send to that node    return [        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]    ]def collect_summaries(state: OverallState):    return {        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]    }async def _reduce(input: dict) -> str:    prompt = reduce_prompt.invoke(input)    response = await llm.ainvoke(prompt)    return response.content# Add node to collapse summariesasync def collapse_summaries(state: OverallState):    doc_lists = split_list_of_docs(        state[\"collapsed_summaries\"], length_function, token_max    )    results = []    for doc_list in doc_lists:        results.append(await acollapse_docs(doc_list, _reduce))    return {\"collapsed_summaries\": results}# This represents a conditional edge in the graph that determines# if we should collapse the summaries or notdef should_collapse(    state: OverallState,) -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:    num_tokens = length_function(state[\"collapsed_summaries\"])    if num_tokens > token_max:        return \"collapse_summaries\"    else:        return \"generate_final_summary\"# Here we will generate the final summaryasync def generate_final_summary(state: OverallState):    response = await _reduce(state[\"collapsed_summaries\"])    return {\"final_summary\": response}# Construct the graph# Nodes:graph = StateGraph(OverallState)graph.add_node(\"generate_summary\", generate_summary)  # same as beforegraph.add_node(\"collect_summaries\", collect_summaries)graph.add_node(\"collapse_summaries\", collapse_summaries)graph.add_node(\"generate_final_summary\", generate_final_summary)# Edges:graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])graph.add_edge(\"generate_summary\", \"collect_summaries\")graph.add_conditional_edges(\"collect_summaries\", should_collapse)graph.add_conditional_edges(\"collapse_summaries\", should_collapse)graph.add_edge(\"generate_final_summary\", END)app = graph.compile()API Reference:Document | Send | StateGraph\\nLangGraph allows the graph structure to be plotted to help visualize its function:\\nfrom IPython.display import ImageImage(app.get_graph().draw_mermaid_png())\\n\\nWhen running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.\\nNote that because we have a loop in the graph, it can be helpful to specify a recursion_limit on its execution. This will raise a specific error when the specified limit is exceeded.\\nasync for step in app.astream(    {\"contents\": [doc.page_content for doc in split_docs]},    {\"recursion_limit\": 10},):    print(list(step.keys()))\\n[\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'collect_summaries\\'][\\'collapse_summaries\\'][\\'collapse_summaries\\'][\\'generate_final_summary\\']\\nprint(step)\\n{\\'generate_final_summary\\': {\\'final_summary\\': \\'The consolidated summary of the main themes from the provided documents is as follows:\\\\n\\\\n1. **Integration of Large Language Models (LLMs) in Autonomous Agents**: The documents explore the evolving role of LLMs in autonomous systems, emphasizing their enhanced reasoning and acting capabilities through methodologies that incorporate structured planning, memory systems, and tool use.\\\\n\\\\n2. **Core Components of Autonomous Agents**:\\\\n   - **Planning**: Techniques like task decomposition (e.g., Chain of Thought) and external classical planners are utilized to facilitate long-term planning by breaking down complex tasks.\\\\n   - **Memory**: The memory system is divided into short-term (in-context learning) and long-term memory, with parallels drawn between human memory and machine learning to improve agent performance.\\\\n   - **Tool Use**: Agents utilize external APIs and algorithms to enhance problem-solving abilities, exemplified by frameworks like HuggingGPT that manage task workflows.\\\\n\\\\n3. **Neuro-Symbolic Architectures**: The integration of MRKL (Modular Reasoning, Knowledge, and Language) systems combines neural and symbolic expert modules with LLMs, addressing challenges in tasks such as verbal math problem-solving.\\\\n\\\\n4. **Specialized Applications**: Case studies, such as ChemCrow and projects in anticancer drug discovery, demonstrate the advantages of LLMs augmented with expert tools in specialized domains.\\\\n\\\\n5. **Challenges and Limitations**: The documents highlight challenges such as hallucination in model outputs and the finite context length of LLMs, which affects their ability to incorporate historical information and perform self-reflection. Techniques like Chain of Hindsight and Algorithm Distillation are discussed to enhance model performance through iterative learning.\\\\n\\\\n6. **Structured Software Development**: A systematic approach to creating Python software projects is emphasized, focusing on defining core components, managing dependencies, and adhering to best practices for documentation.\\\\n\\\\nOverall, the integration of structured planning, memory systems, and advanced tool use aims to enhance the capabilities of LLM-powered autonomous agents while addressing the challenges and limitations these technologies face in real-world applications.\\'}}\\nIn the corresponding LangSmith trace we can see the individual LLM calls, grouped under their respective nodes.\\nGo deeper\\u200b\\nCustomization\\n\\nAs shown above, you can customize the LLMs and prompts for map and reduce stages.\\n\\nReal-world use-case\\n\\nSee this blog post case-study on analyzing user interactions (questions about LangChain documentation)!\\nThe blog post and associated repo also introduce clustering as a means of summarization.\\nThis opens up another path beyond the stuff or map-reduce approaches that is worth considering.\\n\\n\\nNext steps\\u200b\\nWe encourage you to check out the how-to guides for more detail on:\\n\\nOther summarization strategies, such as iterative refinement\\nBuilt-in document loaders and text-splitters\\nIntegrating various combine-document chains into a RAG application\\nIncorporating retrieval into a chatbot\\n\\nand other concepts.Edit this pagePreviousBuild a Question/Answering system over SQL dataNextHow-to guidesConceptsSetupJupyter NotebookInstallationLangSmithOverviewSetupStuff: summarize in a single LLM callStreamingGo deeperMap-Reduce: summarize long texts via parallelizationMapReduceOrchestration via LangGraphGo deeperNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/graph/', 'title': 'Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain', 'description': \"In this guide we'll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer. First, we will show a simple out-of-the-box option and then implement a more sophisticated version with LangGraph.\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Question Answering application over a Graph DatabaseOn this pageBuild a Question Answering application over a Graph Database\\nIn this guide we\\'ll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer. First, we will show a simple out-of-the-box option and then implement a more sophisticated version with LangGraph.\\n‚ö†Ô∏è Security note ‚ö†Ô∏è\\u200b\\nBuilding Q&A systems of graph databases requires executing model-generated graph queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent\\'s needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see here.\\nArchitecture\\u200b\\nAt a high-level, the steps of most graph chains are:\\n\\nConvert question to a graph database query: Model converts user input to a graph database query (e.g. Cypher).\\nExecute graph database query: Execute the graph database query.\\nAnswer the question: Model responds to user input using the query results.\\n\\n\\nSetup\\u200b\\nFirst, get required packages and set environment variables.\\nIn this example, we will be using Neo4j graph database.\\n%pip install --upgrade --quiet langchain langchain-neo4j langchain-openai langgraph\\nWe default to OpenAI models in this guide.\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")# Uncomment the below to use LangSmith. Not required.# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nEnter your OpenAI API key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\\nNext, we need to define Neo4j credentials.\\nFollow these installation steps to set up a Neo4j database.\\nos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"os.environ[\"NEO4J_PASSWORD\"] = \"password\"\\nThe below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.\\nfrom langchain_neo4j import Neo4jGraphgraph = Neo4jGraph()# Import movie informationmovies_query = \"\"\"LOAD CSV WITH HEADERS FROM \\'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv\\'AS rowMERGE (m:Movie {id:row.movieId})SET m.released = date(row.released),    m.title = row.title,    m.imdbRating = toFloat(row.imdbRating)FOREACH (director in split(row.director, \\'|\\') |     MERGE (p:Person {name:trim(director)})    MERGE (p)-[:DIRECTED]->(m))FOREACH (actor in split(row.actors, \\'|\\') |     MERGE (p:Person {name:trim(actor)})    MERGE (p)-[:ACTED_IN]->(m))FOREACH (genre in split(row.genres, \\'|\\') |     MERGE (g:Genre {name:trim(genre)})    MERGE (m)-[:IN_GENRE]->(g))\"\"\"graph.query(movies_query)\\n[]\\nGraph schema\\u200b\\nIn order for an LLM to be able to generate a Cypher statement, it needs information about the graph schema. When you instantiate a graph object, it retrieves the information about the graph schema. If you later make any changes to the graph, you can run the refresh_schema method to refresh the schema information.\\ngraph.refresh_schema()print(graph.schema)\\nNode properties:Person {name: STRING}Movie {id: STRING, released: DATE, title: STRING, imdbRating: FLOAT}Genre {name: STRING}Chunk {id: STRING, embedding: LIST, text: STRING, question: STRING, query: STRING}Relationship properties:The relationships:(:Person)-[:DIRECTED]->(:Movie)(:Person)-[:ACTED_IN]->(:Movie)(:Movie)-[:IN_GENRE]->(:Genre)\\nFor more involved schema information, you can use enhanced_schema option.\\nenhanced_graph = Neo4jGraph(enhanced_schema=True)print(enhanced_graph.schema)\\nReceived notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. (\\'config\\' used by \\'apoc.meta.graphSample\\' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, \\'type\\'), count: apoc.any.property(rel, \\'count\\')}] AS relationships\"``````outputNode properties:- **Person**  - `name`: STRING Example: \"John Lasseter\"- **Movie**  - `id`: STRING Example: \"1\"  - `released`: DATE Min: 1964-12-16, Max: 1996-09-15  - `title`: STRING Example: \"Toy Story\"  - `imdbRating`: FLOAT Min: 2.4, Max: 9.3- **Genre**  - `name`: STRING Example: \"Adventure\"- **Chunk**  - `id`: STRING Available options: [\\'d66006059fd78d63f3df90cc1059639a\\', \\'0e3dcb4502853979d12357690a95ec17\\', \\'c438c6bcdcf8e4fab227f29f8e7ff204\\', \\'97fe701ec38057594464beaa2df0710e\\', \\'b54f9286e684373498c4504b4edd9910\\', \\'5b50a72c3a4954b0ff7a0421be4f99b9\\', \\'fb28d41771e717255f0d8f6c799ede32\\', \\'58e6f14dd2e6c6702cf333f2335c499c\\']  - `text`: STRING Available options: [\\'How many artists are there?\\', \\'Which actors played in the movie Casino?\\', \\'How many movies has Tom Hanks acted in?\\', \"List all the genres of the movie Schindler\\'s List\", \\'Which actors have worked in movies from both the c\\', \\'Which directors have made movies with at least thr\\', \\'Identify movies where directors also played a role\\', \\'Find the actor with the highest number of movies i\\']  - `question`: STRING Available options: [\\'How many artists are there?\\', \\'Which actors played in the movie Casino?\\', \\'How many movies has Tom Hanks acted in?\\', \"List all the genres of the movie Schindler\\'s List\", \\'Which actors have worked in movies from both the c\\', \\'Which directors have made movies with at least thr\\', \\'Identify movies where directors also played a role\\', \\'Find the actor with the highest number of movies i\\']  - `query`: STRING Available options: [\\'MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN coun\\', \"MATCH (m:Movie {title: \\'Casino\\'})<-[:ACTED_IN]-(a)\", \"MATCH (a:Person {name: \\'Tom Hanks\\'})-[:ACTED_IN]->\", \"MATCH (m:Movie {title: \\'Schindler\\'s List\\'})-[:IN_G\", \\'MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]\\', \\'MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_I\\', \\'MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACT\\', \\'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.na\\']Relationship properties:The relationships:(:Person)-[:DIRECTED]->(:Movie)(:Person)-[:ACTED_IN]->(:Movie)(:Movie)-[:IN_GENRE]->(:Genre)\\nThe enhanced_schema option enriches property information by including details such as minimum and maximum values for floats and dates, as well as example values for string properties. This additional context helps guide the LLM toward generating more accurate and effective queries.\\nGreat! We\\'ve got a graph database that we can query. Now let\\'s try hooking it up to an LLM.\\nGraphQACypherChain\\u200b\\nLet\\'s use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question.\\n\\nLangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: GraphCypherQAChain\\nfrom langchain_neo4j import GraphCypherQAChainfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)chain = GraphCypherQAChain.from_llm(    graph=enhanced_graph, llm=llm, verbose=True, allow_dangerous_requests=True)response = chain.invoke({\"query\": \"What was the cast of the Casino?\"})response\\n\\x1b[1m> Entering new GraphCypherQAChain chain...\\x1b[0mGenerated Cypher:\\x1b[32;1m\\x1b[1;3mcypherMATCH (p:Person)-[:ACTED_IN]->(m:Movie {title: \"Casino\"})RETURN p.name\\x1b[0mFull Context:\\x1b[32;1m\\x1b[1;3m[{\\'p.name\\': \\'Robert De Niro\\'}, {\\'p.name\\': \\'Joe Pesci\\'}, {\\'p.name\\': \\'Sharon Stone\\'}, {\\'p.name\\': \\'James Woods\\'}]\\x1b[0m\\x1b[1m> Finished chain.\\x1b[0m\\n{\\'query\\': \\'What was the cast of the Casino?\\', \\'result\\': \\'Robert De Niro, Joe Pesci, Sharon Stone, and James Woods were the cast of Casino.\\'}\\nAdvanced implementation with LangGraph\\u200b\\nWhile the GraphCypherQAChain is effective for quick demonstrations, it may face challenges in production environments. Transitioning to LangGraph can enhance the workflow, but implementing natural language to query flows in production remains a complex task. Nevertheless, there are several strategies to significantly improve accuracy and reliability, which we will explore next.\\nHere is the visualized LangGraph flow we will implement:\\n\\nWe will begin by defining the Input, Output, and Overall state of the LangGraph application.\\nfrom operator import addfrom typing import Annotated, Listfrom typing_extensions import TypedDictclass InputState(TypedDict):    question: strclass OverallState(TypedDict):    question: str    next_action: str    cypher_statement: str    cypher_errors: List[str]    database_records: List[dict]    steps: Annotated[List[str], add]class OutputState(TypedDict):    answer: str    steps: List[str]    cypher_statement: str\\nThe first step is a simple guardrails step, where we validate whether the question pertains to movies or their cast. If it doesn\\'t, we notify the user that we cannot answer any other questions. Otherwise, we move on to the Cypher generation step.\\nfrom typing import Literalfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldguardrails_system = \"\"\"As an intelligent assistant, your primary objective is to decide whether a given question is related to movies or not. If the question is related to movies, output \"movie\". Otherwise, output \"end\".To make this decision, assess the content of the question and determine if it refers to any movie, actor, director, film industry, or related topics. Provide only the specified output: \"movie\" or \"end\".\"\"\"guardrails_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            guardrails_system,        ),        (            \"human\",            (\"{question}\"),        ),    ])class GuardrailsOutput(BaseModel):    decision: Literal[\"movie\", \"end\"] = Field(        description=\"Decision on whether the question is related to movies\"    )guardrails_chain = guardrails_prompt | llm.with_structured_output(GuardrailsOutput)def guardrails(state: InputState) -> OverallState:    \"\"\"    Decides if the question is related to movies or not.    \"\"\"    guardrails_output = guardrails_chain.invoke({\"question\": state.get(\"question\")})    database_records = None    if guardrails_output.decision == \"end\":        database_records = \"This questions is not about movies or their cast. Therefore I cannot answer this question.\"    return {        \"next_action\": guardrails_output.decision,        \"database_records\": database_records,        \"steps\": [\"guardrail\"],    }API Reference:ChatPromptTemplate\\nFew-shot prompting\\u200b\\nConverting natural language into accurate queries is challenging. One way to enhance this process is by providing relevant few-shot examples to guide the LLM in query generation. To achieve this, we will use the SemanticSimilarityExampleSelector to dynamically select the most relevant examples.\\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_neo4j import Neo4jVectorfrom langchain_openai import OpenAIEmbeddingsexamples = [    {        \"question\": \"How many artists are there?\",        \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)\",    },    {        \"question\": \"Which actors played in the movie Casino?\",        \"query\": \"MATCH (m:Movie {title: \\'Casino\\'})<-[:ACTED_IN]-(a) RETURN a.name\",    },    {        \"question\": \"How many movies has Tom Hanks acted in?\",        \"query\": \"MATCH (a:Person {name: \\'Tom Hanks\\'})-[:ACTED_IN]->(m:Movie) RETURN count(m)\",    },    {        \"question\": \"List all the genres of the movie Schindler\\'s List\",        \"query\": \"MATCH (m:Movie {title: \\'Schindler\\'s List\\'})-[:IN_GENRE]->(g:Genre) RETURN g.name\",    },    {        \"question\": \"Which actors have worked in movies from both the comedy and action genres?\",        \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = \\'Comedy\\' AND g2.name = \\'Action\\' RETURN DISTINCT a.name\",    },    {        \"question\": \"Which directors have made movies with at least three different actors named \\'John\\'?\",        \"query\": \"MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH \\'John\\' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name\",    },    {        \"question\": \"Identify movies where directors also played a role in the film.\",        \"query\": \"MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACTED_IN]->(m) RETURN m.title, p.name\",    },    {        \"question\": \"Find the actor with the highest number of movies in the database.\",        \"query\": \"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1\",    },]example_selector = SemanticSimilarityExampleSelector.from_examples(    examples, OpenAIEmbeddings(), Neo4jVector, k=5, input_keys=[\"question\"])API Reference:SemanticSimilarityExampleSelector\\nNext, we implement the Cypher generation chain, also known as text2cypher. The prompt includes an enhanced graph schema, dynamically selected few-shot examples, and the user‚Äôs question. This combination enables the generation of a Cypher query to retrieve relevant information from the database.\\nfrom langchain_core.output_parsers import StrOutputParsertext2cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            (                \"Given an input question, convert it to a Cypher query. No pre-amble.\"                \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"            ),        ),        (            \"human\",            (                \"\"\"You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!Here is the schema information{schema}Below are a number of examples of questions and their corresponding Cypher queries.{fewshot_examples}User input: {question}Cypher query:\"\"\"            ),        ),    ])text2cypher_chain = text2cypher_prompt | llm | StrOutputParser()def generate_cypher(state: OverallState) -> OverallState:    \"\"\"    Generates a cypher statement based on the provided schema and user input    \"\"\"    NL = \"\\\\n\"    fewshot_examples = (NL * 2).join(        [            f\"Question: {el[\\'question\\']}{NL}Cypher:{el[\\'query\\']}\"            for el in example_selector.select_examples(                {\"question\": state.get(\"question\")}            )        ]    )    generated_cypher = text2cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"fewshot_examples\": fewshot_examples,            \"schema\": enhanced_graph.schema,        }    )    return {\"cypher_statement\": generated_cypher, \"steps\": [\"generate_cypher\"]}API Reference:StrOutputParser\\nQuery validation\\u200b\\nThe next step is to validate the generated Cypher statement and ensuring that all property values are accurate. While numbers and dates typically don‚Äôt require validation, strings such as movie titles or people‚Äôs names do. In this example, we‚Äôll use a basic CONTAINS clause for validation, though more advanced mapping and validation techniques can be implemented if needed.\\nFirst, we will create a chain that detects any errors in the Cypher statement and extracts the property values it references.\\nfrom typing import List, Optionalvalidate_cypher_system = \"\"\"You are a Cypher expert reviewing a statement written by a junior developer.\"\"\"validate_cypher_user = \"\"\"You must check the following:* Are there any syntax errors in the Cypher statement?* Are there any missing or undefined variables in the Cypher statement?* Are any node labels missing from the schema?* Are any relationship types missing from the schema?* Are any of the properties not included in the schema?* Does the Cypher statement include enough information to answer the question?Examples of good errors:* Label (:Foo) does not exist, did you mean (:Bar)?* Property bar does not exist for label Foo, did you mean baz?* Relationship FOO does not exist, did you mean FOO_BAR?Schema:{schema}The question is:{question}The Cypher statement is:{cypher}Make sure you don\\'t make any mistakes!\"\"\"validate_cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            validate_cypher_system,        ),        (            \"human\",            (validate_cypher_user),        ),    ])class Property(BaseModel):    \"\"\"    Represents a filter condition based on a specific node property in a graph in a Cypher statement.    \"\"\"    node_label: str = Field(        description=\"The label of the node to which this property belongs.\"    )    property_key: str = Field(description=\"The key of the property being filtered.\")    property_value: str = Field(        description=\"The value that the property is being matched against.\"    )class ValidateCypherOutput(BaseModel):    \"\"\"    Represents the validation result of a Cypher query\\'s output,    including any errors and applied filters.    \"\"\"    errors: Optional[List[str]] = Field(        description=\"A list of syntax or semantical errors in the Cypher statement. Always explain the discrepancy between schema and Cypher statement\"    )    filters: Optional[List[Property]] = Field(        description=\"A list of property-based filters applied in the Cypher statement.\"    )validate_cypher_chain = validate_cypher_prompt | llm.with_structured_output(    ValidateCypherOutput)\\nLLMs often struggle with correctly determining relationship directions in generated Cypher statements. Since we have access to the schema, we can deterministically correct these directions using the CypherQueryCorrector.\\nNote: The CypherQueryCorrector is an experimental feature and doesn\\'t support all the newest Cypher syntax.\\nfrom langchain_neo4j.chains.graph_qa.cypher_utils import CypherQueryCorrector, Schema# Cypher query corrector is experimentalcorrector_schema = [    Schema(el[\"start\"], el[\"type\"], el[\"end\"])    for el in enhanced_graph.structured_schema.get(\"relationships\")]cypher_query_corrector = CypherQueryCorrector(corrector_schema)\\nNow we can implement the Cypher validation step. First, we use the EXPLAIN method to detect any syntax errors. Next, we leverage the LLM to identify potential issues and extract the properties used for filtering. For string properties, we validate them against the database using a simple CONTAINS clause.\\nBased on the validation results, the process can take the following paths:\\n\\nIf value mapping fails, we end the conversation and inform the user that we couldn\\'t identify a specific property value (e.g., a person or movie title).\\nIf errors are found, we route the query for correction.\\nIf no issues are detected, we proceed to the Cypher execution step.\\n\\nfrom neo4j.exceptions import CypherSyntaxErrordef validate_cypher(state: OverallState) -> OverallState:    \"\"\"    Validates the Cypher statements and maps any property values to the database.    \"\"\"    errors = []    mapping_errors = []    # Check for syntax errors    try:        enhanced_graph.query(f\"EXPLAIN {state.get(\\'cypher_statement\\')}\")    except CypherSyntaxError as e:        errors.append(e.message)    # Experimental feature for correcting relationship directions    corrected_cypher = cypher_query_corrector(state.get(\"cypher_statement\"))    if not corrected_cypher:        errors.append(\"The generated Cypher statement doesn\\'t fit the graph schema\")    if not corrected_cypher == state.get(\"cypher_statement\"):        print(\"Relationship direction was corrected\")    # Use LLM to find additional potential errors and get the mapping for values    llm_output = validate_cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"schema\": enhanced_graph.schema,            \"cypher\": state.get(\"cypher_statement\"),        }    )    if llm_output.errors:        errors.extend(llm_output.errors)    if llm_output.filters:        for filter in llm_output.filters:            # Do mapping only for string values            if (                not [                    prop                    for prop in enhanced_graph.structured_schema[\"node_props\"][                        filter.node_label                    ]                    if prop[\"property\"] == filter.property_key                ][0][\"type\"]                == \"STRING\"            ):                continue            mapping = enhanced_graph.query(                f\"MATCH (n:{filter.node_label}) WHERE toLower(n.`{filter.property_key}`) = toLower($value) RETURN \\'yes\\' LIMIT 1\",                {\"value\": filter.property_value},            )            if not mapping:                print(                    f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\"                )                mapping_errors.append(                    f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\"                )    if mapping_errors:        next_action = \"end\"    elif errors:        next_action = \"correct_cypher\"    else:        next_action = \"execute_cypher\"    return {        \"next_action\": next_action,        \"cypher_statement\": corrected_cypher,        \"cypher_errors\": errors,        \"steps\": [\"validate_cypher\"],    }\\nThe Cypher correction step takes the existing Cypher statement, any identified errors, and the original question to generate a corrected version of the query.\\ncorrect_cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            (                \"You are a Cypher expert reviewing a statement written by a junior developer. \"                \"You need to correct the Cypher statement based on the provided errors. No pre-amble.\"                \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"            ),        ),        (            \"human\",            (                \"\"\"Check for invalid syntax or semantics and return a corrected Cypher statement.Schema:{schema}Note: Do not include any explanations or apologies in your responses.Do not wrap the response in any backticks or anything else.Respond with a Cypher statement only!Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.The question is:{question}The Cypher statement is:{cypher}The errors are:{errors}Corrected Cypher statement: \"\"\"            ),        ),    ])correct_cypher_chain = correct_cypher_prompt | llm | StrOutputParser()def correct_cypher(state: OverallState) -> OverallState:    \"\"\"    Correct the Cypher statement based on the provided errors.    \"\"\"    corrected_cypher = correct_cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"errors\": state.get(\"cypher_errors\"),            \"cypher\": state.get(\"cypher_statement\"),            \"schema\": enhanced_graph.schema,        }    )    return {        \"next_action\": \"validate_cypher\",        \"cypher_statement\": corrected_cypher,        \"steps\": [\"correct_cypher\"],    }\\nWe need to add a step that executes the given Cypher statement. If no results are returned, we should explicitly handle this scenario, as leaving the context empty can sometimes lead to LLM hallucinations.\\nno_results = \"I couldn\\'t find any relevant information in the database\"def execute_cypher(state: OverallState) -> OverallState:    \"\"\"    Executes the given Cypher statement.    \"\"\"    records = enhanced_graph.query(state.get(\"cypher_statement\"))    return {        \"database_records\": records if records else no_results,        \"next_action\": \"end\",        \"steps\": [\"execute_cypher\"],    }\\nThe final step is to generate the answer. This involves combining the initial question with the database output to produce a relevant response.\\ngenerate_final_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant\",        ),        (            \"human\",            (                \"\"\"Use the following results retrieved from a database to providea succinct, definitive answer to the user\\'s question.Respond as if you are answering the question directly.Results: {results}Question: {question}\"\"\"            ),        ),    ])generate_final_chain = generate_final_prompt | llm | StrOutputParser()def generate_final_answer(state: OverallState) -> OutputState:    \"\"\"    Decides if the question is related to movies.    \"\"\"    final_answer = generate_final_chain.invoke(        {\"question\": state.get(\"question\"), \"results\": state.get(\"database_records\")}    )    return {\"answer\": final_answer, \"steps\": [\"generate_final_answer\"]}\\nNext, we will implement the LangGraph workflow, starting with defining the conditional edge functions.\\ndef guardrails_condition(    state: OverallState,) -> Literal[\"generate_cypher\", \"generate_final_answer\"]:    if state.get(\"next_action\") == \"end\":        return \"generate_final_answer\"    elif state.get(\"next_action\") == \"movie\":        return \"generate_cypher\"def validate_cypher_condition(    state: OverallState,) -> Literal[\"generate_final_answer\", \"correct_cypher\", \"execute_cypher\"]:    if state.get(\"next_action\") == \"end\":        return \"generate_final_answer\"    elif state.get(\"next_action\") == \"correct_cypher\":        return \"correct_cypher\"    elif state.get(\"next_action\") == \"execute_cypher\":        return \"execute_cypher\"\\nLet\\'s put it all together now.\\nfrom IPython.display import Image, displayfrom langgraph.graph import END, START, StateGraphlanggraph = StateGraph(OverallState, input=InputState, output=OutputState)langgraph.add_node(guardrails)langgraph.add_node(generate_cypher)langgraph.add_node(validate_cypher)langgraph.add_node(correct_cypher)langgraph.add_node(execute_cypher)langgraph.add_node(generate_final_answer)langgraph.add_edge(START, \"guardrails\")langgraph.add_conditional_edges(    \"guardrails\",    guardrails_condition,)langgraph.add_edge(\"generate_cypher\", \"validate_cypher\")langgraph.add_conditional_edges(    \"validate_cypher\",    validate_cypher_condition,)langgraph.add_edge(\"execute_cypher\", \"generate_final_answer\")langgraph.add_edge(\"correct_cypher\", \"validate_cypher\")langgraph.add_edge(\"generate_final_answer\", END)langgraph = langgraph.compile()# Viewdisplay(Image(langgraph.get_graph().draw_mermaid_png()))API Reference:StateGraph\\n\\nWe can now test the application by asking an irrelevant question.\\nlanggraph.invoke({\"question\": \"What\\'s the weather in Spain?\"})\\n{\\'answer\\': \"I\\'m sorry, but I cannot provide current weather information. Please check a reliable weather website or app for the latest updates on the weather in Spain.\", \\'steps\\': [\\'guardrail\\', \\'generate_final_answer\\']}\\nLet\\'s now ask something relevant about the movies.\\nlanggraph.invoke({\"question\": \"What was the cast of the Casino?\"})\\n{\\'answer\\': \\'The cast of \"Casino\" includes Robert De Niro, Joe Pesci, Sharon Stone, and James Woods.\\', \\'steps\\': [\\'guardrail\\',  \\'generate_cypher\\',  \\'validate_cypher\\',  \\'execute_cypher\\',  \\'generate_final_answer\\'], \\'cypher_statement\\': \"MATCH (m:Movie {title: \\'Casino\\'})<-[:ACTED_IN]-(a:Person) RETURN a.name\"}\\nNext steps\\u200b\\nFor other graph techniques like this and more check out:\\n\\nSemantic layer: Techniques for implementing semantic layers.\\nConstructing graphs: Techniques for constructing knowledge graphs.\\nEdit this pagePreviousTutorialsNextTutorials‚ö†Ô∏è Security note ‚ö†Ô∏èArchitectureSetupGraph schemaGraphQACypherChainAdvanced implementation with LangGraphFew-shot promptingQuery validationNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0592e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list_langchain=[item for sublist in docs_langchain for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81305c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Tutorials | ü¶úÔ∏èüîó LangChain', 'description': 'New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTutorials | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsOn this pageTutorials\\nNew to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\\nGet started\\u200b\\nFamiliarize yourself with LangChain\\'s open-source components by building simple applications.\\nIf you\\'re looking to get started with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our supported integrations.\\n\\nChat models and prompts: Build a simple LLM application with prompt templates and chat models.\\nSemantic search: Build a semantic search engine over a PDF with document loaders, embedding models, and vector stores.\\nClassification: Classify text into categories or labels using chat models with structured outputs.\\nExtraction: Extract structured data from text and other unstructured media using chat models and few-shot examples.\\n\\nRefer to the how-to guides for more detail on using all LangChain components.\\nOrchestration\\u200b\\nGet started using LangGraph to assemble LangChain components into full-featured applications.\\n\\nChatbots: Build a chatbot that incorporates memory.\\nAgents: Build an agent that interacts with external tools.\\nRetrieval Augmented Generation (RAG) Part 1: Build an application that uses your own documents to inform its responses.\\nRetrieval Augmented Generation (RAG) Part 2: Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\\nQuestion-Answering with SQL: Build a question-answering system that executes SQL queries to inform its responses.\\nSummarization: Generate summaries of (potentially long) texts.\\nQuestion-Answering with Graph Databases: Build a question-answering system that queries a graph database to inform its responses.\\n\\nLangSmith\\u200b\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse LangSmith tutorials here.\\nEvaluation\\u200b\\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\\n\\nEvaluate your LLM application\\nEdit this pagePreviousIntroductionNextBuild a Question Answering application over a Graph DatabaseGet startedOrchestrationLangSmithEvaluationCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates\\nIn this quickstart we\\'ll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it\\'s just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you\\'ll have a high level overview of:\\n\\n\\nUsing language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet\\'s dive in!\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"export LANGSMITH_PROJECT=\"default\" # or any other project name\\nOr, if in a notebook, you can set them with:\\nimport getpassimport ostry:    # load environment variables from .env file (requires `python-dotenv`)    from dotenv import load_dotenv    load_dotenv()except ImportError:    passos.environ[\"LANGSMITH_TRACING\"] = \"true\"if \"LANGSMITH_API_KEY\" not in os.environ:    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(        prompt=\"Enter your LangSmith API key (optional): \"    )if \"LANGSMITH_PROJECT\" not in os.environ:    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(        prompt=\\'Enter your LangSmith Project Name (default = \"default\"): \\'    )    if not os.environ.get(\"LANGSMITH_PROJECT\"):        os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\\nUsing Language Models\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.\\n\\n\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(content=\"Translate the following from English into Italian\"),    HumanMessage(content=\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage\\nAIMessage(content=\\'Ciao!\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 3, \\'prompt_tokens\\': 20, \\'total_tokens\\': 23, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-32654a56-627c-40e1-a141-ad9350bbfd3e-0\\', usage_metadata={\\'input_tokens\\': 20, \\'output_tokens\\': 3, \\'total_tokens\\': 23, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\ntipIf we\\'ve enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\\nNote that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:\\n\\nlanguage: The language to translate text into\\ntext: The text to translate\\n\\nfrom langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])\\nWe can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\\nThat\\'s it! In this tutorial you\\'ve learned how to create your first simple LLM application. You\\'ve learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we\\'ve got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we\\'ve got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\\n\\nChat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pagePreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/retrievers/', 'title': 'Build a semantic search engine | ü¶úÔ∏èüîó LangChain', 'description': \"This tutorial will familiarize you with LangChain's document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here).\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a semantic search engine | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a semantic search engineOn this pageBuild a semantic search engine\\nThis tutorial will familiarize you with LangChain\\'s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here).\\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.\\nConcepts\\u200b\\nThis guide focuses on retrieval of text data. We will cover the following concepts:\\n\\nDocuments and document loaders;\\nText splitters;\\nEmbeddings;\\nVector stores and retrievers.\\n\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nThis tutorial requires the langchain-community and pypdf packages:\\n\\nPipCondapip install langchain-community pypdfconda install langchain-community pypdf -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nDocuments and Document Loaders\\u200b\\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\\n\\npage_content: a string representing the content;\\nmetadata: a dict containing arbitrary metadata;\\nid: (optional) a string identifier for the document.\\n\\nThe metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\\nWe can generate sample documents when desired:\\nfrom langchain_core.documents import Documentdocuments = [    Document(        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",        metadata={\"source\": \"mammal-pets-doc\"},    ),    Document(        page_content=\"Cats are independent pets that often enjoy their own space.\",        metadata={\"source\": \"mammal-pets-doc\"},    ),]API Reference:Document\\nHowever, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.\\nLoading documents\\u200b\\nLet\\'s load a PDF into a sequence of Document objects. There is a sample PDF in the LangChain repo here -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders. Let\\'s select PyPDFLoader, which is fairly lightweight.\\nfrom langchain_community.document_loaders import PyPDFLoaderfile_path = \"../example_data/nke-10k-2023.pdf\"loader = PyPDFLoader(file_path)docs = loader.load()print(len(docs))\\n107\\ntipSee this guide for more detail on PDF document loaders.\\nPyPDFLoader loads one Document object per PDF page. For each, we can easily access:\\n\\nThe string content of the page;\\nMetadata containing the file name and page number.\\n\\nprint(f\"{docs[0].page_content[:200]}\\\\n\")print(docs[0].metadata)\\nTable of ContentsUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)‚òë ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934FO{\\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'page\\': 0}\\nSplitting\\u200b\\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\\nwith 200 characters of overlap between chunks. The overlap helps\\nmitigate the possibility of separating a statement from important\\ncontext related to it. We use the\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nWe set add_start_index=True so that the character index where each\\nsplit Document starts within the initial Document is preserved as\\nmetadata attribute ‚Äústart_index‚Äù.\\nSee this guide for more detail about working with PDFs, including how to extract text from specific sections and images.\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000, chunk_overlap=200, add_start_index=True)all_splits = text_splitter.split_documents(docs)len(all_splits)\\n514\\nEmbeddings\\u200b\\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\\nLangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let\\'s select a model:\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nvector_1 = embeddings.embed_query(all_splits[0].page_content)vector_2 = embeddings.embed_query(all_splits[1].page_content)assert len(vector_1) == len(vector_2)print(f\"Generated vectors of length {len(vector_1)}\\\\n\")print(vector_1[:10])\\nGenerated vectors of length 1536[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\\nVector stores\\u200b\\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\\nLangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\\'s select a vector store:\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nHaving instantiated our vector store, we can now index the documents.\\nids = vector_store.add_documents(documents=all_splits)\\nNote that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.\\nOnce we\\'ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\\n\\nSynchronously and asynchronously;\\nBy string query and by vector;\\nWith and without returning similarity scores;\\nBy similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).\\n\\nThe methods will generally include a list of Document objects in their outputs.\\nUsage\\u200b\\nEmbeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\\nReturn documents based on similarity to a string query:\\nresults = vector_store.similarity_search(    \"How many distribution centers does Nike have in the US?\")print(results[0])\\npage_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:U.S. RETAIL STORES NUMBERNIKE Brand factory stores 213 NIKE Brand in-line stores (including employee-only stores) 74 Converse stores (including factory stores) 82 TOTAL 369 In the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.2023 FORM 10-K 2\\' metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}\\nAsync query:\\nresults = await vector_store.asimilarity_search(\"When was Nike incorporated?\")print(results[0])\\npage_content=\\'Table of ContentsPART IITEM 1. BUSINESSGENERALNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE isthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail storesand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\' metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\nReturn scores:\\n# Note that providers implement different scores; the score here# is a distance metric that varies inversely with similarity.results = vector_store.similarity_search_with_score(\"What was Nike\\'s revenue in 2023?\")doc, score = results[0]print(f\"Score: {score}\\\\n\")print(doc)\\nScore: 0.23699893057346344page_content=\\'Table of ContentsFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTSThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:FISCAL 2023 COMPARED TO FISCAL 2022‚Ä¢NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.The increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,2 and 1 percentage points to NIKE, Inc. Revenues, respectively.‚Ä¢NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. Thisincrease was primarily due to higher revenues in Men\\'s, the Jordan Brand, Women\\'s and Kids\\' which grew 17%, 35%,11% and 10%, respectively, on a wholesaleequivalent basis.\\' metadata={\\'page\\': 35, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\nReturn documents based on similarity to an embedded query:\\nembedding = embeddings.embed_query(\"How were Nike\\'s margins impacted in 2023?\")results = vector_store.similarity_search_by_vector(embedding)print(results[0])\\npage_content=\\'Table of ContentsGROSS MARGINFISCAL 2023 COMPARED TO FISCAL 2022For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:*Wholesale equivalentThe decrease in gross margin for fiscal 2023 was primarily due to:‚Ä¢Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well asproduct mix;‚Ä¢Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity inthe prior period resulting from lower available inventory supply;‚Ä¢Unfavorable changes in net foreign currency exchange rates, including hedges; and‚Ä¢Lower off-price margin, on a wholesale equivalent basis.This was partially offset by:\\' metadata={\\'page\\': 36, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\nLearn more:\\n\\nAPI reference\\nHow-to guide\\nIntegration-specific docs\\n\\nRetrievers\\u200b\\nLangChain VectorStore objects do not subclass Runnable. LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\\nWe can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method:\\nfrom typing import Listfrom langchain_core.documents import Documentfrom langchain_core.runnables import chain@chaindef retriever(query: str) -> List[Document]:    return vector_store.similarity_search(query, k=1)retriever.batch(    [        \"How many distribution centers does Nike have in the US?\",        \"When was Nike incorporated?\",    ],)API Reference:Document | chain\\n[[Document(metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}, page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\\\nU.S. RETAIL STORES NUMBER\\\\nNIKE Brand factory stores 213 \\\\nNIKE Brand in-line stores (including employee-only stores) 74 \\\\nConverse stores (including factory stores) 82 \\\\nTOTAL 369 \\\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\\\n2023 FORM 10-K 2\\')], [Document(metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}, page_content=\\'Table of Contents\\\\nPART I\\\\nITEM 1. BUSINESS\\\\nGENERAL\\\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\')]]\\nVectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\\nretriever = vector_store.as_retriever(    search_type=\"similarity\",    search_kwargs={\"k\": 1},)retriever.batch(    [        \"How many distribution centers does Nike have in the US?\",        \"When was Nike incorporated?\",    ],)\\n[[Document(metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}, page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\\\nU.S. RETAIL STORES NUMBER\\\\nNIKE Brand factory stores 213 \\\\nNIKE Brand in-line stores (including employee-only stores) 74 \\\\nConverse stores (including factory stores) 82 \\\\nTOTAL 369 \\\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\\\n2023 FORM 10-K 2\\')], [Document(metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}, page_content=\\'Table of Contents\\\\nPART I\\\\nITEM 1. BUSINESS\\\\nGENERAL\\\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\')]]\\nVectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\". We can use the latter to threshold documents output by the retriever by similarity score.\\nRetrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial.\\nLearn more:\\u200b\\nRetrieval strategies can be rich and complex. For example:\\n\\nWe can infer hard rules and filters from a query (e.g., \"using documents published after 2020\");\\nWe can return documents that are linked to the retrieved context in some way (e.g., via some document taxonomy);\\nWe can generate multiple embeddings for each unit of context;\\nWe can ensemble results from multiple retrievers;\\nWe can assign weights to documents, e.g., to weigh recent documents higher.\\n\\nThe retrievers section of the how-to guides covers these and other built-in retrieval strategies.\\nIt is also straightforward to extend the BaseRetriever class in order to implement custom retrievers. See our how-to guide here.\\nNext steps\\u200b\\nYou\\'ve now seen how to build a semantic search engine over a PDF document.\\nFor more on document loaders:\\n\\nConceptual guide\\nHow-to guides\\nAvailable integrations\\n\\nFor more on embeddings:\\n\\nConceptual guide\\nHow-to guides\\nAvailable integrations\\n\\nFor more on vector stores:\\n\\nConceptual guide\\nHow-to guides\\nAvailable integrations\\n\\nFor more on RAG, see:\\n\\nBuild a Retrieval Augmented Generation (RAG) App\\nRelated how-to guides\\nEdit this pagePreviousBuild a Retrieval Augmented Generation (RAG) App: Part 1NextBuild a Question/Answering system over SQL dataConceptsSetupJupyter NotebookInstallationLangSmithDocuments and Document LoadersLoading documentsSplittingEmbeddingsVector storesUsageRetrieversLearn more:Next stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/classification/', 'title': 'Tagging | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTagging | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsTaggingOn this page\\nClassify Text into Labels\\nTagging means labeling a document with classes such as:\\n\\nSentiment\\nLanguage\\nStyle (formal, informal etc.)\\nCovered topics\\nPolitical tendency\\n\\n\\nOverview\\u200b\\nTagging has a few components:\\n\\nfunction: Like extraction, tagging uses functions to specify how the model should tag a document\\nschema: defines how we want to tag the document\\n\\nQuickstart\\u200b\\nLet\\'s see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We\\'ll use the with_structured_output method supported by OpenAI models.\\npip install -U langchain-core\\nWe\\'ll need to load a chat model:\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s specify a Pydantic model with a few properties and their expected type in our schema.\\nfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldtagging_prompt = ChatPromptTemplate.from_template(    \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the \\'Classification\\' function.Passage:{input}\"\"\")class Classification(BaseModel):    sentiment: str = Field(description=\"The sentiment of the text\")    aggressiveness: int = Field(        description=\"How aggressive the text is on a scale from 1 to 10\"    )    language: str = Field(description=\"The language the text is written in\")# Structured LLMstructured_llm = llm.with_structured_output(Classification)API Reference:ChatPromptTemplate\\ninp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response\\nClassification(sentiment=\\'positive\\', aggressiveness=1, language=\\'Spanish\\')\\nIf we want dictionary output, we can just call .model_dump()\\ninp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response.model_dump()\\n{\\'sentiment\\': \\'angry\\', \\'aggressiveness\\': 8, \\'language\\': \\'Spanish\\'}\\nAs we can see in the examples, it correctly interprets what we want.\\nThe results vary so that we may get, for example, sentiments in different languages (\\'positive\\', \\'enojado\\' etc.).\\nWe will see how to control these results in the next section.\\nFiner control\\u200b\\nCareful schema definition gives us more control over the model\\'s output.\\nSpecifically, we can define:\\n\\nPossible values for each property\\nDescription to make sure that the model understands the property\\nRequired properties to be returned\\n\\nLet\\'s redeclare our Pydantic model to control for each of the previously mentioned aspects using enums:\\nclass Classification(BaseModel):    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])    aggressiveness: int = Field(        ...,        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",        enum=[1, 2, 3, 4, 5],    )    language: str = Field(        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]    )\\ntagging_prompt = ChatPromptTemplate.from_template(    \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the \\'Classification\\' function.Passage:{input}\"\"\")structured_llm = llm.with_structured_output(Classification)\\nNow the answers will be restricted in a way we expect!\\ninp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\\nClassification(sentiment=\\'happy\\', aggressiveness=1, language=\\'spanish\\')\\ninp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\\nClassification(sentiment=\\'sad\\', aggressiveness=4, language=\\'spanish\\')\\ninp = \"Weather is ok here, I can go outside without much more than a coat\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\\nClassification(sentiment=\\'happy\\', aggressiveness=1, language=\\'english\\')\\nThe LangSmith trace lets us peek under the hood:\\n\\nGoing deeper\\u200b\\n\\nYou can use the metadata tagger document transformer to extract metadata from a LangChain Document.\\nThis covers the same basic functionality as the tagging chain, only applied to a LangChain Document.\\nEdit this pagePreviousBuild an AgentNextBuild a Retrieval Augmented Generation (RAG) App: Part 1OverviewQuickstartFiner controlGoing deeperCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/extraction/', 'title': 'Build an Extraction Chain | ü¶úÔ∏èüîó LangChain', 'description': 'In this tutorial, we will use tool-calling features of chat models to extract structured information from unstructured text. We will also demonstrate how to use few-shot prompting in this context to improve performance.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild an Extraction Chain | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild an Extraction ChainOn this pageBuild an Extraction Chain\\nIn this tutorial, we will use tool-calling features of chat models to extract structured information from unstructured text. We will also demonstrate how to use few-shot prompting in this context to improve performance.\\nimportantThis tutorial requires langchain-core>=0.3.20 and will only work with models that support tool calling.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install --upgrade langchain-coreconda install langchain-core -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nThe Schema\\u200b\\nFirst, we need to describe what information we want to extract from the text.\\nWe\\'ll use Pydantic to define an example schema  to extract personal information.\\nfrom typing import Optionalfrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    # ^ Doc-string for the entity Person.    # This doc-string is sent to the LLM as the description of the schema Person,    # and it can help to improve extraction results.    # Note that:    # 1. Each field is an `optional` -- this allows the model to decline to extract it!    # 2. Each field has a `description` -- this description is used by the LLM.    # Having a good description can help improve extraction results.    name: Optional[str] = Field(default=None, description=\"The name of the person\")    hair_color: Optional[str] = Field(        default=None, description=\"The color of the person\\'s hair if known\"    )    height_in_meters: Optional[str] = Field(        default=None, description=\"Height measured in meters\"    )\\nThere are two best practices when defining schema:\\n\\nDocument the attributes and the schema itself: This information is sent to the LLM and is used to improve the quality of information extraction.\\nDo not force the LLM to make up information! Above we used Optional for the attributes allowing the LLM to output None if it doesn\\'t know the answer.\\n\\nimportantFor best performance, document the schema well and make sure the model isn\\'t forced to return results if there\\'s no information to be extracted in the text.\\nThe Extractor\\u200b\\nLet\\'s create an information extractor using the schema we defined above.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder# Define a custom prompt to provide instructions and any additional context.# 1) You can add examples into the prompt template to improve extraction quality# 2) Introduce additional parameters to take context into account (e.g., include metadata#    about the document from which the text was extracted.)prompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are an expert extraction algorithm. \"            \"Only extract relevant information from the text. \"            \"If you do not know the value of an attribute asked to extract, \"            \"return null for the attribute\\'s value.\",        ),        # Please see the how-to about improving performance with        # reference examples.        # MessagesPlaceholder(\\'examples\\'),        (\"human\", \"{text}\"),    ])API Reference:ChatPromptTemplate | MessagesPlaceholder\\nWe need to use a model that supports function/tool calling.\\nPlease review the documentation for all models that can be used with this API.\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nstructured_llm = llm.with_structured_output(schema=Person)\\nLet\\'s test it out:\\ntext = \"Alan Smith is 6 feet tall and has blond hair.\"prompt = prompt_template.invoke({\"text\": text})structured_llm.invoke(prompt)\\nPerson(name=\\'Alan Smith\\', hair_color=\\'blond\\', height_in_meters=\\'1.83\\')\\nimportantExtraction is Generative ü§ØLLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters\\neven though it was provided in feet!\\nWe can see the LangSmith trace here. Note that the chat model portion of the trace reveals the exact sequence of messages sent to the model, tools invoked, and other metadata.\\nMultiple Entities\\u200b\\nIn most cases, you should be extracting a list of entities rather than a single entity.\\nThis can be easily achieved using pydantic by nesting models inside one another.\\nfrom typing import List, Optionalfrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    # ^ Doc-string for the entity Person.    # This doc-string is sent to the LLM as the description of the schema Person,    # and it can help to improve extraction results.    # Note that:    # 1. Each field is an `optional` -- this allows the model to decline to extract it!    # 2. Each field has a `description` -- this description is used by the LLM.    # Having a good description can help improve extraction results.    name: Optional[str] = Field(default=None, description=\"The name of the person\")    hair_color: Optional[str] = Field(        default=None, description=\"The color of the person\\'s hair if known\"    )    height_in_meters: Optional[str] = Field(        default=None, description=\"Height measured in meters\"    )class Data(BaseModel):    \"\"\"Extracted data about people.\"\"\"    # Creates a model so that we can extract multiple entities.    people: List[Person]\\nimportantExtraction results might not be perfect here. Read on to see how to use Reference Examples to improve the quality of extraction, and check out our extraction how-to guides for more detail.\\nstructured_llm = llm.with_structured_output(schema=Data)text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"prompt = prompt_template.invoke({\"text\": text})structured_llm.invoke(prompt)\\nData(people=[Person(name=\\'Jeff\\', hair_color=\\'black\\', height_in_meters=\\'1.83\\'), Person(name=\\'Anna\\', hair_color=\\'black\\', height_in_meters=None)])\\ntipWhen the schema accommodates the extraction of multiple entities, it also allows the model to extract no entities if no relevant information\\nis in the text by providing an empty list.This is usually a good thing! It allows specifying required attributes on an entity without necessarily forcing the model to detect this entity.\\nWe can see the LangSmith trace here.\\nReference examples\\u200b\\nThe behavior of LLM applications can be steered using few-shot prompting. For chat models, this can take the form of a sequence of pairs of input and response messages demonstrating desired behaviors.\\nFor example, we can convey the meaning of a symbol with alternating user and assistant messages:\\nmessages = [    {\"role\": \"user\", \"content\": \"2 ü¶ú 2\"},    {\"role\": \"assistant\", \"content\": \"4\"},    {\"role\": \"user\", \"content\": \"2 ü¶ú 3\"},    {\"role\": \"assistant\", \"content\": \"5\"},    {\"role\": \"user\", \"content\": \"3 ü¶ú 4\"},]response = llm.invoke(messages)print(response.content)\\n7\\nStructured output often uses tool calling under-the-hood. This typically involves the generation of AI messages containing tool calls, as well as tool messages containing the results of tool calls. What should a sequence of messages look like in this case?\\nDifferent chat model providers impose different requirements for valid message sequences. Some will accept a (repeating) message sequence of the form:\\n\\nUser message\\nAI message with tool call\\nTool message with result\\n\\nOthers require a final AI message containing some sort of response.\\nLangChain includes a utility function tool_example_to_messages that will generate a valid sequence for most model providers. It simplifies the generation of structured few-shot examples by just requiring Pydantic representations of the corresponding tool calls.\\nLet\\'s try this out. We can convert pairs of input strings and desired Pydantic objects to a sequence of messages that can be provided to a chat model. Under the hood, LangChain will format the tool calls to each provider\\'s required format.\\nNote: this version of tool_example_to_messages requires langchain-core>=0.3.20.\\nfrom langchain_core.utils.function_calling import tool_example_to_messagesexamples = [    (        \"The ocean is vast and blue. It\\'s more than 20,000 feet deep.\",        Data(people=[]),    ),    (        \"Fiona traveled far from France to Spain.\",        Data(people=[Person(name=\"Fiona\", height_in_meters=None, hair_color=None)]),    ),]messages = []for txt, tool_call in examples:    if tool_call.people:        # This final message is optional for some providers        ai_response = \"Detected people.\"    else:        ai_response = \"Detected no people.\"    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))API Reference:tool_example_to_messages\\nInspecting the result, we see these two example pairs generated eight messages:\\nfor message in messages:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================The ocean is vast and blue. It\\'s more than 20,000 feet deep.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  Data (d8f2e054-7fb9-417f-b28f-0447a775b2c3) Call ID: d8f2e054-7fb9-417f-b28f-0447a775b2c3  Args:    people: []=================================\\x1b[1m Tool Message \\x1b[0m=================================You have correctly called this tool.==================================\\x1b[1m Ai Message \\x1b[0m==================================Detected no people.================================\\x1b[1m Human Message \\x1b[0m=================================Fiona traveled far from France to Spain.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  Data (0178939e-a4b1-4d2a-a93e-b87f665cdfd6) Call ID: 0178939e-a4b1-4d2a-a93e-b87f665cdfd6  Args:    people: [{\\'name\\': \\'Fiona\\', \\'hair_color\\': None, \\'height_in_meters\\': None}]=================================\\x1b[1m Tool Message \\x1b[0m=================================You have correctly called this tool.==================================\\x1b[1m Ai Message \\x1b[0m==================================Detected people.\\nLet\\'s compare performance with and without these messages. For example, let\\'s pass a message for which we intend no people to be extracted:\\nmessage_no_extraction = {    \"role\": \"user\",    \"content\": \"The solar system is large, but earth has only 1 moon.\",}structured_llm = llm.with_structured_output(schema=Data)structured_llm.invoke([message_no_extraction])\\nData(people=[Person(name=\\'Earth\\', hair_color=\\'None\\', height_in_meters=\\'0.00\\')])\\nIn this example, the model is liable to erroneously generate records of people.\\nBecause our few-shot examples contain examples of \"negatives\", we encourage the model to behave correctly in this case:\\nstructured_llm.invoke(messages + [message_no_extraction])\\nData(people=[])\\ntipThe LangSmith trace for the run reveals the exact sequence of messages sent to the chat model, tool calls generated, latency, token counts, and other metadata.\\nSee this guide for more detail on extraction workflows with reference examples, including how to incorporate prompt templates and customize the generation of example messages.\\nNext steps\\u200b\\nNow that you understand the basics of extraction with LangChain, you\\'re ready to proceed to the rest of the how-to guides:\\n\\nAdd Examples: More detail on using reference examples to improve performance.\\nHandle Long Text: What should you do if the text does not fit into the context window of the LLM?\\nUse a Parsing Approach: Use a prompt based approach to extract with models that do not support tool/function calling.\\nEdit this pagePreviousBuild a Retrieval Augmented Generation (RAG) App: Part 2NextBuild an AgentSetupJupyter NotebookInstallationLangSmithThe SchemaThe ExtractorMultiple EntitiesReference examplesNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | ü¶úÔ∏èüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Chatbot | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a ChatbotOn this pageBuild a Chatbot\\nnoteThis tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nOverview\\u200b\\nWe\\'ll go over an example of how to design and implement an LLM-powered chatbot.\\nThis chatbot will be able to have a conversation and remember previous interactions with a chat model.\\nNote that this chatbot that we build will only use the language model to have a conversation.\\nThere are several other related concepts that you may be looking for:\\n\\nConversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nThis tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nFor this tutorial we will need langchain-core and langgraph. This guide requires langgraph >= 0.2.28.\\n\\nPipCondapip install langchain-core langgraph>0.2.27conda install langchain-core langgraph>0.2.27 -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, (you\\'ll need to create an API key from the Settings -> API Keys page on the LangSmith website), make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nQuickstart\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hi! I\\'m Bob\")])API Reference:HumanMessage\\nAIMessage(content=\\'Hi Bob! How can I assist you today?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 10, \\'prompt_tokens\\': 11, \\'total_tokens\\': 21, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 10, \\'total_tokens\\': 21, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nThe model on its own does not have any concept of state. For example, if you ask a followup question:\\nmodel.invoke([HumanMessage(content=\"What\\'s my name?\")])\\nAIMessage(content=\"I\\'m sorry, but I don\\'t have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 34, \\'prompt_tokens\\': 11, \\'total_tokens\\': 45, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-a2d13a18-7022-4784-b54f-f85c097d1075-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 34, \\'total_tokens\\': 45, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nLet\\'s take a look at the example LangSmith trace\\nWe can see that it doesn\\'t take the previous conversation turn into context, and cannot answer the question.\\nThis makes for a terrible chatbot experience!\\nTo get around this, we need to pass the entire conversation history into the model. Let\\'s see what happens when we do that:\\nfrom langchain_core.messages import AIMessagemodel.invoke(    [        HumanMessage(content=\"Hi! I\\'m Bob\"),        AIMessage(content=\"Hello Bob! How can I assist you today?\"),        HumanMessage(content=\"What\\'s my name?\"),    ])API Reference:AIMessage\\nAIMessage(content=\\'Your name is Bob! How can I help you today, Bob?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 14, \\'prompt_tokens\\': 33, \\'total_tokens\\': 47, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-34bcccb3-446e-42f2-b1de-52c09936c02c-0\\', usage_metadata={\\'input_tokens\\': 33, \\'output_tokens\\': 14, \\'total_tokens\\': 47, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nAnd now we can see that we get a good response!\\nThis is the basic idea underpinning a chatbot\\'s ability to interact conversationally.\\nSo how do we best implement this?\\nMessage persistence\\u200b\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\nfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define a new graphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    response = model.invoke(state[\"messages\"])    return {\"messages\": response}# Define the (single) node in the graphworkflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)# Add memorymemory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:MemorySaver | StateGraph\\nWe now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like:\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\\nThis enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\\nWe can then invoke the application:\\nquery = \"Hi! I\\'m Bob.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()  # output contains all messages in state\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Hi Bob! How can I assist you today?\\nquery = \"What\\'s my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob! How can I help you today, Bob?\\nGreat! Our chatbot now remembers things about us. If we change the config to reference a different thread_id, we can see that it starts the conversation fresh.\\nconfig = {\"configurable\": {\"thread_id\": \"abc234\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================I\\'m sorry, but I don\\'t have access to personal information about you unless you\\'ve shared it in this conversation. How can I assist you today?\\nHowever, we can always go back to the original conversation (since we are persisting it in a database)\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob. What would you like to discuss today?\\nThis is how we can support a chatbot having conversations with many users!\\ntipFor async support, update the call_model node to be an async function and use .ainvoke when invoking the application:# Async function for node:async def call_model(state: MessagesState):    response = await model.ainvoke(state[\"messages\"])    return {\"messages\": response}# Define graph as before:workflow = StateGraph(state_schema=MessagesState)workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)app = workflow.compile(checkpointer=MemorySaver())# Async invocation:output = await app.ainvoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\nRight now, all we\\'ve done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.\\nPrompt templates\\u200b\\nPrompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let\\'s now make that a bit more complicated. First, let\\'s add in a system message with some custom instructions (but still taking messages as input). Next, we\\'ll add in more input besides just the messages.\\nTo add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You talk like a pirate. Answer all questions to the best of your ability.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])API Reference:ChatPromptTemplate | MessagesPlaceholder\\nWe can now update our application to incorporate this template:\\nworkflow = StateGraph(state_schema=MessagesState)def call_model(state: MessagesState):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": response}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nWe invoke the application in the same way:\\nconfig = {\"configurable\": {\"thread_id\": \"abc345\"}}query = \"Hi! I\\'m Jim.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ahoy there, Jim! What brings ye to these waters today? Be ye seekin\\' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!\\nAwesome! Let\\'s now make our prompt a little bit more complicated. Let\\'s assume that the prompt template now looks something like this:\\nprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])\\nNote that we have added a new language input to the prompt. Our application now has two parameters-- the input messages and language. We should update our application\\'s state to reflect this:\\nfrom typing import Sequencefrom langchain_core.messages import BaseMessagefrom langgraph.graph.message import add_messagesfrom typing_extensions import Annotated, TypedDictclass State(TypedDict):    messages: Annotated[Sequence[BaseMessage], add_messages]    language: strworkflow = StateGraph(state_schema=State)def call_model(state: State):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:BaseMessage | add_messages\\nconfig = {\"configurable\": {\"thread_id\": \"abc456\"}}query = \"Hi! I\\'m Bob.\"language = \"Spanish\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================¬°Hola, Bob! ¬øC√≥mo puedo ayudarte hoy?\\nNote that the entire state is persisted, so we can omit parameters like language if no changes are desired:\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Tu nombre es Bob. ¬øHay algo m√°s en lo que pueda ayudarte?\\nTo help you understand what\\'s happening internally, check out this LangSmith trace.\\nManaging Conversation History\\u200b\\nOne important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\\nImportantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\\nWe can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the Message History class.\\nLangChain comes with a few built-in helpers for managing a list of messages. In this case we\\'ll use the trim_messages helper to reduce how many messages we\\'re sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:\\nfrom langchain_core.messages import SystemMessage, trim_messagestrimmer = trim_messages(    max_tokens=65,    strategy=\"last\",    token_counter=model,    include_system=True,    allow_partial=False,    start_on=\"human\",)messages = [    SystemMessage(content=\"you\\'re a good assistant\"),    HumanMessage(content=\"hi! I\\'m bob\"),    AIMessage(content=\"hi!\"),    HumanMessage(content=\"I like vanilla ice cream\"),    AIMessage(content=\"nice\"),    HumanMessage(content=\"whats 2 + 2\"),    AIMessage(content=\"4\"),    HumanMessage(content=\"thanks\"),    AIMessage(content=\"no problem!\"),    HumanMessage(content=\"having fun?\"),    AIMessage(content=\"yes!\"),]trimmer.invoke(messages)API Reference:SystemMessage | trim_messages\\n[SystemMessage(content=\"you\\'re a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'whats 2 + 2\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'4\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'thanks\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'no problem!\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'having fun?\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'yes!\\', additional_kwargs={}, response_metadata={})]\\nTo  use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\\nworkflow = StateGraph(state_schema=State)def call_model(state: State):    print(f\"Messages before trimming: {len(state[\\'messages\\'])}\")    trimmed_messages = trimmer.invoke(state[\"messages\"])    print(f\"Messages after trimming: {len(trimmed_messages)}\")    print(\"Remaining messages:\")    for msg in trimmed_messages:        print(f\"  {type(msg).__name__}: {msg.content}\")    prompt = prompt_template.invoke(        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}    )    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nNow if we try asking the model our name, it won\\'t know it since we trimmed that part of the chat history. (By defining our trim stragegy as \\'last\\', we are only keeping the most recent messages that fit within the max_tokens.)\\nconfig = {\"configurable\": {\"thread_id\": \"abc567\"}}query = \"What is my name?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\nMessages before trimming: 12Messages after trimming: 8Remaining messages:  SystemMessage: you\\'re a good assistant  HumanMessage: whats 2 + 2  AIMessage: 4  HumanMessage: thanks  AIMessage: no problem!  HumanMessage: having fun?  AIMessage: yes!  HumanMessage: What is my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================I don\\'t know your name. If you\\'d like to share it, feel free!\\nBut if we ask about information that is within the last few messages, it remembers:\\nconfig = {\"configurable\": {\"thread_id\": \"abc678\"}}query = \"What math problem was asked?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\nMessages before trimming: 12Messages after trimming: 8Remaining messages:  SystemMessage: you\\'re a good assistant  HumanMessage: whats 2 + 2  AIMessage: 4  HumanMessage: thanks  AIMessage: no problem!  HumanMessage: having fun?  AIMessage: yes!  HumanMessage: What math problem was asked?==================================\\x1b[1m Ai Message \\x1b[0m==================================The math problem that was asked was \"what\\'s 2 + 2.\"\\nIf you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace.\\nStreaming\\u200b\\nNow we\\'ve got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\\nIt\\'s actually super easy to do this!\\nBy default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead:\\nconfig = {\"configurable\": {\"thread_id\": \"abc789\"}}query = \"Hi I\\'m Todd, please tell me a joke.\"language = \"English\"input_messages = [HumanMessage(query)]for chunk, metadata in app.stream(    {\"messages\": input_messages, \"language\": language},    config,    stream_mode=\"messages\",):    if isinstance(chunk, AIMessage):  # Filter to just model responses        print(chunk.content, end=\"|\")\\n|Hi| Todd|!| Here|‚Äôs| a| joke| for| you|:|Why| don\\'t| scientists| trust| atoms|?|Because| they| make| up| everything|!||\\nNext Steps\\u200b\\nNow that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:\\n\\nConversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nIf you want to dive deeper on specifics, some things worth checking out are:\\n\\nStreaming: streaming is crucial for chat applications\\nHow to add message history: for a deeper dive into all things related to message history\\nHow to manage large message history: more techniques for managing a large chat history\\nLangGraph main docs: for more detail on building with LangGraph\\nEdit this pagePreviousBuild a simple LLM application with chat models and prompt templatesNextBuild a Retrieval Augmented Generation (RAG) App: Part 2OverviewSetupJupyter NotebookInstallationLangSmithQuickstartMessage persistencePrompt templatesManaging Conversation HistoryStreamingNext StepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/agents/', 'title': 'Build an Agent | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain supports the creation of agents, or systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild an Agent | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild an AgentOn this pageBuild an Agent\\nLangChain supports the creation of agents, or systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action.\\nAfter executing actions, the results can be fed back into the LLM to determine whether more actions are needed, or whether it is okay to finish. This is often achieved via tool-calling.\\nIn this tutorial we will build an agent that can interact with a search engine. You will be able to ask this agent questions, watch it call the search tool, and have conversations with it.\\nEnd-to-end agent\\u200b\\nThe code snippet below represents a fully functional agent that uses an LLM to decide which tools to use. It is equipped with a generic search tool. It has conversational memory - meaning that it can be used as a multi-turn chatbot.\\nIn the rest of the guide, we will walk through the individual components and what each part does - but if you want to just grab some code and get started, feel free to use this!\\n# Import relevant functionalityfrom langchain.chat_models import init_chat_modelfrom langchain_tavily import TavilySearchfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.prebuilt import create_react_agent# Create the agentmemory = MemorySaver()model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")search = TavilySearch(max_results=2)tools = [search]agent_executor = create_react_agent(model, tools, checkpointer=memory)API Reference:MemorySaver | create_react_agent\\n# Use the agentconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_message = {    \"role\": \"user\",    \"content\": \"Hi, I\\'m Bob and I live in SF.\",}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi, I\\'m Bob and I live in SF.==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello Bob! I notice you\\'ve introduced yourself and mentioned you live in SF (San Francisco), but you haven\\'t asked a specific question or made a request that requires the use of any tools. Is there something specific you\\'d like to know about San Francisco or any other topic? I\\'d be happy to help you find information using the available search tools.\\ninput_message = {    \"role\": \"user\",    \"content\": \"What\\'s the weather where I live?\",}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s the weather where I live?==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \\'Let me search for current weather information in San Francisco.\\', \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_011kSdheoJp8THURoLmeLtZo\\', \\'input\\': {\\'query\\': \\'current weather San Francisco CA\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_011kSdheoJp8THURoLmeLtZo) Call ID: toolu_011kSdheoJp8THURoLmeLtZo  Args:    query: current weather San Francisco CA=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco CA\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.944705, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.86441374, \"raw_content\": null}], \"response_time\": 2.34}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9 milesThis is quite typical weather for San Francisco, with the characteristic fog that the city is known for. Would you like to know anything else about the weather or San Francisco in general?\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect interactive environments for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc), and observing these cases is a great way to better understand building with LLMs.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n%pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nTavily\\u200b\\nWe will be using Tavily (a search engine) as a tool.\\nIn order to use it, you will need to get and set an API key:\\nexport TAVILY_API_KEY=\"...\"\\nOr, if in a notebook, you can set it with:\\nimport getpassimport osos.environ[\"TAVILY_API_KEY\"] = getpass.getpass()\\nDefine tools\\u200b\\nWe first need to create the tools we want to use. Our main tool of choice will be Tavily - a search engine. We can use the dedicated langchain-tavily integration package to easily use Tavily search engine as tool with LangChain.\\nfrom langchain_tavily import TavilySearchsearch = TavilySearch(max_results=2)search_results = search.invoke(\"What is the weather in SF\")print(search_results)# If we want, we can create other tools.# Once we have all the tools we want, we can put them in a list that we will reference later.tools = [search]\\n{\\'query\\': \\'What is the weather in SF\\', \\'follow_up_questions\\': None, \\'answer\\': None, \\'images\\': [], \\'results\\': [{\\'title\\': \\'Weather in San Francisco, CA\\', \\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \\'score\\': 0.9185379, \\'raw_content\\': None}, {\\'title\\': \\'Weather in San Francisco in June 2025\\', \\'url\\': \\'https://world-weather.info/forecast/usa/san_francisco/june-2025/\\', \\'content\\': \"Weather in San Francisco in June 2025 (California) - Detailed Weather Forecast for a Month *   Weather in San Francisco Weather in San Francisco in June 2025 *   1 +63¬∞ +55¬∞ *   2 +66¬∞ +54¬∞ *   3 +66¬∞ +55¬∞ *   4 +66¬∞ +54¬∞ *   5 +66¬∞ +55¬∞ *   6 +66¬∞ +57¬∞ *   7 +64¬∞ +55¬∞ *   8 +63¬∞ +55¬∞ *   9 +63¬∞ +54¬∞ *   10 +59¬∞ +54¬∞ *   11 +59¬∞ +54¬∞ *   12 +61¬∞ +54¬∞ Weather in Washington, D.C.**+68¬∞** Sacramento**+81¬∞** Pleasanton**+72¬∞** Redwood City**+68¬∞** San Leandro**+61¬∞** San Mateo**+64¬∞** San Rafael**+70¬∞** San Ramon**+64¬∞** South San Francisco**+61¬∞** Daly City**+59¬∞** Wilder**+66¬∞** Woodacre**+70¬∞** world\\'s temperature today Colchani day+50¬∞F night+16¬∞F Az Zubayr day+124¬∞F night+93¬∞F Weather forecast on your site Install _San Francisco_ +61¬∞ Temperature units\", \\'score\\': 0.7978881, \\'raw_content\\': None}], \\'response_time\\': 2.62}\\ntipIn many applications, you may want to define custom tools. LangChain supports custom\\ntool creation via Python functions and other means. Refer to the\\nHow to create tools guide for details.\\nUsing Language Models\\u200b\\nNext, let\\'s learn how to use a language model to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nYou can call the language model by passing in a list of messages. By default, the response is a content string.\\nquery = \"Hi!\"response = model.invoke([{\"role\": \"user\", \"content\": query}])response.text()\\n\\'Hello! How can I help you today?\\'\\nWe can now see what it is like to enable this model to do tool calling. In order to enable that we use .bind_tools to give the language model knowledge of these tools\\nmodel_with_tools = model.bind_tools(tools)\\nWe can now call the model. Let\\'s first call it with a normal message, and see how it responds. We can look at both the content field as well as the tool_calls field.\\nquery = \"Hi!\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\\\n\")print(f\"Tool calls: {response.tool_calls}\")\\nMessage content: Hello! I\\'m here to help you. I have access to a powerful search tool that can help answer questions and find information about various topics. What would you like to know about?Feel free to ask any question or request information, and I\\'ll do my best to assist you using the available tools.Tool calls: []\\nNow, let\\'s try calling it with some input that would expect a tool to be called.\\nquery = \"Search for the weather in SF\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\\\n\")print(f\"Tool calls: {response.tool_calls}\")\\nMessage content: I\\'ll help you search for information about the weather in San Francisco.Tool calls: [{\\'name\\': \\'tavily_search\\', \\'args\\': {\\'query\\': \\'current weather San Francisco\\'}, \\'id\\': \\'toolu_015gdPn1jbB2Z21DmN2RAnti\\', \\'type\\': \\'tool_call\\'}]\\nWe can see that there\\'s now no text content, but there is a tool call! It wants us to call the Tavily Search tool.\\nThis isn\\'t calling that tool yet - it\\'s just telling us to. In order to actually call it, we\\'ll want to create our agent.\\nCreate the agent\\u200b\\nNow that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent.\\nCurrently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.\\nNow, we can initialize the agent with the LLM and the tools.\\nNote that we are passing in the model, not model_with_tools. That is because create_react_agent will call .bind_tools for us under the hood.\\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(model, tools)API Reference:create_react_agent\\nRun the agent\\u200b\\nWe can now run the agent with a few queries! Note that for now, these are all stateless queries (it won\\'t remember previous interactions). Note that the agent will return the final state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs).\\nFirst up, let\\'s see how it responds when there\\'s no need to call a tool:\\ninput_message = {\"role\": \"user\", \"content\": \"Hi!\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi!==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello! I\\'m here to help you with your questions using the available search tools. Please feel free to ask any question, and I\\'ll do my best to find relevant and accurate information for you.\\nIn order to see exactly what is happening under the hood (and to make sure it\\'s not calling a tool) we can take a look at the LangSmith trace\\nLet\\'s now try it out on an example where it should be invoking the tool\\ninput_message = {\"role\": \"user\", \"content\": \"Search for the weather in SF\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Search for the weather in SF==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \"I\\'ll help you search for weather information in San Francisco. Let me use the search engine to find current weather conditions.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01WWcXGnArosybujpKzdmARZ\\', \\'input\\': {\\'query\\': \\'current weather San Francisco SF\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_01WWcXGnArosybujpKzdmARZ) Call ID: toolu_01WWcXGnArosybujpKzdmARZ  Args:    query: current weather San Francisco SF=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco SF\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.885373, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.8830044, \"raw_content\": null}], \"response_time\": 2.6}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Conditions: Foggy- Wind: 4.0 mph from the SW- Humidity: 86%- Visibility: 9.0 milesThe weather appears to be typical for San Francisco, with morning fog and mild temperatures. The \"feels like\" temperature is 52.4¬∞F (11.3¬∞C).\\nWe can check out the LangSmith trace to make sure it\\'s calling the search tool effectively.\\nStreaming Messages\\u200b\\nWe\\'ve seen how the agent can be called with .invoke to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\\nfor step in agent_executor.stream({\"messages\": [input_message]}, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Search for the weather in SF==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \"I\\'ll help you search for information about the weather in San Francisco.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01DCPnJES53Fcr7YWnZ47kDG\\', \\'input\\': {\\'query\\': \\'current weather San Francisco\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_01DCPnJES53Fcr7YWnZ47kDG) Call ID: toolu_01DCPnJES53Fcr7YWnZ47kDG  Args:    query: current weather San Francisco=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168506, \\'localtime\\': \\'2025-06-17 06:55\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.9542825, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.8638634, \"raw_content\": null}], \"response_time\": 2.57}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9.0 miles- Feels like: 52.4¬∞F (11.3¬∞C)This is quite typical weather for San Francisco, which is known for its fog, especially during the morning hours. The city\\'s proximity to the ocean and unique geographical features often result in mild temperatures and foggy conditions.\\nStreaming tokens\\u200b\\nIn addition to streaming back messages, it is also useful to stream back tokens.\\nWe can do this by specifying stream_mode=\"messages\".\\n::: note\\nBelow we use message.text(), which requires langchain-core>=0.3.37.\\n:::\\nfor step, metadata in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"messages\"):    if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()):        print(text, end=\"|\")\\nI|\\'ll help you search for information| about the weather in San Francisco.|Base|d on the search results, here|\\'s the current weather in| San Francisco:-| Temperature: 53.1¬∞F (|11.7¬∞C)-| Condition: Foggy- Wind:| 4.0 mph from| the Southwest- Humidity|: 86%|- Visibility: 9|.0 miles- Pressure: |30.02 in|HgThe weather| is characteristic of San Francisco, with| foggy conditions and mild temperatures|. The \"feels like\" temperature is slightly| lower at 52.4|¬∞F (11.|3¬∞C)| due to the wind chill effect|.|\\nAdding in memory\\u200b\\nAs mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a thread_id when invoking the agent (so it knows which thread/conversation to resume from).\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()API Reference:MemorySaver\\nagent_executor = create_react_agent(model, tools, checkpointer=memory)config = {\"configurable\": {\"thread_id\": \"abc123\"}}\\ninput_message = {\"role\": \"user\", \"content\": \"Hi, I\\'m Bob!\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi, I\\'m Bob!==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello Bob! I\\'m an AI assistant who can help you search for information using specialized search tools. Is there anything specific you\\'d like to know about or search for? I\\'m happy to help you find accurate and up-to-date information on various topics.\\ninput_message = {\"role\": \"user\", \"content\": \"What\\'s my name?\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob, as you introduced yourself earlier. I can remember information shared within our conversation without needing to search for it.\\nExample LangSmith trace\\nIf you want to start a new conversation, all you have to do is change the thread_id used\\nconfig = {\"configurable\": {\"thread_id\": \"xyz123\"}}input_message = {\"role\": \"user\", \"content\": \"What\\'s my name?\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================I apologize, but I don\\'t have access to any tools that would tell me your name. I can only assist you with searching for publicly available information using the tavily_search function. I don\\'t have access to personal information about users. If you\\'d like to tell me your name, I\\'ll be happy to address you by it.\\nConclusion\\u200b\\nThat\\'s a wrap! In this quick start we covered how to create a simple agent.\\nWe\\'ve then shown how to stream back a response - not only with the intermediate steps, but also tokens!\\nWe\\'ve also added in memory so you can have a conversation with them.\\nAgents are a complex topic with lots to learn!\\nFor more information on Agents, please check out the LangGraph documentation. This has it\\'s own set of concepts, tutorials, and how-to guides.Edit this pagePreviousBuild an Extraction ChainNextTaggingEnd-to-end agentSetupJupyter NotebookInstallationLangSmithTavilyDefine toolsUsing Language ModelsCreate the agentRun the agentStreaming MessagesStreaming tokensAdding in memoryConclusionCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 1On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 1\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis is a multi-part tutorial:\\n\\nPart 1 (this guide) introduces RAG and walks through a minimal implementation.\\nPart 2 extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\\n\\nThis tutorial will show how to build a simple Q&A application\\nover a text data source. Along the way we‚Äôll go over a typical Q&A\\narchitecture and highlight additional resources for more advanced Q&A techniques. We‚Äôll also see\\nhow LangSmith can help us trace and understand our application.\\nLangSmith will become increasingly helpful as our application grows in\\ncomplexity.\\nIf you\\'re already familiar with basic retrieval, you might also be interested in\\nthis high-level overview of different retrieval techniques.\\nNote: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.\\nOverview\\u200b\\nA typical RAG application has two main components:\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\\nRetrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\nNote: the indexing portion of this tutorial will largely follow the semantic search tutorial.\\nThe most common full sequence from raw data to answer looks like:\\nIndexing\\u200b\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\\'t fit in a model\\'s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\nRetrieval and generation\\u200b\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nOnce we\\'ve indexed our data, we will use LangGraph as our orchestration framework to implement the retrieval and generation steps.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nThis tutorial requires these langchain dependencies:\\n\\nPipConda%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraphconda install langchain-text-splitters langchain-community langgraph -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nComponents\\u200b\\nWe will need to select three components from LangChain\\'s suite of integrations.\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nPreview\\u200b\\nIn this guide we‚Äôll build an app that answers questions about the website\\'s content. The specific website we will use is the LLM Powered Autonomous\\nAgents blog post\\nby Lilian Weng, which allows us to ask questions about the contents of\\nthe post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~50\\nlines of code.\\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Index chunks_ = vector_store.add_documents(documents=all_splits)# Define prompt for question-answering# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    context: List[Document]    answer: str# Define application stepsdef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:Document | StateGraph\\nresponse = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(response[\"answer\"])\\nTask Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model\\'s thinking process.\\nCheck out the LangSmith\\ntrace.\\nDetailed walkthrough\\u200b\\nLet‚Äôs go through the above code step-by-step to really understand what‚Äôs\\ngoing on.\\n1. Indexing\\u200b\\nnoteThis section is an abbreviated version of the content in the semantic search tutorial.\\nIf you\\'re comfortable with document loaders, embeddings, and vector stores,\\nfeel free to skip to the next section on retrieval and generation.\\nLoading documents\\u200b\\nWe need to first load the blog post contents. We can use\\nDocumentLoaders\\nfor this, which are objects that load in data from a source and return a\\nlist of\\nDocument\\nobjects.\\nIn this case we‚Äôll use the\\nWebBaseLoader,\\nwhich uses urllib to load HTML from web URLs and BeautifulSoup to\\nparse it to text. We can customize the HTML -> text parsing by passing\\nin parameters into the BeautifulSoup parser via bs_kwargs (see\\nBeautifulSoup\\ndocs).\\nIn this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or\\n‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nimport bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()assert len(docs) == 1print(f\"Total characters: {len(docs[0].page_content)}\")\\nTotal characters: 43131\\nprint(docs[0].page_content[:500])\\n      LLM Powered Autonomous Agents    Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In\\nGo deeper\\u200b\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nDocs:\\nDetailed documentation on how to use DocumentLoaders.\\nIntegrations: 160+\\nintegrations to choose from.\\nInterface:\\nAPI reference for the base interface.\\n\\nSplitting documents\\u200b\\nOur loaded document is over 42k characters which is too long to fit\\ninto the context window of many models. Even for those models that could\\nfit the full post in their context window, models can struggle to find\\ninformation in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and\\nvector storage. This should help us retrieve only the most relevant parts\\nof the blog post at run time.\\nAs in the semantic search tutorial, we use a\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000,  # chunk size (characters)    chunk_overlap=200,  # chunk overlap (characters)    add_start_index=True,  # track index in original document)all_splits = text_splitter.split_documents(docs)print(f\"Split blog post into {len(all_splits)} sub-documents.\")\\nSplit blog post into 66 sub-documents.\\nGo deeper\\u200b\\nTextSplitter: Object that splits a list of Documents into smaller\\nchunks. Subclass of DocumentTransformers.\\n\\nLearn more about splitting text using different methods by reading the how-to docs\\nCode (py or js)\\nScientific papers\\nInterface: API reference for the base interface.\\n\\nDocumentTransformer: Object that performs a transformation on a list\\nof Document objects.\\n\\nDocs: Detailed documentation on how to use DocumentTransformers\\nIntegrations\\nInterface: API reference for the base interface.\\n\\nStoring documents\\u200b\\nNow we need to index our 66 text chunks so that we can search over them\\nat runtime. Following the semantic search tutorial,\\nour approach is to embed the contents of each document split and insert these embeddings\\ninto a vector store. Given an input query, we can then use\\nvector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command\\nusing the vector store and embeddings model selected at the start of the tutorial.\\ndocument_ids = vector_store.add_documents(documents=all_splits)print(document_ids[:3])\\n[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\nGo deeper\\u200b\\nEmbeddings: Wrapper around a text embedding model, used for converting\\ntext to embeddings.\\n\\nDocs: Detailed documentation on how to use embeddings.\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and\\nquerying embeddings.\\n\\nDocs: Detailed documentation on how to use vector stores.\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point\\nwe have a query-able vector store containing the chunked contents of our\\nblog post. Given a user question, we should ideally be able to return\\nthe snippets of the blog post that answer the question.\\n2. Retrieval and Generation\\u200b\\nNow let‚Äôs write the actual application logic. We want to create a simple\\napplication that takes a user question, searches for documents relevant\\nto that question, passes the retrieved documents and initial question to\\na model, and returns an answer.\\nFor generation, we will use the chat model selected at the start of the tutorial.\\nWe‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub\\n(here).\\nfrom langchain import hub# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke(    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}).to_messages()assert len(example_messages) == 1print(example_messages[0].content)\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.Question: (question goes here) Context: (context goes here) Answer:\\nWe\\'ll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\\n\\nWe can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.\\nWe get streamlined deployments via LangGraph Platform.\\nLangSmith will automatically trace the steps of our application together.\\nWe can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes.\\n\\nTo use LangGraph, we need to define three things:\\n\\nThe state of our application;\\nThe nodes of our application (i.e., application steps);\\nThe \"control flow\" of our application (e.g., the ordering of the steps).\\n\\nState:\\u200b\\nThe state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\\nFor a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer:\\nfrom langchain_core.documents import Documentfrom typing_extensions import List, TypedDictclass State(TypedDict):    question: str    context: List[Document]    answer: strAPI Reference:Document\\nNodes (application steps)\\u200b\\nLet\\'s start with a simple sequence of two steps: retrieval and generation.\\ndef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}\\nOur retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.\\nControl flow\\u200b\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the retrieval and generation steps into a single sequence.\\nfrom langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:StateGraph\\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nDo I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:question = \"...\"retrieved_docs = vector_store.similarity_search(question)docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)prompt = prompt.invoke({\"question\": question, \"context\": docs_content})answer = llm.invoke(prompt)The benefits of LangGraph include:\\nSupport for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\\nAutomatic support for tracing via LangSmith and deployments via LangGraph Platform;\\nSupport for persistence, human-in-the-loop, and other features.\\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in Part 2 of the tutorial, LangGraph\\'s management and persistence of state simplifies these applications enormously.\\nUsage\\u200b\\nLet\\'s test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\\nInvoke:\\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f\"Context: {result[\\'context\\']}\\\\n\\\\n\")print(f\"Answer: {result[\\'answer\\']}\")\\nContext: [Document(id=\\'a42dc78b-8f76-472a-9e25-180508af74f3\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}, page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\'), Document(id=\\'c0e45887-d0b0-483d-821a-bb5d8316d51d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}, page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\'), Document(id=\\'4cc7f318-35f5-440f-a4a4-145b5f0b918d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 29630}, page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\'), Document(id=\\'f621ade4-9b0d-471f-a522-44eb5feeba0c\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]Answer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs.\\nStream steps:\\nfor step in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"):    print(f\"{step}\\\\n\\\\n----------------\\\\n\")\\n{\\'retrieve\\': {\\'context\\': [Document(id=\\'a42dc78b-8f76-472a-9e25-180508af74f3\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}, page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\'), Document(id=\\'c0e45887-d0b0-483d-821a-bb5d8316d51d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}, page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\'), Document(id=\\'4cc7f318-35f5-440f-a4a4-145b5f0b918d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 29630}, page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\'), Document(id=\\'f621ade4-9b0d-471f-a522-44eb5feeba0c\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}----------------{\\'generate\\': {\\'answer\\': \\'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.\\'}}----------------\\nStream tokens:\\nfor message, metadata in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"):    print(message.content, end=\"|\")\\n|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|\\'s| reasoning| process|.||\\ntipFor async invocations, use:result = await graph.ainvoke(...)andasync for step in graph.astream(...):\\nReturning sources\\u200b\\nNote that by storing the retrieved context in the state of the graph, we recover sources for the model\\'s generated answer in the \"context\" field of the state. See this guide on returning sources for more detail.\\nGo deeper\\u200b\\nChat models take in a sequence of messages and return a message.\\n\\nDocs\\nIntegrations: 25+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nCustomizing the prompt\\nAs shown above, we can load prompts (e.g., this RAG\\nprompt) from the prompt\\nhub. The prompt can also be easily customized. For example:\\nfrom langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)API Reference:PromptTemplate\\nQuery analysis\\u200b\\nSo far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:\\n\\nIn addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\");\\nThe model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.\\n\\nQuery analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let\\'s add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.\\ntotal_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"all_splits[0].metadata\\n{\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 8, \\'section\\': \\'beginning\\'}\\nWe will need to update the documents in our vector store. We will use a simple InMemoryVectorStore for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store integration documentation for relevant features of your chosen vector store.\\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)API Reference:InMemoryVectorStore\\nLet\\'s next define a schema for our search query. We will use structured output for this purpose. Here we define a query as containing a string query and a document section (either \"beginning\", \"middle\", or \"end\"), but this can be defined however you like.\\nfrom typing import Literalfrom typing_extensions import Annotatedclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]\\nFinally, we add a step to our LangGraph application to generate a query from the user\\'s raw input:\\nclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()\\nFull Code:from typing import Literalimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import Annotated, List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Update metadata (illustration purposes)total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"# Index chunksvector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)# Define schema for searchclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()API Reference:Document | InMemoryVectorStore | StateGraph\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nWe can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.\\nfor step in graph.stream(    {\"question\": \"What does the end of the post say about Task Decomposition?\"},    stream_mode=\"updates\",):    print(f\"{step}\\\\n\\\\n----------------\\\\n\")\\n{\\'analyze_query\\': {\\'query\\': {\\'query\\': \\'Task Decomposition\\', \\'section\\': \\'end\\'}}}----------------{\\'retrieve\\': {\\'context\\': [Document(id=\\'d6cef137-e1e8-4ddc-91dc-b62bd33c6020\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 39221, \\'section\\': \\'end\\'}, page_content=\\'Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\\\n\\\\n\\\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\'), Document(id=\\'d1834ae1-eb6a-43d7-a023-08dfa5028799\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 39086, \\'section\\': \\'end\\'}, page_content=\\'}\\\\n]\\\\nChallenges#\\\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\'), Document(id=\\'ca7f06e4-2c2e-4788-9a81-2418d82213d9\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 32942, \\'section\\': \\'end\\'}, page_content=\\'}\\\\n]\\\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\\\nSystem message:\\'), Document(id=\\'1fcc2736-30f4-4ef6-90f2-c64af92118cb\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 35127, \\'section\\': \\'end\\'}, page_content=\\'\"content\": \"You will get instructions for code to write.\\\\\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\\\\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\\\`\\\\`\\\\`LANG\\\\\\\\nCODE\\\\\\\\n\\\\`\\\\`\\\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\"entrypoint\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease\\')]}}----------------{\\'generate\\': {\\'answer\\': \\'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.\\'}}----------------\\nIn both the streamed steps and the LangSmith trace, we can now observe the structured query that was fed into the retrieval step.\\nQuery Analysis is a rich problem with a wide range of approaches. Refer to the how-to guides for more examples.\\nNext steps\\u200b\\nWe\\'ve covered the steps to build a basic Q&A app over data:\\n\\nLoading data with a Document Loader\\nChunking the indexed data with a Text Splitter to make it more easily usable by a model\\nEmbedding the data and storing the data in a vectorstore\\nRetrieving the previously stored chunks in response to incoming questions\\nGenerating an answer using the retrieved chunks as context.\\n\\nIn Part 2 of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.\\nFurther reading:\\n\\nReturn sources: Learn how to return source documents\\nStreaming: Learn how to stream outputs and intermediate steps\\nAdd chat history: Learn how to add chat history to your app\\nRetrieval conceptual guide: A high-level overview of specific retrieval techniques\\nEdit this pagePreviousTaggingNextBuild a semantic search engineOverviewIndexingRetrieval and generationSetupJupyter NotebookInstallationLangSmithComponentsPreviewDetailed walkthrough1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationQuery analysisNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain', 'description': 'In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 2On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 2\\nIn many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\\nThis is the second part of a multi-part tutorial:\\n\\nPart 1 introduces RAG and walks through a minimal implementation.\\nPart 2 (this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\\n\\nHere we focus on adding logic for incorporating historical messages. This involves the management of a chat history.\\nWe will cover two approaches:\\n\\nChains, in which we execute at most one retrieval step;\\nAgents, in which we give an LLM discretion to execute multiple retrieval steps.\\n\\nnoteThe methods presented here leverage tool-calling capabilities in modern chat models. See this page for a table of models supporting tool calling features.\\nFor the external knowledge source, we will use the same LLM Powered Autonomous Agents blog post by Lilian Weng from the Part 1 of the RAG tutorial.\\nSetup\\u200b\\nComponents\\u200b\\nWe will need to select three components from LangChain\\'s suite of integrations.\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nDependencies\\u200b\\nIn addition, we\\'ll use the following packages:\\n%%capture --no-stderr%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"if not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nChains\\u200b\\nLet\\'s first revisit the vector store we built in Part 1, which indexes an LLM Powered Autonomous Agents blog post by Lilian Weng.\\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)API Reference:Document\\n# Index chunks_ = vector_store.add_documents(documents=all_splits)\\nIn the Part 1 of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of messages. In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via tool messages. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have\\n\\nUser input as a HumanMessage;\\nVector store query as an AIMessage with tool calls;\\nRetrieved documents as a ToolMessage;\\nFinal response as a AIMessage.\\n\\nThis model for state is so versatile that LangGraph offers a built-in version for convenience:\\nfrom langgraph.graph import MessagesState, StateGraphgraph_builder = StateGraph(MessagesState)API Reference:StateGraph\\nLeveraging tool-calling to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:\\n\\nHuman: \"What is Task Decomposition?\"\\nAI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\\nHuman: \"What are common ways of doing it?\"\\n\\nIn this scenario, a model could generate a query such as \"common approaches to task decomposition\". Tool-calling facilitates this naturally. As in the query analysis section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user).\\nLet\\'s turn our retrieval step into a tool:\\nfrom langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def retrieve(query: str):    \"\"\"Retrieve information related to a query.\"\"\"    retrieved_docs = vector_store.similarity_search(query, k=2)    serialized = \"\\\\n\\\\n\".join(        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")        for doc in retrieved_docs    )    return serialized, retrieved_docsAPI Reference:tool\\nSee this guide for more detail on creating tools.\\nOur graph will consist of three nodes:\\n\\nA node that fields the user input, either generating a query for the retriever or responding directly;\\nA node for the retriever tool that executes the retrieval step;\\nA node that generates the final response using the retrieved context.\\n\\nWe build them below. Note that we leverage another pre-built LangGraph component, ToolNode, that executes the tool and adds the result as a ToolMessage to the state.\\nfrom langchain_core.messages import SystemMessagefrom langgraph.prebuilt import ToolNode# Step 1: Generate an AIMessage that may include a tool-call to be sent.def query_or_respond(state: MessagesState):    \"\"\"Generate tool call for retrieval or respond.\"\"\"    llm_with_tools = llm.bind_tools([retrieve])    response = llm_with_tools.invoke(state[\"messages\"])    # MessagesState appends messages to state instead of overwriting    return {\"messages\": [response]}# Step 2: Execute the retrieval.tools = ToolNode([retrieve])# Step 3: Generate a response using the retrieved content.def generate(state: MessagesState):    \"\"\"Generate answer.\"\"\"    # Get generated ToolMessages    recent_tool_messages = []    for message in reversed(state[\"messages\"]):        if message.type == \"tool\":            recent_tool_messages.append(message)        else:            break    tool_messages = recent_tool_messages[::-1]    # Format into prompt    docs_content = \"\\\\n\\\\n\".join(doc.content for doc in tool_messages)    system_message_content = (        \"You are an assistant for question-answering tasks. \"        \"Use the following pieces of retrieved context to answer \"        \"the question. If you don\\'t know the answer, say that you \"        \"don\\'t know. Use three sentences maximum and keep the \"        \"answer concise.\"        \"\\\\n\\\\n\"        f\"{docs_content}\"    )    conversation_messages = [        message        for message in state[\"messages\"]        if message.type in (\"human\", \"system\")        or (message.type == \"ai\" and not message.tool_calls)    ]    prompt = [SystemMessage(system_message_content)] + conversation_messages    # Run    response = llm.invoke(prompt)    return {\"messages\": [response]}API Reference:SystemMessage | ToolNode\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the steps into a sequence. We also allow the first query_or_respond step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\\nfrom langgraph.graph import ENDfrom langgraph.prebuilt import ToolNode, tools_conditiongraph_builder.add_node(query_or_respond)graph_builder.add_node(tools)graph_builder.add_node(generate)graph_builder.set_entry_point(\"query_or_respond\")graph_builder.add_conditional_edges(    \"query_or_respond\",    tools_condition,    {END: END, \"tools\": \"tools\"},)graph_builder.add_edge(\"tools\", \"generate\")graph_builder.add_edge(\"generate\", END)graph = graph_builder.compile()API Reference:ToolNode | tools_condition\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nLet\\'s test our application.\\nNote that it responds appropriately to messages that do not require an additional retrieval step:\\ninput_message = \"Hello\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hello==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello! How can I assist you today?\\nAnd when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:\\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is Task Decomposition?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_dLjB3rkMoxZZxwUGXi33UBeh) Call ID: call_dLjB3rkMoxZZxwUGXi33UBeh  Args:    query: Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\\x1b[1m Ai Message \\x1b[0m==================================Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often involves techniques like Chain of Thought (CoT), which encourages models to think step by step, enhancing performance on complex tasks. This approach allows for a clearer understanding of the task and aids in structuring the problem-solving process.\\nCheck out the LangSmith trace here.\\nStateful management of chat history\\u200b\\nnoteThis section of the tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nIn production, the Q&A application will usually persist the chat history into a database, and be able to read and update it appropriately.\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nTo manage multiple conversational turns and threads, all we have to do is specify a checkpointer when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\nFor a detailed walkthrough of how to manage message history, head to the How to add message history (memory) guide.\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory)# Specify an ID for the threadconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}API Reference:MemorySaver\\nWe can now invoke similar to before:\\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is Task Decomposition?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_JZb6GLD812bW2mQsJ5EJQDnN) Call ID: call_JZb6GLD812bW2mQsJ5EJQDnN  Args:    query: Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\\x1b[1m Ai Message \\x1b[0m==================================Task Decomposition is a technique used to break down complicated tasks into smaller, manageable steps. It involves using methods like Chain of Thought (CoT) prompting, which encourages the model to think step by step, enhancing performance on complex tasks. This process helps to clarify the model\\'s reasoning and makes it easier to tackle difficult problems.\\ninput_message = \"Can you look up some common ways of doing it?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Can you look up some common ways of doing it?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_kjRI4Y5cJOiB73yvd7dmb6ux) Call ID: call_kjRI4Y5cJOiB73yvd7dmb6ux  Args:    query: common methods of task decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================Common ways of performing Task Decomposition include: (1) using Large Language Models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", (2) employing task-specific instructions such as \"Write a story outline\" for specific tasks, and (3) incorporating human inputs to guide the decomposition process.\\nNote that the query generated by the model in the second question incorporates the conversational context.\\nThe LangSmith trace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step.\\nAgents\\u200b\\nAgents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.\\nBelow we assemble a minimal RAG agent. Using LangGraph\\'s pre-built ReAct agent constructor, we can do this in one line.\\ntipCheck out LangGraph\\'s Agentic RAG tutorial for more advanced formulations.\\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)API Reference:create_react_agent\\nLet\\'s inspect the graph:\\ndisplay(Image(agent_executor.get_graph().draw_mermaid_png()))\\n\\nThe key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.\\nLet\\'s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nconfig = {\"configurable\": {\"thread_id\": \"def234\"}}input_message = (    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"    \"Once you get the answer, look up common extensions of that method.\")for event in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    event[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is the standard method for Task Decomposition?Once you get the answer, look up common extensions of that method.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_Y3YaIzL71B83Cjqa8d2G0O8N) Call ID: call_Y3YaIzL71B83Cjqa8d2G0O8N  Args:    query: standard method for Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_2JntP1x4XQMWwgVpYurE12ff) Call ID: call_2JntP1x4XQMWwgVpYurE12ff  Args:    query: common extensions of Task Decomposition methods=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================The standard method for task decomposition involves using techniques such as Chain of Thought (CoT), where a model is instructed to \"think step by step\" to break down complex tasks into smaller, more manageable components. This approach enhances model performance by allowing for more thorough reasoning and planning. Task decomposition can be accomplished through various means, including:1. Simple prompting (e.g., asking for steps to achieve a goal).2. Task-specific instructions (e.g., asking for a story outline).3. Human inputs to guide the decomposition process.### Common Extensions of Task Decomposition Methods:1. **Tree of Thoughts**: This extension builds on CoT by not only decomposing the problem into thought steps but also generating multiple thoughts at each step, creating a tree structure. The search process can employ breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.These extensions aim to enhance reasoning capabilities and improve the effectiveness of task decomposition in various contexts.\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nNext steps\\u200b\\nWe\\'ve covered the steps to build a basic conversational Q&A application:\\n\\nWe used chains to build a predictable application that generates at most one query per user input;\\nWe used agents to build an application that can iterate on a sequence of queries.\\n\\nTo explore different types of retrievers and retrieval strategies, visit the retrievers section of the how-to guides.\\nFor a detailed walkthrough of LangChain\\'s conversation memory abstractions, visit the How to add message history (memory) guide.\\nTo learn more about agents, check out the conceptual guide and LangGraph agent architectures page.Edit this pagePreviousBuild a ChatbotNextBuild an Extraction ChainSetupComponentsDependenciesLangSmithChainsStateful management of chat historyAgentsNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/sql_qa/', 'title': 'Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Question/Answering system over SQL dataOn this pageBuild a Question/Answering system over SQL data\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nTools\\nAgents\\nLangGraph\\n\\nEnabling a LLM system to query structured data can be qualitatively different from unstructured text data. Whereas in the latter it is common to generate text that can be searched against a vector database, the approach for structured data is often for the LLM to write and execute queries in a DSL, such as SQL. In this guide we\\'ll go over the basic ways to create a Q&A system over tabular data in databases. We will cover implementations using both chains and agents. These systems will allow us to ask a question about the data in a database and get back a natural language answer. The main difference between the two is that our agent can query the database in a loop as many times as it needs to answer the question.\\n‚ö†Ô∏è Security note ‚ö†Ô∏è\\u200b\\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent\\'s needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see here.\\nArchitecture\\u200b\\nAt a high-level, the steps of these systems are:\\n\\nConvert question to SQL query: Model converts user input to a SQL query.\\nExecute SQL query: Execute the query.\\nAnswer the question: Model responds to user input using the query results.\\n\\nNote that querying data in CSVs can follow a similar approach. See our how-to guide on question-answering over CSV data for more detail.\\n\\nSetup\\u200b\\nFirst, get required packages and set environment variables:\\n%%capture --no-stderr%pip install --upgrade --quiet langchain-community langgraph\\n# Comment out the below to opt-out of using LangSmith in this notebook. Not required.if not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()    os.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nSample data\\u200b\\nThe below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow these installation steps to create Chinook.db in the same directory as this notebook. You can also download and build the database via the command line:\\ncurl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db\\nNow, Chinook.db is in our directory and we can interface with it using the SQLAlchemy-driven SQLDatabase class:\\nfrom langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")print(db.dialect)print(db.get_usable_table_names())db.run(\"SELECT * FROM Artist LIMIT 10;\")\\nsqlite[\\'Album\\', \\'Artist\\', \\'Customer\\', \\'Employee\\', \\'Genre\\', \\'Invoice\\', \\'InvoiceLine\\', \\'MediaType\\', \\'Playlist\\', \\'PlaylistTrack\\', \\'Track\\']\\n\"[(1, \\'AC/DC\\'), (2, \\'Accept\\'), (3, \\'Aerosmith\\'), (4, \\'Alanis Morissette\\'), (5, \\'Alice In Chains\\'), (6, \\'Ant√¥nio Carlos Jobim\\'), (7, \\'Apocalyptica\\'), (8, \\'Audioslave\\'), (9, \\'BackBeat\\'), (10, \\'Billy Cobham\\')]\"\\nGreat! We\\'ve got a SQL database that we can query. Now let\\'s try hooking it up to an LLM.\\nChains\\u200b\\nChains are compositions of predictable steps. In LangGraph, we can represent a chain via simple sequence of nodes. Let\\'s create a sequence of steps that, given a question, does the following:\\n\\nconverts the question into a SQL query;\\nexecutes the query;\\nuses the result to answer the original question.\\n\\nThere are scenarios not supported by this arrangement. For example, this system will execute a SQL query for any user input-- even \"hello\". Importantly, as we\\'ll see below, some questions require more than one query to answer. We will address these scenarios in the Agents section.\\nApplication state\\u200b\\nThe LangGraph state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\\nFor this application, we can just keep track of the input question, generated query, query result, and generated answer:\\nfrom typing_extensions import TypedDictclass State(TypedDict):    question: str    query: str    result: str    answer: str\\nNow we just need functions that operate on this state and populate its contents.\\nConvert question to SQL query\\u200b\\nThe first step is to take the user input and convert it to a SQL query. To reliably obtain SQL queries (absent markdown formatting and explanations or clarifications), we will make use of LangChain\\'s structured output abstraction.\\nLet\\'s select a chat model for our application:\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s provide some instructions for our model:\\nfrom langchain_core.prompts import ChatPromptTemplatesystem_message = \"\"\"Given an input question, create a syntactically correct {dialect} query torun to help find the answer. Unless the user specifies in his question aspecific number of examples they wish to obtain, always limit your query toat most {top_k} results. You can order the results by a relevant column toreturn the most interesting examples in the database.Never query for all the columns from a specific table, only ask for a thefew relevant columns given the question.Pay attention to use only the column names that you can see in the schemadescription. Be careful to not query for columns that do not exist. Also,pay attention to which column is in which table.Only use the following tables:{table_info}\"\"\"user_prompt = \"Question: {input}\"query_prompt_template = ChatPromptTemplate(    [(\"system\", system_message), (\"user\", user_prompt)])for message in query_prompt_template.messages:    message.pretty_print()API Reference:ChatPromptTemplate\\n================================\\x1b[1m System Message \\x1b[0m================================Given an input question, create a syntactically correct \\x1b[33;1m\\x1b[1;3m{dialect}\\x1b[0m query torun to help find the answer. Unless the user specifies in his question aspecific number of examples they wish to obtain, always limit your query toat most \\x1b[33;1m\\x1b[1;3m{top_k}\\x1b[0m results. You can order the results by a relevant column toreturn the most interesting examples in the database.Never query for all the columns from a specific table, only ask for a thefew relevant columns given the question.Pay attention to use only the column names that you can see in the schemadescription. Be careful to not query for columns that do not exist. Also,pay attention to which column is in which table.Only use the following tables:\\x1b[33;1m\\x1b[1;3m{table_info}\\x1b[0m================================\\x1b[1m Human Message \\x1b[0m=================================Question: \\x1b[33;1m\\x1b[1;3m{input}\\x1b[0m\\nThe prompt includes several parameters we will need to populate, such as the SQL dialect and table schemas. LangChain\\'s SQLDatabase object includes methods to help with this. Our write_query step will just populate these parameters and prompt a model to generate the SQL query:\\nfrom typing_extensions import Annotatedclass QueryOutput(TypedDict):    \"\"\"Generated SQL query.\"\"\"    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]def write_query(state: State):    \"\"\"Generate SQL query to fetch information.\"\"\"    prompt = query_prompt_template.invoke(        {            \"dialect\": db.dialect,            \"top_k\": 10,            \"table_info\": db.get_table_info(),            \"input\": state[\"question\"],        }    )    structured_llm = llm.with_structured_output(QueryOutput)    result = structured_llm.invoke(prompt)    return {\"query\": result[\"query\"]}\\nLet\\'s test it out:\\nwrite_query({\"question\": \"How many Employees are there?\"})\\n{\\'query\\': \\'SELECT COUNT(*) as employee_count FROM Employee;\\'}\\nExecute query\\u200b\\nThis is the most dangerous part of creating a SQL chain. Consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution (see below).\\nTo execute the query, we will load a tool from langchain-community. Our execute_query node will just wrap this tool:\\nfrom langchain_community.tools.sql_database.tool import QuerySQLDatabaseTooldef execute_query(state: State):    \"\"\"Execute SQL query.\"\"\"    execute_query_tool = QuerySQLDatabaseTool(db=db)    return {\"result\": execute_query_tool.invoke(state[\"query\"])}\\nTesting this step:\\nexecute_query({\"query\": \"SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;\"})\\n{\\'result\\': \\'[(8,)]\\'}\\nGenerate answer\\u200b\\nFinally, our last step generates an answer to the question given the information pulled from the database:\\ndef generate_answer(state: State):    \"\"\"Answer question using retrieved information as context.\"\"\"    prompt = (        \"Given the following user question, corresponding SQL query, \"        \"and SQL result, answer the user question.\\\\n\\\\n\"        f\"Question: {state[\\'question\\']}\\\\n\"        f\"SQL Query: {state[\\'query\\']}\\\\n\"        f\"SQL Result: {state[\\'result\\']}\"    )    response = llm.invoke(prompt)    return {\"answer\": response.content}\\nOrchestrating with LangGraph\\u200b\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the three steps into a single sequence.\\nfrom langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence(    [write_query, execute_query, generate_answer])graph_builder.add_edge(START, \"write_query\")graph = graph_builder.compile()API Reference:StateGraph\\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nLet\\'s test our application! Note that we can stream the results of individual steps:\\nfor step in graph.stream(    {\"question\": \"How many employees are there?\"}, stream_mode=\"updates\"):    print(step)\\n{\\'write_query\\': {\\'query\\': \\'SELECT COUNT(*) as employee_count FROM Employee;\\'}}{\\'execute_query\\': {\\'result\\': \\'[(8,)]\\'}}{\\'generate_answer\\': {\\'answer\\': \\'There are 8 employees in total.\\'}}\\nCheck out the LangSmith trace.\\nHuman-in-the-loop\\u200b\\nLangGraph supports a number of features that can be useful for this workflow. One of them is human-in-the-loop: we can interrupt our application before sensitive steps (such as the execution of a SQL query) for human review. This is enabled by LangGraph\\'s persistence layer, which saves run progress to your storage of choice. Below, we specify storage in-memory:\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory, interrupt_before=[\"execute_query\"])# Now that we\\'re using persistence, we need to specify a thread ID# so that we can continue the run after review.config = {\"configurable\": {\"thread_id\": \"1\"}}API Reference:MemorySaver\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nLet\\'s repeat the same run, adding in a simple yes/no approval step:\\nfor step in graph.stream(    {\"question\": \"How many employees are there?\"},    config,    stream_mode=\"updates\",):    print(step)try:    user_approval = input(\"Do you want to go to execute query? (yes/no): \")except Exception:    user_approval = \"no\"if user_approval.lower() == \"yes\":    # If approved, continue the graph execution    for step in graph.stream(None, config, stream_mode=\"updates\"):        print(step)else:    print(\"Operation cancelled by user.\")\\n{\\'write_query\\': {\\'query\\': \\'SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;\\'}}{\\'__interrupt__\\': ()}``````outputDo you want to go to execute query? (yes/no):  yes``````output{\\'execute_query\\': {\\'result\\': \\'[(8,)]\\'}}{\\'generate_answer\\': {\\'answer\\': \\'There are 8 employees.\\'}}\\nSee this LangGraph guide for more detail and examples.\\nNext steps\\u200b\\nFor more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:\\n\\nPrompting strategies: Advanced prompt engineering techniques.\\nQuery checking: Add query validation and error handling.\\nLarge databases: Techniques for working with large databases.\\n\\nAgents\\u200b\\nAgents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the query generation and execution process. Although their behavior is less predictable than the above \"chain\", they feature some advantages:\\n\\nThey can query the database as many times as needed to answer the user question.\\nThey can recover from errors by running a generated query, catching the traceback and regenerating it correctly.\\nThey can answer questions based on the databases\\' schema as well as on the databases\\' content (like describing a specific table).\\n\\nBelow we assemble a minimal SQL agent. We will equip it with a set of tools using LangChain\\'s SQLDatabaseToolkit. Using LangGraph\\'s pre-built ReAct agent constructor, we can do this in one line.\\ntipCheck out LangGraph\\'s SQL Agent Tutorial for a more advanced formulation of a SQL agent.\\nThe SQLDatabaseToolkit includes tools that can:\\n\\nCreate and execute queries\\nCheck query syntax\\nRetrieve table descriptions\\n... and more\\n\\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkittoolkit = SQLDatabaseToolkit(db=db, llm=llm)tools = toolkit.get_tools()tools\\n[QuerySQLDatabaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column \\'xxxx\\' in \\'field list\\', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), InfoSQLDatabaseTool(description=\\'Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\\', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), QuerySQLCheckerTool(description=\\'Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\\', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name=\\'gpt-4o\\', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=[\\'dialect\\', \\'query\\'], input_types={}, partial_variables={}, template=\\'\\\\n{query}\\\\nDouble check the {dialect} query above for common mistakes, including:\\\\n- Using NOT IN with NULL values\\\\n- Using UNION when UNION ALL should have been used\\\\n- Using BETWEEN for exclusive ranges\\\\n- Data type mismatch in predicates\\\\n- Properly quoting identifiers\\\\n- Using the correct number of arguments for functions\\\\n- Casting to the correct data type\\\\n- Using the proper columns for joins\\\\n\\\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\\\n\\\\nOutput the final SQL query only.\\\\n\\\\nSQL Query: \\'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name=\\'gpt-4o\\', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')), output_parser=StrOutputParser(), llm_kwargs={}))]\\nSystem Prompt\\u200b\\nWe will also want to load a system prompt for our agent. This will consist of instructions for how to behave. Note that the prompt below has several parameters, which we assign below.\\nsystem_message = \"\"\"You are an agent designed to interact with a SQL database.Given an input question, create a syntactically correct {dialect} query to run,then look at the results of the query and return the answer. Unless the userspecifies a specific number of examples they wish to obtain, always limit yourquery to at most {top_k} results.You can order the results by a relevant column to return the most interestingexamples in the database. Never query for all the columns from a specific table,only ask for the relevant columns given the question.You MUST double check your query before executing it. If you get an error whileexecuting a query, rewrite the query and try again.DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to thedatabase.To start you should ALWAYS look at the tables in the database to see what youcan query. Do NOT skip this step.Then you should query the schema of the most relevant tables.\"\"\".format(    dialect=\"SQLite\",    top_k=5,)\\nInitializing agent\\u200b\\nWe will use a prebuilt LangGraph agent to build our agent\\nfrom langchain_core.messages import HumanMessagefrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, tools, prompt=system_message)API Reference:HumanMessage | create_react_agent\\nConsider how the agent responds to the below question:\\nquestion = \"Which country\\'s customers spent the most?\"for step in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Which country\\'s customers spent the most?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_list_tables (call_tFp7HYD6sAAmCShgeqkVZH6Q) Call ID: call_tFp7HYD6sAAmCShgeqkVZH6Q  Args:=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_list_tablesAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_schema (call_KJZ1Jx6JazyDdJa0uH1UeiOz) Call ID: call_KJZ1Jx6JazyDdJa0uH1UeiOz  Args:    table_names: Customer, Invoice=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_schemaCREATE TABLE \"Customer\" (\\t\"CustomerId\" INTEGER NOT NULL, \\t\"FirstName\" NVARCHAR(40) NOT NULL, \\t\"LastName\" NVARCHAR(20) NOT NULL, \\t\"Company\" NVARCHAR(80), \\t\"Address\" NVARCHAR(70), \\t\"City\" NVARCHAR(40), \\t\"State\" NVARCHAR(40), \\t\"Country\" NVARCHAR(40), \\t\"PostalCode\" NVARCHAR(10), \\t\"Phone\" NVARCHAR(24), \\t\"Fax\" NVARCHAR(24), \\t\"Email\" NVARCHAR(60) NOT NULL, \\t\"SupportRepId\" INTEGER, \\tPRIMARY KEY (\"CustomerId\"), \\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\"))/*3 rows from Customer table:CustomerId\\tFirstName\\tLastName\\tCompany\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\tSupportRepId1\\tLu√≠s\\tGon√ßalves\\tEmbraer - Empresa Brasileira de Aeron√°utica S.A.\\tAv. Brigadeiro Faria Lima, 2170\\tS√£o Jos√© dos Campos\\tSP\\tBrazil\\t12227-000\\t+55 (12) 3923-5555\\t+55 (12) 3923-5566\\tluisg@embraer.com.br\\t32\\tLeonie\\tK√∂hler\\tNone\\tTheodor-Heuss-Stra√üe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t+49 0711 2842222\\tNone\\tleonekohler@surfeu.de\\t53\\tFran√ßois\\tTremblay\\tNone\\t1498 rue B√©langer\\tMontr√©al\\tQC\\tCanada\\tH2G 1A7\\t+1 (514) 721-4711\\tNone\\tftremblay@gmail.com\\t3*/CREATE TABLE \"Invoice\" (\\t\"InvoiceId\" INTEGER NOT NULL, \\t\"CustomerId\" INTEGER NOT NULL, \\t\"InvoiceDate\" DATETIME NOT NULL, \\t\"BillingAddress\" NVARCHAR(70), \\t\"BillingCity\" NVARCHAR(40), \\t\"BillingState\" NVARCHAR(40), \\t\"BillingCountry\" NVARCHAR(40), \\t\"BillingPostalCode\" NVARCHAR(10), \\t\"Total\" NUMERIC(10, 2) NOT NULL, \\tPRIMARY KEY (\"InvoiceId\"), \\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\"))/*3 rows from Invoice table:InvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal1\\t2\\t2021-01-01 00:00:00\\tTheodor-Heuss-Stra√üe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.982\\t4\\t2021-01-02 00:00:00\\tUllev√•lsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.963\\t8\\t2021-01-03 00:00:00\\tGr√©trystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94*/==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_query_checker (call_AQuTGbgH63u4gPgyV723yrjX) Call ID: call_AQuTGbgH63u4gPgyV723yrjX  Args:    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_query_checker\\\\`\\\\`\\\\`sqlSELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;\\\\`\\\\`\\\\`==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_query (call_B88EwU44nwwpQL5M9nlcemSU) Call ID: call_B88EwU44nwwpQL5M9nlcemSU  Args:    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_query[(\\'USA\\', 523.06)]==================================\\x1b[1m Ai Message \\x1b[0m==================================The country whose customers spent the most is the USA, with a total spending of 523.06.\\nYou can also use the LangSmith trace to visualize these steps and associated metadata.\\nNote that the agent executes multiple queries until it has the information it needs:\\n\\nList available tables;\\nRetrieves the schema for three tables;\\nQueries multiple of the tables via a join operation.\\n\\nThe agent is then able to use the result of the final query to generate an answer to the original question.\\nThe agent can similarly handle qualitative questions:\\nquestion = \"Describe the playlisttrack table\"for step in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Describe the playlisttrack table==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_list_tables (call_fMF8eTmX5TJDJjc3Mhdg52TI) Call ID: call_fMF8eTmX5TJDJjc3Mhdg52TI  Args:=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_list_tablesAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_schema (call_W8Vkk4NEodkAAIg8nexAszUH) Call ID: call_W8Vkk4NEodkAAIg8nexAszUH  Args:    table_names: PlaylistTrack=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_schemaCREATE TABLE \"PlaylistTrack\" (\\t\"PlaylistId\" INTEGER NOT NULL, \\t\"TrackId\" INTEGER NOT NULL, \\tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\"))/*3 rows from PlaylistTrack table:PlaylistId\\tTrackId1\\t34021\\t33891\\t3390*/==================================\\x1b[1m Ai Message \\x1b[0m==================================The `PlaylistTrack` table is designed to associate tracks with playlists. It has the following structure:- **PlaylistId**: An integer that serves as a foreign key referencing the `Playlist` table. It is part of the composite primary key.- **TrackId**: An integer that serves as a foreign key referencing the `Track` table. It is also part of the composite primary key.The primary key for this table is a composite key consisting of both `PlaylistId` and `TrackId`, ensuring that each track can be uniquely associated with a playlist. The table enforces referential integrity by linking to the `Track` and `Playlist` tables through foreign keys.\\nDealing with high-cardinality columns\\u200b\\nIn order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.\\nWe can achieve this by creating a vector store with all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.\\nFirst we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:\\nimport astimport redef query_as_list(db, query):    res = db.run(query)    res = [el for sub in ast.literal_eval(res) for el in sub if el]    res = [re.sub(r\"\\\\b\\\\d+\\\\b\", \"\", string).strip() for string in res]    return list(set(res))artists = query_as_list(db, \"SELECT Name FROM Artist\")albums = query_as_list(db, \"SELECT Title FROM Album\")albums[:5]\\n[\\'In Through The Out Door\\', \\'Transmission\\', \\'Battlestar Galactica (Classic), Season\\', \\'A Copland Celebration, Vol. I\\', \\'Quiet Songs\\']\\nUsing this function, we can create a retriever tool that the agent can execute at its discretion.\\nLet\\'s select an embeddings model and vector store for this step:\\nSelect an embedding model:\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nSelect a vector store:\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nWe can now construct a retrieval tool that can search over relevant proper nouns in the database:\\nfrom langchain.agents.agent_toolkits import create_retriever_tool_ = vector_store.add_texts(artists + albums)retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})description = (    \"Use to look up values to filter on. Input is an approximate spelling \"    \"of the proper noun, output is valid proper nouns. Use the noun most \"    \"similar to the search.\")retriever_tool = create_retriever_tool(    retriever,    name=\"search_proper_nouns\",    description=description,)\\nLet\\'s try it out:\\nprint(retriever_tool.invoke(\"Alice Chains\"))\\nAlice In ChainsAlanis MorissettePearl JamPearl JamAudioslave\\nThis way, if the agent determines it needs to write a filter based on an artist along the lines of \"Alice Chains\", it can first use the retriever tool to observe relevant values of a column.\\nPutting this together:\\n# Add to system messagesuffix = (    \"If you need to filter on a proper noun like a Name, you must ALWAYS first look up \"    \"the filter value using the \\'search_proper_nouns\\' tool! Do not try to \"    \"guess at the proper name - use this function to find similar ones.\")system = f\"{system_message}\\\\n\\\\n{suffix}\"tools.append(retriever_tool)agent = create_react_agent(llm, tools, prompt=system)\\nquestion = \"How many albums does alis in chain have?\"for step in agent.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================How many albums does alis in chain have?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  search_proper_nouns (call_8ryjsRPLAr79mM3Qvnq6gTOH) Call ID: call_8ryjsRPLAr79mM3Qvnq6gTOH  Args:    query: alis in chain=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: search_proper_nounsAlice In ChainsAisha DuoXisDa Lama Ao CaosA-Sides==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_list_tables (call_NJjtCpU89MBMplssjn1z0xzq) Call ID: call_NJjtCpU89MBMplssjn1z0xzq  Args:  search_proper_nouns (call_1BfrueC9koSIyi4OfMu2Ao8q) Call ID: call_1BfrueC9koSIyi4OfMu2Ao8q  Args:    query: Alice In Chains=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: search_proper_nounsAlice In ChainsPearl JamPearl JamFoo FightersSoundgarden==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_schema (call_Kn09w9jd9swcNzIZ1b5MlKID) Call ID: call_Kn09w9jd9swcNzIZ1b5MlKID  Args:    table_names: Album, Artist=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_schemaCREATE TABLE \"Album\" (\\t\"AlbumId\" INTEGER NOT NULL, \\t\"Title\" NVARCHAR(160) NOT NULL, \\t\"ArtistId\" INTEGER NOT NULL, \\tPRIMARY KEY (\"AlbumId\"), \\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\"))/*3 rows from Album table:AlbumId\\tTitle\\tArtistId1\\tFor Those About To Rock We Salute You\\t12\\tBalls to the Wall\\t23\\tRestless and Wild\\t2*/CREATE TABLE \"Artist\" (\\t\"ArtistId\" INTEGER NOT NULL, \\t\"Name\" NVARCHAR(120), \\tPRIMARY KEY (\"ArtistId\"))/*3 rows from Artist table:ArtistId\\tName1\\tAC/DC2\\tAccept3\\tAerosmith*/==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_query (call_WkHRiPcBoGN9bc58MIupRHKP) Call ID: call_WkHRiPcBoGN9bc58MIupRHKP  Args:    query: SELECT COUNT(*) FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = \\'Alice In Chains\\')=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_query[(1,)]==================================\\x1b[1m Ai Message \\x1b[0m==================================Alice In Chains has released 1 album in the database.\\nAs we can see, both in the streamed steps and in the LangSmith trace, the agent used the search_proper_nouns tool in order to check how to correctly query the database for this specific artist.Edit this pagePreviousBuild a semantic search engineNextSummarize Text‚ö†Ô∏è Security note ‚ö†Ô∏èArchitectureSetupSample dataChainsApplication stateConvert question to SQL queryExecute queryGenerate answerOrchestrating with LangGraphHuman-in-the-loopNext stepsAgentsSystem PromptInitializing agentDealing with high-cardinality columnsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/summarization/', 'title': 'Summarize Text | ü¶úÔ∏èüîó LangChain', 'description': 'This tutorial demonstrates text summarization using built-in chains and LangGraph.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nSummarize Text | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsSummarize TextOn this pageSummarize Text\\ninfoThis tutorial demonstrates text summarization using built-in chains and LangGraph.A previous version of this page showcased the legacy chains StuffDocumentsChain, MapReduceDocumentsChain, and RefineDocumentsChain. See here for information on using those abstractions and a comparison with the methods demonstrated in this tutorial.\\nSuppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content.\\nLLMs are a great tool for this given their proficiency in understanding and synthesizing text.\\nIn the context of retrieval-augmented generation, summarizing text can help distill the information in a large number of retrieved documents to provide context for a LLM.\\nIn this walkthrough we\\'ll go over how to summarize content from multiple documents using LLMs.\\n\\nConcepts\\u200b\\nConcepts we will cover are:\\n\\n\\nUsing language models.\\n\\n\\nUsing document loaders, specifically the WebBaseLoader to load content from an HTML webpage.\\n\\n\\nTwo ways to summarize or otherwise combine documents.\\n\\nStuff, which simply concatenates documents into a prompt;\\nMap-reduce, for larger sets of documents. This splits documents into batches, summarizes those, and then summarizes the summaries.\\n\\n\\n\\nShorter, targeted guides on these strategies and others, including iterative refinement, can be found in the how-to guides.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nOverview\\u200b\\nA central question for building a summarizer is how to pass your documents into the LLM\\'s context window. Two common approaches for this are:\\n\\n\\nStuff: Simply \"stuff\" all your documents into a single prompt. This is the simplest approach (see here for more on the create_stuff_documents_chain constructor, which is used for this method).\\n\\n\\nMap-reduce: Summarize each document on its own in a \"map\" step and then \"reduce\" the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method).\\n\\n\\nNote that map-reduce is especially effective when understanding of a sub-document does not rely on preceding context. For example, when summarizing a corpus of many, shorter documents. In other cases, such as summarizing a novel or body of text with an inherent sequence, iterative refinement may be more effective.\\n\\nSetup\\u200b\\nFirst set environment variables and install packages:\\n%pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4 langchain-community# Set env var OPENAI_API_KEY or load from a .env file# import dotenv# dotenv.load_dotenv()\\nimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nFirst we load in our documents. We will use WebBaseLoader to load a blog post:\\nfrom langchain_community.document_loaders import WebBaseLoaderloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")docs = loader.load()\\nLet\\'s next select a LLM:\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nStuff: summarize in a single LLM call\\u200b\\nWe can use create_stuff_documents_chain, especially if using larger context window models such as:\\n\\n128k token OpenAI gpt-4o\\n200k token Anthropic claude-3-5-sonnet-latest\\n\\nThe chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:\\nfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain.chains.llm import LLMChainfrom langchain_core.prompts import ChatPromptTemplate# Define promptprompt = ChatPromptTemplate.from_messages(    [(\"system\", \"Write a concise summary of the following:\\\\\\\\n\\\\\\\\n{context}\")])# Instantiate chainchain = create_stuff_documents_chain(llm, prompt)# Invoke chainresult = chain.invoke({\"context\": docs})print(result)API Reference:ChatPromptTemplate\\nThe article \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture that includes three main components: Planning, Memory, and Tool Use. 1. **Planning** involves task decomposition, where complex tasks are broken down into manageable subgoals, and self-reflection, allowing agents to learn from past actions to improve future performance. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for enhancing reasoning and planning.2. **Memory** is categorized into short-term and long-term memory, with mechanisms for fast retrieval using Maximum Inner Product Search (MIPS) algorithms. This allows agents to retain and recall information effectively.3. **Tool Use** enables agents to interact with external APIs and tools, enhancing their capabilities beyond the limitations of their training data. Examples include MRKL systems and frameworks like HuggingGPT, which facilitate task planning and execution.The article also addresses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes with case studies demonstrating the practical applications of these concepts in scientific discovery and interactive simulations. Overall, the article emphasizes the potential of LLMs as powerful problem solvers in autonomous agent systems.\\nStreaming\\u200b\\nNote that we can also stream the result token-by-token:\\nfor token in chain.stream({\"context\": docs}):    print(token, end=\"|\")\\n|The| article| \"|LL|M| Powered| Autonomous| Agents|\"| by| Lil|ian| W|eng| discusses| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| outlines| a| system| architecture| that| includes| three| main| components|:| Planning|,| Memory|,| and| Tool| Use|.| |1|.| **|Planning|**| involves| task| decomposition|,| where| complex| tasks| are| broken| down| into| manageable| sub|go|als|,| and| self|-ref|lection|,| allowing| agents| to| learn| from| past| actions| to| improve| future| performance|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| enhancing| reasoning| and| planning|.|2|.| **|Memory|**| is| categorized| into| short|-term| and| long|-term| memory|,| with| mechanisms| for| fast| retrieval| using| Maximum| Inner| Product| Search| (|M|IPS|)| algorithms|.| This| allows| agents| to| retain| and| recall| information| effectively|.|3|.| **|Tool| Use|**| emphasizes| the| integration| of| external| APIs| and| tools| to| extend| the| capabilities| of| L|LM|s|,| enabling| them| to| perform| tasks| beyond| their| inherent| limitations|.| Examples| include| MR|KL| systems| and| frameworks| like| Hug|ging|GPT|,| which| facilitate| task| planning| and| execution|.|The| article| also| addresses| challenges| such| as| finite| context| length|,| difficulties| in| long|-term| planning|,| and| the| reliability| of| natural| language| interfaces|.| It| concludes| with| case| studies| demonstrating| the| practical| applications| of| L|LM|-powered| agents| in| scientific| discovery| and| interactive| simulations|.| Overall|,| the| piece| illustrates| the| potential| of| L|LM|s| as| general| problem| sol|vers| and| their| evolving| role| in| autonomous| systems|.||\\nGo deeper\\u200b\\n\\nYou can easily customize the prompt.\\nYou can easily try different LLMs, (e.g., Claude) via the llm parameter.\\n\\nMap-Reduce: summarize long texts via parallelization\\u200b\\nLet\\'s unpack the map reduce approach. For this, we\\'ll first map each document to an individual summary using an LLM. Then we\\'ll reduce or consolidate those summaries into a single global summary.\\nNote that the map step is typically parallelized over the input documents.\\nLangGraph, built on top of langchain-core, supports map-reduce workflows and is well-suited to this problem:\\n\\nLangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;\\nLangGraph\\'s checkpointing supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.\\nThe LangGraph implementation is straightforward to modify and extend, as we will see below.\\n\\nMap\\u200b\\nLet\\'s first define the prompt associated with the map step. We can use the same summarization prompt as in the stuff approach, above:\\nfrom langchain_core.prompts import ChatPromptTemplatemap_prompt = ChatPromptTemplate.from_messages(    [(\"system\", \"Write a concise summary of the following:\\\\\\\\n\\\\\\\\n{context}\")])API Reference:ChatPromptTemplate\\nWe can also use the Prompt Hub to store and fetch prompts.\\nThis will work with your LangSmith API key.\\nFor example, see the map prompt here.\\nfrom langchain import hubmap_prompt = hub.pull(\"rlm/map-prompt\")\\nReduce\\u200b\\nWe also define a prompt that takes the document mapping results and reduces them into a single output.\\n# Also available via the hub: `hub.pull(\"rlm/reduce-prompt\")`reduce_template = \"\"\"The following is a set of summaries:{docs}Take these and distill it into a final, consolidated summaryof the main themes.\"\"\"reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\\nOrchestration via LangGraph\\u200b\\nBelow we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.\\nMap-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model\\'s context window size. Here we implement a recursive \"collapsing\" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.\\nFirst we chunk the blog post into smaller \"sub documents\" to be mapped:\\nfrom langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    chunk_size=1000, chunk_overlap=0)split_docs = text_splitter.split_documents(docs)print(f\"Generated {len(split_docs)} documents.\")\\nCreated a chunk of size 1003, which is longer than the specified 1000``````outputGenerated 14 documents.\\nNext, we define our graph. Note that we define an artificially low maximum token length of 1,000 tokens to illustrate the \"collapsing\" step.\\nimport operatorfrom typing import Annotated, List, Literal, TypedDictfrom langchain.chains.combine_documents.reduce import (    acollapse_docs,    split_list_of_docs,)from langchain_core.documents import Documentfrom langgraph.constants import Sendfrom langgraph.graph import END, START, StateGraphtoken_max = 1000def length_function(documents: List[Document]) -> int:    \"\"\"Get number of tokens for input contents.\"\"\"    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)# This will be the overall state of the main graph.# It will contain the input document contents, corresponding# summaries, and a final summary.class OverallState(TypedDict):    # Notice here we use the operator.add    # This is because we want combine all the summaries we generate    # from individual nodes back into one list - this is essentially    # the \"reduce\" part    contents: List[str]    summaries: Annotated[list, operator.add]    collapsed_summaries: List[Document]    final_summary: str# This will be the state of the node that we will \"map\" all# documents to in order to generate summariesclass SummaryState(TypedDict):    content: str# Here we generate a summary, given a documentasync def generate_summary(state: SummaryState):    prompt = map_prompt.invoke(state[\"content\"])    response = await llm.ainvoke(prompt)    return {\"summaries\": [response.content]}# Here we define the logic to map out over the documents# We will use this an edge in the graphdef map_summaries(state: OverallState):    # We will return a list of `Send` objects    # Each `Send` object consists of the name of a node in the graph    # as well as the state to send to that node    return [        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]    ]def collect_summaries(state: OverallState):    return {        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]    }async def _reduce(input: dict) -> str:    prompt = reduce_prompt.invoke(input)    response = await llm.ainvoke(prompt)    return response.content# Add node to collapse summariesasync def collapse_summaries(state: OverallState):    doc_lists = split_list_of_docs(        state[\"collapsed_summaries\"], length_function, token_max    )    results = []    for doc_list in doc_lists:        results.append(await acollapse_docs(doc_list, _reduce))    return {\"collapsed_summaries\": results}# This represents a conditional edge in the graph that determines# if we should collapse the summaries or notdef should_collapse(    state: OverallState,) -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:    num_tokens = length_function(state[\"collapsed_summaries\"])    if num_tokens > token_max:        return \"collapse_summaries\"    else:        return \"generate_final_summary\"# Here we will generate the final summaryasync def generate_final_summary(state: OverallState):    response = await _reduce(state[\"collapsed_summaries\"])    return {\"final_summary\": response}# Construct the graph# Nodes:graph = StateGraph(OverallState)graph.add_node(\"generate_summary\", generate_summary)  # same as beforegraph.add_node(\"collect_summaries\", collect_summaries)graph.add_node(\"collapse_summaries\", collapse_summaries)graph.add_node(\"generate_final_summary\", generate_final_summary)# Edges:graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])graph.add_edge(\"generate_summary\", \"collect_summaries\")graph.add_conditional_edges(\"collect_summaries\", should_collapse)graph.add_conditional_edges(\"collapse_summaries\", should_collapse)graph.add_edge(\"generate_final_summary\", END)app = graph.compile()API Reference:Document | Send | StateGraph\\nLangGraph allows the graph structure to be plotted to help visualize its function:\\nfrom IPython.display import ImageImage(app.get_graph().draw_mermaid_png())\\n\\nWhen running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.\\nNote that because we have a loop in the graph, it can be helpful to specify a recursion_limit on its execution. This will raise a specific error when the specified limit is exceeded.\\nasync for step in app.astream(    {\"contents\": [doc.page_content for doc in split_docs]},    {\"recursion_limit\": 10},):    print(list(step.keys()))\\n[\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'collect_summaries\\'][\\'collapse_summaries\\'][\\'collapse_summaries\\'][\\'generate_final_summary\\']\\nprint(step)\\n{\\'generate_final_summary\\': {\\'final_summary\\': \\'The consolidated summary of the main themes from the provided documents is as follows:\\\\n\\\\n1. **Integration of Large Language Models (LLMs) in Autonomous Agents**: The documents explore the evolving role of LLMs in autonomous systems, emphasizing their enhanced reasoning and acting capabilities through methodologies that incorporate structured planning, memory systems, and tool use.\\\\n\\\\n2. **Core Components of Autonomous Agents**:\\\\n   - **Planning**: Techniques like task decomposition (e.g., Chain of Thought) and external classical planners are utilized to facilitate long-term planning by breaking down complex tasks.\\\\n   - **Memory**: The memory system is divided into short-term (in-context learning) and long-term memory, with parallels drawn between human memory and machine learning to improve agent performance.\\\\n   - **Tool Use**: Agents utilize external APIs and algorithms to enhance problem-solving abilities, exemplified by frameworks like HuggingGPT that manage task workflows.\\\\n\\\\n3. **Neuro-Symbolic Architectures**: The integration of MRKL (Modular Reasoning, Knowledge, and Language) systems combines neural and symbolic expert modules with LLMs, addressing challenges in tasks such as verbal math problem-solving.\\\\n\\\\n4. **Specialized Applications**: Case studies, such as ChemCrow and projects in anticancer drug discovery, demonstrate the advantages of LLMs augmented with expert tools in specialized domains.\\\\n\\\\n5. **Challenges and Limitations**: The documents highlight challenges such as hallucination in model outputs and the finite context length of LLMs, which affects their ability to incorporate historical information and perform self-reflection. Techniques like Chain of Hindsight and Algorithm Distillation are discussed to enhance model performance through iterative learning.\\\\n\\\\n6. **Structured Software Development**: A systematic approach to creating Python software projects is emphasized, focusing on defining core components, managing dependencies, and adhering to best practices for documentation.\\\\n\\\\nOverall, the integration of structured planning, memory systems, and advanced tool use aims to enhance the capabilities of LLM-powered autonomous agents while addressing the challenges and limitations these technologies face in real-world applications.\\'}}\\nIn the corresponding LangSmith trace we can see the individual LLM calls, grouped under their respective nodes.\\nGo deeper\\u200b\\nCustomization\\n\\nAs shown above, you can customize the LLMs and prompts for map and reduce stages.\\n\\nReal-world use-case\\n\\nSee this blog post case-study on analyzing user interactions (questions about LangChain documentation)!\\nThe blog post and associated repo also introduce clustering as a means of summarization.\\nThis opens up another path beyond the stuff or map-reduce approaches that is worth considering.\\n\\n\\nNext steps\\u200b\\nWe encourage you to check out the how-to guides for more detail on:\\n\\nOther summarization strategies, such as iterative refinement\\nBuilt-in document loaders and text-splitters\\nIntegrating various combine-document chains into a RAG application\\nIncorporating retrieval into a chatbot\\n\\nand other concepts.Edit this pagePreviousBuild a Question/Answering system over SQL dataNextHow-to guidesConceptsSetupJupyter NotebookInstallationLangSmithOverviewSetupStuff: summarize in a single LLM callStreamingGo deeperMap-Reduce: summarize long texts via parallelizationMapReduceOrchestration via LangGraphGo deeperNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/graph/', 'title': 'Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain', 'description': \"In this guide we'll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer. First, we will show a simple out-of-the-box option and then implement a more sophisticated version with LangGraph.\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Question Answering application over a Graph DatabaseOn this pageBuild a Question Answering application over a Graph Database\\nIn this guide we\\'ll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer. First, we will show a simple out-of-the-box option and then implement a more sophisticated version with LangGraph.\\n‚ö†Ô∏è Security note ‚ö†Ô∏è\\u200b\\nBuilding Q&A systems of graph databases requires executing model-generated graph queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent\\'s needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see here.\\nArchitecture\\u200b\\nAt a high-level, the steps of most graph chains are:\\n\\nConvert question to a graph database query: Model converts user input to a graph database query (e.g. Cypher).\\nExecute graph database query: Execute the graph database query.\\nAnswer the question: Model responds to user input using the query results.\\n\\n\\nSetup\\u200b\\nFirst, get required packages and set environment variables.\\nIn this example, we will be using Neo4j graph database.\\n%pip install --upgrade --quiet langchain langchain-neo4j langchain-openai langgraph\\nWe default to OpenAI models in this guide.\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")# Uncomment the below to use LangSmith. Not required.# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nEnter your OpenAI API key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\\nNext, we need to define Neo4j credentials.\\nFollow these installation steps to set up a Neo4j database.\\nos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"os.environ[\"NEO4J_PASSWORD\"] = \"password\"\\nThe below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.\\nfrom langchain_neo4j import Neo4jGraphgraph = Neo4jGraph()# Import movie informationmovies_query = \"\"\"LOAD CSV WITH HEADERS FROM \\'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv\\'AS rowMERGE (m:Movie {id:row.movieId})SET m.released = date(row.released),    m.title = row.title,    m.imdbRating = toFloat(row.imdbRating)FOREACH (director in split(row.director, \\'|\\') |     MERGE (p:Person {name:trim(director)})    MERGE (p)-[:DIRECTED]->(m))FOREACH (actor in split(row.actors, \\'|\\') |     MERGE (p:Person {name:trim(actor)})    MERGE (p)-[:ACTED_IN]->(m))FOREACH (genre in split(row.genres, \\'|\\') |     MERGE (g:Genre {name:trim(genre)})    MERGE (m)-[:IN_GENRE]->(g))\"\"\"graph.query(movies_query)\\n[]\\nGraph schema\\u200b\\nIn order for an LLM to be able to generate a Cypher statement, it needs information about the graph schema. When you instantiate a graph object, it retrieves the information about the graph schema. If you later make any changes to the graph, you can run the refresh_schema method to refresh the schema information.\\ngraph.refresh_schema()print(graph.schema)\\nNode properties:Person {name: STRING}Movie {id: STRING, released: DATE, title: STRING, imdbRating: FLOAT}Genre {name: STRING}Chunk {id: STRING, embedding: LIST, text: STRING, question: STRING, query: STRING}Relationship properties:The relationships:(:Person)-[:DIRECTED]->(:Movie)(:Person)-[:ACTED_IN]->(:Movie)(:Movie)-[:IN_GENRE]->(:Genre)\\nFor more involved schema information, you can use enhanced_schema option.\\nenhanced_graph = Neo4jGraph(enhanced_schema=True)print(enhanced_graph.schema)\\nReceived notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. (\\'config\\' used by \\'apoc.meta.graphSample\\' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, \\'type\\'), count: apoc.any.property(rel, \\'count\\')}] AS relationships\"``````outputNode properties:- **Person**  - `name`: STRING Example: \"John Lasseter\"- **Movie**  - `id`: STRING Example: \"1\"  - `released`: DATE Min: 1964-12-16, Max: 1996-09-15  - `title`: STRING Example: \"Toy Story\"  - `imdbRating`: FLOAT Min: 2.4, Max: 9.3- **Genre**  - `name`: STRING Example: \"Adventure\"- **Chunk**  - `id`: STRING Available options: [\\'d66006059fd78d63f3df90cc1059639a\\', \\'0e3dcb4502853979d12357690a95ec17\\', \\'c438c6bcdcf8e4fab227f29f8e7ff204\\', \\'97fe701ec38057594464beaa2df0710e\\', \\'b54f9286e684373498c4504b4edd9910\\', \\'5b50a72c3a4954b0ff7a0421be4f99b9\\', \\'fb28d41771e717255f0d8f6c799ede32\\', \\'58e6f14dd2e6c6702cf333f2335c499c\\']  - `text`: STRING Available options: [\\'How many artists are there?\\', \\'Which actors played in the movie Casino?\\', \\'How many movies has Tom Hanks acted in?\\', \"List all the genres of the movie Schindler\\'s List\", \\'Which actors have worked in movies from both the c\\', \\'Which directors have made movies with at least thr\\', \\'Identify movies where directors also played a role\\', \\'Find the actor with the highest number of movies i\\']  - `question`: STRING Available options: [\\'How many artists are there?\\', \\'Which actors played in the movie Casino?\\', \\'How many movies has Tom Hanks acted in?\\', \"List all the genres of the movie Schindler\\'s List\", \\'Which actors have worked in movies from both the c\\', \\'Which directors have made movies with at least thr\\', \\'Identify movies where directors also played a role\\', \\'Find the actor with the highest number of movies i\\']  - `query`: STRING Available options: [\\'MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN coun\\', \"MATCH (m:Movie {title: \\'Casino\\'})<-[:ACTED_IN]-(a)\", \"MATCH (a:Person {name: \\'Tom Hanks\\'})-[:ACTED_IN]->\", \"MATCH (m:Movie {title: \\'Schindler\\'s List\\'})-[:IN_G\", \\'MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]\\', \\'MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_I\\', \\'MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACT\\', \\'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.na\\']Relationship properties:The relationships:(:Person)-[:DIRECTED]->(:Movie)(:Person)-[:ACTED_IN]->(:Movie)(:Movie)-[:IN_GENRE]->(:Genre)\\nThe enhanced_schema option enriches property information by including details such as minimum and maximum values for floats and dates, as well as example values for string properties. This additional context helps guide the LLM toward generating more accurate and effective queries.\\nGreat! We\\'ve got a graph database that we can query. Now let\\'s try hooking it up to an LLM.\\nGraphQACypherChain\\u200b\\nLet\\'s use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question.\\n\\nLangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: GraphCypherQAChain\\nfrom langchain_neo4j import GraphCypherQAChainfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)chain = GraphCypherQAChain.from_llm(    graph=enhanced_graph, llm=llm, verbose=True, allow_dangerous_requests=True)response = chain.invoke({\"query\": \"What was the cast of the Casino?\"})response\\n\\x1b[1m> Entering new GraphCypherQAChain chain...\\x1b[0mGenerated Cypher:\\x1b[32;1m\\x1b[1;3mcypherMATCH (p:Person)-[:ACTED_IN]->(m:Movie {title: \"Casino\"})RETURN p.name\\x1b[0mFull Context:\\x1b[32;1m\\x1b[1;3m[{\\'p.name\\': \\'Robert De Niro\\'}, {\\'p.name\\': \\'Joe Pesci\\'}, {\\'p.name\\': \\'Sharon Stone\\'}, {\\'p.name\\': \\'James Woods\\'}]\\x1b[0m\\x1b[1m> Finished chain.\\x1b[0m\\n{\\'query\\': \\'What was the cast of the Casino?\\', \\'result\\': \\'Robert De Niro, Joe Pesci, Sharon Stone, and James Woods were the cast of Casino.\\'}\\nAdvanced implementation with LangGraph\\u200b\\nWhile the GraphCypherQAChain is effective for quick demonstrations, it may face challenges in production environments. Transitioning to LangGraph can enhance the workflow, but implementing natural language to query flows in production remains a complex task. Nevertheless, there are several strategies to significantly improve accuracy and reliability, which we will explore next.\\nHere is the visualized LangGraph flow we will implement:\\n\\nWe will begin by defining the Input, Output, and Overall state of the LangGraph application.\\nfrom operator import addfrom typing import Annotated, Listfrom typing_extensions import TypedDictclass InputState(TypedDict):    question: strclass OverallState(TypedDict):    question: str    next_action: str    cypher_statement: str    cypher_errors: List[str]    database_records: List[dict]    steps: Annotated[List[str], add]class OutputState(TypedDict):    answer: str    steps: List[str]    cypher_statement: str\\nThe first step is a simple guardrails step, where we validate whether the question pertains to movies or their cast. If it doesn\\'t, we notify the user that we cannot answer any other questions. Otherwise, we move on to the Cypher generation step.\\nfrom typing import Literalfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldguardrails_system = \"\"\"As an intelligent assistant, your primary objective is to decide whether a given question is related to movies or not. If the question is related to movies, output \"movie\". Otherwise, output \"end\".To make this decision, assess the content of the question and determine if it refers to any movie, actor, director, film industry, or related topics. Provide only the specified output: \"movie\" or \"end\".\"\"\"guardrails_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            guardrails_system,        ),        (            \"human\",            (\"{question}\"),        ),    ])class GuardrailsOutput(BaseModel):    decision: Literal[\"movie\", \"end\"] = Field(        description=\"Decision on whether the question is related to movies\"    )guardrails_chain = guardrails_prompt | llm.with_structured_output(GuardrailsOutput)def guardrails(state: InputState) -> OverallState:    \"\"\"    Decides if the question is related to movies or not.    \"\"\"    guardrails_output = guardrails_chain.invoke({\"question\": state.get(\"question\")})    database_records = None    if guardrails_output.decision == \"end\":        database_records = \"This questions is not about movies or their cast. Therefore I cannot answer this question.\"    return {        \"next_action\": guardrails_output.decision,        \"database_records\": database_records,        \"steps\": [\"guardrail\"],    }API Reference:ChatPromptTemplate\\nFew-shot prompting\\u200b\\nConverting natural language into accurate queries is challenging. One way to enhance this process is by providing relevant few-shot examples to guide the LLM in query generation. To achieve this, we will use the SemanticSimilarityExampleSelector to dynamically select the most relevant examples.\\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_neo4j import Neo4jVectorfrom langchain_openai import OpenAIEmbeddingsexamples = [    {        \"question\": \"How many artists are there?\",        \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)\",    },    {        \"question\": \"Which actors played in the movie Casino?\",        \"query\": \"MATCH (m:Movie {title: \\'Casino\\'})<-[:ACTED_IN]-(a) RETURN a.name\",    },    {        \"question\": \"How many movies has Tom Hanks acted in?\",        \"query\": \"MATCH (a:Person {name: \\'Tom Hanks\\'})-[:ACTED_IN]->(m:Movie) RETURN count(m)\",    },    {        \"question\": \"List all the genres of the movie Schindler\\'s List\",        \"query\": \"MATCH (m:Movie {title: \\'Schindler\\'s List\\'})-[:IN_GENRE]->(g:Genre) RETURN g.name\",    },    {        \"question\": \"Which actors have worked in movies from both the comedy and action genres?\",        \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = \\'Comedy\\' AND g2.name = \\'Action\\' RETURN DISTINCT a.name\",    },    {        \"question\": \"Which directors have made movies with at least three different actors named \\'John\\'?\",        \"query\": \"MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH \\'John\\' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name\",    },    {        \"question\": \"Identify movies where directors also played a role in the film.\",        \"query\": \"MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACTED_IN]->(m) RETURN m.title, p.name\",    },    {        \"question\": \"Find the actor with the highest number of movies in the database.\",        \"query\": \"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1\",    },]example_selector = SemanticSimilarityExampleSelector.from_examples(    examples, OpenAIEmbeddings(), Neo4jVector, k=5, input_keys=[\"question\"])API Reference:SemanticSimilarityExampleSelector\\nNext, we implement the Cypher generation chain, also known as text2cypher. The prompt includes an enhanced graph schema, dynamically selected few-shot examples, and the user‚Äôs question. This combination enables the generation of a Cypher query to retrieve relevant information from the database.\\nfrom langchain_core.output_parsers import StrOutputParsertext2cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            (                \"Given an input question, convert it to a Cypher query. No pre-amble.\"                \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"            ),        ),        (            \"human\",            (                \"\"\"You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!Here is the schema information{schema}Below are a number of examples of questions and their corresponding Cypher queries.{fewshot_examples}User input: {question}Cypher query:\"\"\"            ),        ),    ])text2cypher_chain = text2cypher_prompt | llm | StrOutputParser()def generate_cypher(state: OverallState) -> OverallState:    \"\"\"    Generates a cypher statement based on the provided schema and user input    \"\"\"    NL = \"\\\\n\"    fewshot_examples = (NL * 2).join(        [            f\"Question: {el[\\'question\\']}{NL}Cypher:{el[\\'query\\']}\"            for el in example_selector.select_examples(                {\"question\": state.get(\"question\")}            )        ]    )    generated_cypher = text2cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"fewshot_examples\": fewshot_examples,            \"schema\": enhanced_graph.schema,        }    )    return {\"cypher_statement\": generated_cypher, \"steps\": [\"generate_cypher\"]}API Reference:StrOutputParser\\nQuery validation\\u200b\\nThe next step is to validate the generated Cypher statement and ensuring that all property values are accurate. While numbers and dates typically don‚Äôt require validation, strings such as movie titles or people‚Äôs names do. In this example, we‚Äôll use a basic CONTAINS clause for validation, though more advanced mapping and validation techniques can be implemented if needed.\\nFirst, we will create a chain that detects any errors in the Cypher statement and extracts the property values it references.\\nfrom typing import List, Optionalvalidate_cypher_system = \"\"\"You are a Cypher expert reviewing a statement written by a junior developer.\"\"\"validate_cypher_user = \"\"\"You must check the following:* Are there any syntax errors in the Cypher statement?* Are there any missing or undefined variables in the Cypher statement?* Are any node labels missing from the schema?* Are any relationship types missing from the schema?* Are any of the properties not included in the schema?* Does the Cypher statement include enough information to answer the question?Examples of good errors:* Label (:Foo) does not exist, did you mean (:Bar)?* Property bar does not exist for label Foo, did you mean baz?* Relationship FOO does not exist, did you mean FOO_BAR?Schema:{schema}The question is:{question}The Cypher statement is:{cypher}Make sure you don\\'t make any mistakes!\"\"\"validate_cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            validate_cypher_system,        ),        (            \"human\",            (validate_cypher_user),        ),    ])class Property(BaseModel):    \"\"\"    Represents a filter condition based on a specific node property in a graph in a Cypher statement.    \"\"\"    node_label: str = Field(        description=\"The label of the node to which this property belongs.\"    )    property_key: str = Field(description=\"The key of the property being filtered.\")    property_value: str = Field(        description=\"The value that the property is being matched against.\"    )class ValidateCypherOutput(BaseModel):    \"\"\"    Represents the validation result of a Cypher query\\'s output,    including any errors and applied filters.    \"\"\"    errors: Optional[List[str]] = Field(        description=\"A list of syntax or semantical errors in the Cypher statement. Always explain the discrepancy between schema and Cypher statement\"    )    filters: Optional[List[Property]] = Field(        description=\"A list of property-based filters applied in the Cypher statement.\"    )validate_cypher_chain = validate_cypher_prompt | llm.with_structured_output(    ValidateCypherOutput)\\nLLMs often struggle with correctly determining relationship directions in generated Cypher statements. Since we have access to the schema, we can deterministically correct these directions using the CypherQueryCorrector.\\nNote: The CypherQueryCorrector is an experimental feature and doesn\\'t support all the newest Cypher syntax.\\nfrom langchain_neo4j.chains.graph_qa.cypher_utils import CypherQueryCorrector, Schema# Cypher query corrector is experimentalcorrector_schema = [    Schema(el[\"start\"], el[\"type\"], el[\"end\"])    for el in enhanced_graph.structured_schema.get(\"relationships\")]cypher_query_corrector = CypherQueryCorrector(corrector_schema)\\nNow we can implement the Cypher validation step. First, we use the EXPLAIN method to detect any syntax errors. Next, we leverage the LLM to identify potential issues and extract the properties used for filtering. For string properties, we validate them against the database using a simple CONTAINS clause.\\nBased on the validation results, the process can take the following paths:\\n\\nIf value mapping fails, we end the conversation and inform the user that we couldn\\'t identify a specific property value (e.g., a person or movie title).\\nIf errors are found, we route the query for correction.\\nIf no issues are detected, we proceed to the Cypher execution step.\\n\\nfrom neo4j.exceptions import CypherSyntaxErrordef validate_cypher(state: OverallState) -> OverallState:    \"\"\"    Validates the Cypher statements and maps any property values to the database.    \"\"\"    errors = []    mapping_errors = []    # Check for syntax errors    try:        enhanced_graph.query(f\"EXPLAIN {state.get(\\'cypher_statement\\')}\")    except CypherSyntaxError as e:        errors.append(e.message)    # Experimental feature for correcting relationship directions    corrected_cypher = cypher_query_corrector(state.get(\"cypher_statement\"))    if not corrected_cypher:        errors.append(\"The generated Cypher statement doesn\\'t fit the graph schema\")    if not corrected_cypher == state.get(\"cypher_statement\"):        print(\"Relationship direction was corrected\")    # Use LLM to find additional potential errors and get the mapping for values    llm_output = validate_cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"schema\": enhanced_graph.schema,            \"cypher\": state.get(\"cypher_statement\"),        }    )    if llm_output.errors:        errors.extend(llm_output.errors)    if llm_output.filters:        for filter in llm_output.filters:            # Do mapping only for string values            if (                not [                    prop                    for prop in enhanced_graph.structured_schema[\"node_props\"][                        filter.node_label                    ]                    if prop[\"property\"] == filter.property_key                ][0][\"type\"]                == \"STRING\"            ):                continue            mapping = enhanced_graph.query(                f\"MATCH (n:{filter.node_label}) WHERE toLower(n.`{filter.property_key}`) = toLower($value) RETURN \\'yes\\' LIMIT 1\",                {\"value\": filter.property_value},            )            if not mapping:                print(                    f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\"                )                mapping_errors.append(                    f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\"                )    if mapping_errors:        next_action = \"end\"    elif errors:        next_action = \"correct_cypher\"    else:        next_action = \"execute_cypher\"    return {        \"next_action\": next_action,        \"cypher_statement\": corrected_cypher,        \"cypher_errors\": errors,        \"steps\": [\"validate_cypher\"],    }\\nThe Cypher correction step takes the existing Cypher statement, any identified errors, and the original question to generate a corrected version of the query.\\ncorrect_cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            (                \"You are a Cypher expert reviewing a statement written by a junior developer. \"                \"You need to correct the Cypher statement based on the provided errors. No pre-amble.\"                \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"            ),        ),        (            \"human\",            (                \"\"\"Check for invalid syntax or semantics and return a corrected Cypher statement.Schema:{schema}Note: Do not include any explanations or apologies in your responses.Do not wrap the response in any backticks or anything else.Respond with a Cypher statement only!Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.The question is:{question}The Cypher statement is:{cypher}The errors are:{errors}Corrected Cypher statement: \"\"\"            ),        ),    ])correct_cypher_chain = correct_cypher_prompt | llm | StrOutputParser()def correct_cypher(state: OverallState) -> OverallState:    \"\"\"    Correct the Cypher statement based on the provided errors.    \"\"\"    corrected_cypher = correct_cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"errors\": state.get(\"cypher_errors\"),            \"cypher\": state.get(\"cypher_statement\"),            \"schema\": enhanced_graph.schema,        }    )    return {        \"next_action\": \"validate_cypher\",        \"cypher_statement\": corrected_cypher,        \"steps\": [\"correct_cypher\"],    }\\nWe need to add a step that executes the given Cypher statement. If no results are returned, we should explicitly handle this scenario, as leaving the context empty can sometimes lead to LLM hallucinations.\\nno_results = \"I couldn\\'t find any relevant information in the database\"def execute_cypher(state: OverallState) -> OverallState:    \"\"\"    Executes the given Cypher statement.    \"\"\"    records = enhanced_graph.query(state.get(\"cypher_statement\"))    return {        \"database_records\": records if records else no_results,        \"next_action\": \"end\",        \"steps\": [\"execute_cypher\"],    }\\nThe final step is to generate the answer. This involves combining the initial question with the database output to produce a relevant response.\\ngenerate_final_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant\",        ),        (            \"human\",            (                \"\"\"Use the following results retrieved from a database to providea succinct, definitive answer to the user\\'s question.Respond as if you are answering the question directly.Results: {results}Question: {question}\"\"\"            ),        ),    ])generate_final_chain = generate_final_prompt | llm | StrOutputParser()def generate_final_answer(state: OverallState) -> OutputState:    \"\"\"    Decides if the question is related to movies.    \"\"\"    final_answer = generate_final_chain.invoke(        {\"question\": state.get(\"question\"), \"results\": state.get(\"database_records\")}    )    return {\"answer\": final_answer, \"steps\": [\"generate_final_answer\"]}\\nNext, we will implement the LangGraph workflow, starting with defining the conditional edge functions.\\ndef guardrails_condition(    state: OverallState,) -> Literal[\"generate_cypher\", \"generate_final_answer\"]:    if state.get(\"next_action\") == \"end\":        return \"generate_final_answer\"    elif state.get(\"next_action\") == \"movie\":        return \"generate_cypher\"def validate_cypher_condition(    state: OverallState,) -> Literal[\"generate_final_answer\", \"correct_cypher\", \"execute_cypher\"]:    if state.get(\"next_action\") == \"end\":        return \"generate_final_answer\"    elif state.get(\"next_action\") == \"correct_cypher\":        return \"correct_cypher\"    elif state.get(\"next_action\") == \"execute_cypher\":        return \"execute_cypher\"\\nLet\\'s put it all together now.\\nfrom IPython.display import Image, displayfrom langgraph.graph import END, START, StateGraphlanggraph = StateGraph(OverallState, input=InputState, output=OutputState)langgraph.add_node(guardrails)langgraph.add_node(generate_cypher)langgraph.add_node(validate_cypher)langgraph.add_node(correct_cypher)langgraph.add_node(execute_cypher)langgraph.add_node(generate_final_answer)langgraph.add_edge(START, \"guardrails\")langgraph.add_conditional_edges(    \"guardrails\",    guardrails_condition,)langgraph.add_edge(\"generate_cypher\", \"validate_cypher\")langgraph.add_conditional_edges(    \"validate_cypher\",    validate_cypher_condition,)langgraph.add_edge(\"execute_cypher\", \"generate_final_answer\")langgraph.add_edge(\"correct_cypher\", \"validate_cypher\")langgraph.add_edge(\"generate_final_answer\", END)langgraph = langgraph.compile()# Viewdisplay(Image(langgraph.get_graph().draw_mermaid_png()))API Reference:StateGraph\\n\\nWe can now test the application by asking an irrelevant question.\\nlanggraph.invoke({\"question\": \"What\\'s the weather in Spain?\"})\\n{\\'answer\\': \"I\\'m sorry, but I cannot provide current weather information. Please check a reliable weather website or app for the latest updates on the weather in Spain.\", \\'steps\\': [\\'guardrail\\', \\'generate_final_answer\\']}\\nLet\\'s now ask something relevant about the movies.\\nlanggraph.invoke({\"question\": \"What was the cast of the Casino?\"})\\n{\\'answer\\': \\'The cast of \"Casino\" includes Robert De Niro, Joe Pesci, Sharon Stone, and James Woods.\\', \\'steps\\': [\\'guardrail\\',  \\'generate_cypher\\',  \\'validate_cypher\\',  \\'execute_cypher\\',  \\'generate_final_answer\\'], \\'cypher_statement\\': \"MATCH (m:Movie {title: \\'Casino\\'})<-[:ACTED_IN]-(a:Person) RETURN a.name\"}\\nNext steps\\u200b\\nFor other graph techniques like this and more check out:\\n\\nSemantic layer: Techniques for implementing semantic layers.\\nConstructing graphs: Techniques for constructing knowledge graphs.\\nEdit this pagePreviousTutorialsNextTutorials‚ö†Ô∏è Security note ‚ö†Ô∏èArchitectureSetupGraph schemaGraphQACypherChainAdvanced implementation with LangGraphFew-shot promptingQuery validationNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ddfc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_list_langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e133f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,chunk_overlap=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b132446",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_split_langchain=text_spitter.split_documents(doc_list_langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e02f83d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_split_langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9d7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f44421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_langchain=FAISS.from_documents(\n",
    "    documents=doc_split_langchain,\n",
    "    embedding=embeddings\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16aa854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_retriever=vector_store_langchain.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "523c0a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='16c80f48-94d7-4680-bfdd-c6ce3eaf98bd', metadata={'source': 'https://python.langchain.com/docs/tutorials/classification/', 'title': 'Tagging | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='Tagging | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(id='ddc960f3-d177-4583-b973-59d3d8866494', metadata={'source': 'https://python.langchain.com/docs/tutorials/extraction/', 'title': 'Build an Extraction Chain | ü¶úÔ∏èüîó LangChain', 'description': 'In this tutorial, we will use tool-calling features of chat models to extract structured information from unstructured text. We will also demonstrate how to use few-shot prompting in this context to improve performance.', 'language': 'en'}, page_content='Build an Extraction Chain | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(id='891c6d71-4879-424c-b442-2ed226c3d5e0', metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Tutorials | ü¶úÔ∏èüîó LangChain', 'description': 'New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.', 'language': 'en'}, page_content='Tutorials | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(id='e57c9e6b-0f19-43dc-a98f-104c4321617a', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you'll have a high level overview of:\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_retriever.invoke('what is langchain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6cf27",
   "metadata": {},
   "source": [
    "# langgraph docs retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e31b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_langgraph=[\n",
    "    'https://langchain-ai.github.io/langgraph/concepts/why-langgraph/',\n",
    "    'https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/',\n",
    "    'https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/',\n",
    "    'https://langchain-ai.github.io/langgraph/tutorials/get-started/3-add-memory/',\n",
    "    'https://langchain-ai.github.io/langgraph/tutorials/get-started/4-human-in-the-loop/',\n",
    "    'https://langchain-ai.github.io/langgraph/tutorials/get-started/5-customize-state/',\n",
    "    'https://langchain-ai.github.io/langgraph/tutorials/get-started/6-time-travel/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "920366e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_langgraph=[WebBaseLoader(url).load() for url in urls_langchain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4b0deb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Tutorials | ü¶úÔ∏èüîó LangChain', 'description': 'New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTutorials | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsOn this pageTutorials\\nNew to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\\nGet started\\u200b\\nFamiliarize yourself with LangChain\\'s open-source components by building simple applications.\\nIf you\\'re looking to get started with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our supported integrations.\\n\\nChat models and prompts: Build a simple LLM application with prompt templates and chat models.\\nSemantic search: Build a semantic search engine over a PDF with document loaders, embedding models, and vector stores.\\nClassification: Classify text into categories or labels using chat models with structured outputs.\\nExtraction: Extract structured data from text and other unstructured media using chat models and few-shot examples.\\n\\nRefer to the how-to guides for more detail on using all LangChain components.\\nOrchestration\\u200b\\nGet started using LangGraph to assemble LangChain components into full-featured applications.\\n\\nChatbots: Build a chatbot that incorporates memory.\\nAgents: Build an agent that interacts with external tools.\\nRetrieval Augmented Generation (RAG) Part 1: Build an application that uses your own documents to inform its responses.\\nRetrieval Augmented Generation (RAG) Part 2: Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\\nQuestion-Answering with SQL: Build a question-answering system that executes SQL queries to inform its responses.\\nSummarization: Generate summaries of (potentially long) texts.\\nQuestion-Answering with Graph Databases: Build a question-answering system that queries a graph database to inform its responses.\\n\\nLangSmith\\u200b\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse LangSmith tutorials here.\\nEvaluation\\u200b\\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\\n\\nEvaluate your LLM application\\nEdit this pagePreviousIntroductionNextBuild a Question Answering application over a Graph DatabaseGet startedOrchestrationLangSmithEvaluationCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates\\nIn this quickstart we\\'ll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it\\'s just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you\\'ll have a high level overview of:\\n\\n\\nUsing language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet\\'s dive in!\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"export LANGSMITH_PROJECT=\"default\" # or any other project name\\nOr, if in a notebook, you can set them with:\\nimport getpassimport ostry:    # load environment variables from .env file (requires `python-dotenv`)    from dotenv import load_dotenv    load_dotenv()except ImportError:    passos.environ[\"LANGSMITH_TRACING\"] = \"true\"if \"LANGSMITH_API_KEY\" not in os.environ:    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(        prompt=\"Enter your LangSmith API key (optional): \"    )if \"LANGSMITH_PROJECT\" not in os.environ:    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(        prompt=\\'Enter your LangSmith Project Name (default = \"default\"): \\'    )    if not os.environ.get(\"LANGSMITH_PROJECT\"):        os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\\nUsing Language Models\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.\\n\\n\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(content=\"Translate the following from English into Italian\"),    HumanMessage(content=\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage\\nAIMessage(content=\\'Ciao!\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 3, \\'prompt_tokens\\': 20, \\'total_tokens\\': 23, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-32654a56-627c-40e1-a141-ad9350bbfd3e-0\\', usage_metadata={\\'input_tokens\\': 20, \\'output_tokens\\': 3, \\'total_tokens\\': 23, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\ntipIf we\\'ve enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\\nNote that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:\\n\\nlanguage: The language to translate text into\\ntext: The text to translate\\n\\nfrom langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])\\nWe can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\\nThat\\'s it! In this tutorial you\\'ve learned how to create your first simple LLM application. You\\'ve learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we\\'ve got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we\\'ve got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\\n\\nChat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pagePreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/retrievers/', 'title': 'Build a semantic search engine | ü¶úÔ∏èüîó LangChain', 'description': \"This tutorial will familiarize you with LangChain's document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here).\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a semantic search engine | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a semantic search engineOn this pageBuild a semantic search engine\\nThis tutorial will familiarize you with LangChain\\'s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here).\\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.\\nConcepts\\u200b\\nThis guide focuses on retrieval of text data. We will cover the following concepts:\\n\\nDocuments and document loaders;\\nText splitters;\\nEmbeddings;\\nVector stores and retrievers.\\n\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nThis tutorial requires the langchain-community and pypdf packages:\\n\\nPipCondapip install langchain-community pypdfconda install langchain-community pypdf -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nDocuments and Document Loaders\\u200b\\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\\n\\npage_content: a string representing the content;\\nmetadata: a dict containing arbitrary metadata;\\nid: (optional) a string identifier for the document.\\n\\nThe metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\\nWe can generate sample documents when desired:\\nfrom langchain_core.documents import Documentdocuments = [    Document(        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",        metadata={\"source\": \"mammal-pets-doc\"},    ),    Document(        page_content=\"Cats are independent pets that often enjoy their own space.\",        metadata={\"source\": \"mammal-pets-doc\"},    ),]API Reference:Document\\nHowever, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.\\nLoading documents\\u200b\\nLet\\'s load a PDF into a sequence of Document objects. There is a sample PDF in the LangChain repo here -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders. Let\\'s select PyPDFLoader, which is fairly lightweight.\\nfrom langchain_community.document_loaders import PyPDFLoaderfile_path = \"../example_data/nke-10k-2023.pdf\"loader = PyPDFLoader(file_path)docs = loader.load()print(len(docs))\\n107\\ntipSee this guide for more detail on PDF document loaders.\\nPyPDFLoader loads one Document object per PDF page. For each, we can easily access:\\n\\nThe string content of the page;\\nMetadata containing the file name and page number.\\n\\nprint(f\"{docs[0].page_content[:200]}\\\\n\")print(docs[0].metadata)\\nTable of ContentsUNITED STATESSECURITIES AND EXCHANGE COMMISSIONWashington, D.C. 20549FORM 10-K(Mark One)‚òë ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934FO{\\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'page\\': 0}\\nSplitting\\u200b\\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\\nwith 200 characters of overlap between chunks. The overlap helps\\nmitigate the possibility of separating a statement from important\\ncontext related to it. We use the\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nWe set add_start_index=True so that the character index where each\\nsplit Document starts within the initial Document is preserved as\\nmetadata attribute ‚Äústart_index‚Äù.\\nSee this guide for more detail about working with PDFs, including how to extract text from specific sections and images.\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000, chunk_overlap=200, add_start_index=True)all_splits = text_splitter.split_documents(docs)len(all_splits)\\n514\\nEmbeddings\\u200b\\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\\nLangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let\\'s select a model:\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nvector_1 = embeddings.embed_query(all_splits[0].page_content)vector_2 = embeddings.embed_query(all_splits[1].page_content)assert len(vector_1) == len(vector_2)print(f\"Generated vectors of length {len(vector_1)}\\\\n\")print(vector_1[:10])\\nGenerated vectors of length 1536[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\\nVector stores\\u200b\\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\\nLangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\\'s select a vector store:\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nHaving instantiated our vector store, we can now index the documents.\\nids = vector_store.add_documents(documents=all_splits)\\nNote that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.\\nOnce we\\'ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\\n\\nSynchronously and asynchronously;\\nBy string query and by vector;\\nWith and without returning similarity scores;\\nBy similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).\\n\\nThe methods will generally include a list of Document objects in their outputs.\\nUsage\\u200b\\nEmbeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\\nReturn documents based on similarity to a string query:\\nresults = vector_store.similarity_search(    \"How many distribution centers does Nike have in the US?\")print(results[0])\\npage_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:U.S. RETAIL STORES NUMBERNIKE Brand factory stores 213 NIKE Brand in-line stores (including employee-only stores) 74 Converse stores (including factory stores) 82 TOTAL 369 In the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.2023 FORM 10-K 2\\' metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}\\nAsync query:\\nresults = await vector_store.asimilarity_search(\"When was Nike incorporated?\")print(results[0])\\npage_content=\\'Table of ContentsPART IITEM 1. BUSINESSGENERALNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE isthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail storesand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\' metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\nReturn scores:\\n# Note that providers implement different scores; the score here# is a distance metric that varies inversely with similarity.results = vector_store.similarity_search_with_score(\"What was Nike\\'s revenue in 2023?\")doc, score = results[0]print(f\"Score: {score}\\\\n\")print(doc)\\nScore: 0.23699893057346344page_content=\\'Table of ContentsFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTSThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:FISCAL 2023 COMPARED TO FISCAL 2022‚Ä¢NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.The increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,2 and 1 percentage points to NIKE, Inc. Revenues, respectively.‚Ä¢NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. Thisincrease was primarily due to higher revenues in Men\\'s, the Jordan Brand, Women\\'s and Kids\\' which grew 17%, 35%,11% and 10%, respectively, on a wholesaleequivalent basis.\\' metadata={\\'page\\': 35, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\nReturn documents based on similarity to an embedded query:\\nembedding = embeddings.embed_query(\"How were Nike\\'s margins impacted in 2023?\")results = vector_store.similarity_search_by_vector(embedding)print(results[0])\\npage_content=\\'Table of ContentsGROSS MARGINFISCAL 2023 COMPARED TO FISCAL 2022For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:*Wholesale equivalentThe decrease in gross margin for fiscal 2023 was primarily due to:‚Ä¢Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well asproduct mix;‚Ä¢Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity inthe prior period resulting from lower available inventory supply;‚Ä¢Unfavorable changes in net foreign currency exchange rates, including hedges; and‚Ä¢Lower off-price margin, on a wholesale equivalent basis.This was partially offset by:\\' metadata={\\'page\\': 36, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\nLearn more:\\n\\nAPI reference\\nHow-to guide\\nIntegration-specific docs\\n\\nRetrievers\\u200b\\nLangChain VectorStore objects do not subclass Runnable. LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\\nWe can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method:\\nfrom typing import Listfrom langchain_core.documents import Documentfrom langchain_core.runnables import chain@chaindef retriever(query: str) -> List[Document]:    return vector_store.similarity_search(query, k=1)retriever.batch(    [        \"How many distribution centers does Nike have in the US?\",        \"When was Nike incorporated?\",    ],)API Reference:Document | chain\\n[[Document(metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}, page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\\\nU.S. RETAIL STORES NUMBER\\\\nNIKE Brand factory stores 213 \\\\nNIKE Brand in-line stores (including employee-only stores) 74 \\\\nConverse stores (including factory stores) 82 \\\\nTOTAL 369 \\\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\\\n2023 FORM 10-K 2\\')], [Document(metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}, page_content=\\'Table of Contents\\\\nPART I\\\\nITEM 1. BUSINESS\\\\nGENERAL\\\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\')]]\\nVectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\\nretriever = vector_store.as_retriever(    search_type=\"similarity\",    search_kwargs={\"k\": 1},)retriever.batch(    [        \"How many distribution centers does Nike have in the US?\",        \"When was Nike incorporated?\",    ],)\\n[[Document(metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}, page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\\\nU.S. RETAIL STORES NUMBER\\\\nNIKE Brand factory stores 213 \\\\nNIKE Brand in-line stores (including employee-only stores) 74 \\\\nConverse stores (including factory stores) 82 \\\\nTOTAL 369 \\\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\\\n2023 FORM 10-K 2\\')], [Document(metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}, page_content=\\'Table of Contents\\\\nPART I\\\\nITEM 1. BUSINESS\\\\nGENERAL\\\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\')]]\\nVectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\". We can use the latter to threshold documents output by the retriever by similarity score.\\nRetrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial.\\nLearn more:\\u200b\\nRetrieval strategies can be rich and complex. For example:\\n\\nWe can infer hard rules and filters from a query (e.g., \"using documents published after 2020\");\\nWe can return documents that are linked to the retrieved context in some way (e.g., via some document taxonomy);\\nWe can generate multiple embeddings for each unit of context;\\nWe can ensemble results from multiple retrievers;\\nWe can assign weights to documents, e.g., to weigh recent documents higher.\\n\\nThe retrievers section of the how-to guides covers these and other built-in retrieval strategies.\\nIt is also straightforward to extend the BaseRetriever class in order to implement custom retrievers. See our how-to guide here.\\nNext steps\\u200b\\nYou\\'ve now seen how to build a semantic search engine over a PDF document.\\nFor more on document loaders:\\n\\nConceptual guide\\nHow-to guides\\nAvailable integrations\\n\\nFor more on embeddings:\\n\\nConceptual guide\\nHow-to guides\\nAvailable integrations\\n\\nFor more on vector stores:\\n\\nConceptual guide\\nHow-to guides\\nAvailable integrations\\n\\nFor more on RAG, see:\\n\\nBuild a Retrieval Augmented Generation (RAG) App\\nRelated how-to guides\\nEdit this pagePreviousBuild a Retrieval Augmented Generation (RAG) App: Part 1NextBuild a Question/Answering system over SQL dataConceptsSetupJupyter NotebookInstallationLangSmithDocuments and Document LoadersLoading documentsSplittingEmbeddingsVector storesUsageRetrieversLearn more:Next stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/classification/', 'title': 'Tagging | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTagging | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsTaggingOn this page\\nClassify Text into Labels\\nTagging means labeling a document with classes such as:\\n\\nSentiment\\nLanguage\\nStyle (formal, informal etc.)\\nCovered topics\\nPolitical tendency\\n\\n\\nOverview\\u200b\\nTagging has a few components:\\n\\nfunction: Like extraction, tagging uses functions to specify how the model should tag a document\\nschema: defines how we want to tag the document\\n\\nQuickstart\\u200b\\nLet\\'s see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We\\'ll use the with_structured_output method supported by OpenAI models.\\npip install -U langchain-core\\nWe\\'ll need to load a chat model:\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s specify a Pydantic model with a few properties and their expected type in our schema.\\nfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldtagging_prompt = ChatPromptTemplate.from_template(    \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the \\'Classification\\' function.Passage:{input}\"\"\")class Classification(BaseModel):    sentiment: str = Field(description=\"The sentiment of the text\")    aggressiveness: int = Field(        description=\"How aggressive the text is on a scale from 1 to 10\"    )    language: str = Field(description=\"The language the text is written in\")# Structured LLMstructured_llm = llm.with_structured_output(Classification)API Reference:ChatPromptTemplate\\ninp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response\\nClassification(sentiment=\\'positive\\', aggressiveness=1, language=\\'Spanish\\')\\nIf we want dictionary output, we can just call .model_dump()\\ninp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})response = structured_llm.invoke(prompt)response.model_dump()\\n{\\'sentiment\\': \\'angry\\', \\'aggressiveness\\': 8, \\'language\\': \\'Spanish\\'}\\nAs we can see in the examples, it correctly interprets what we want.\\nThe results vary so that we may get, for example, sentiments in different languages (\\'positive\\', \\'enojado\\' etc.).\\nWe will see how to control these results in the next section.\\nFiner control\\u200b\\nCareful schema definition gives us more control over the model\\'s output.\\nSpecifically, we can define:\\n\\nPossible values for each property\\nDescription to make sure that the model understands the property\\nRequired properties to be returned\\n\\nLet\\'s redeclare our Pydantic model to control for each of the previously mentioned aspects using enums:\\nclass Classification(BaseModel):    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])    aggressiveness: int = Field(        ...,        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",        enum=[1, 2, 3, 4, 5],    )    language: str = Field(        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]    )\\ntagging_prompt = ChatPromptTemplate.from_template(    \"\"\"Extract the desired information from the following passage.Only extract the properties mentioned in the \\'Classification\\' function.Passage:{input}\"\"\")structured_llm = llm.with_structured_output(Classification)\\nNow the answers will be restricted in a way we expect!\\ninp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\\nClassification(sentiment=\\'happy\\', aggressiveness=1, language=\\'spanish\\')\\ninp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\\nClassification(sentiment=\\'sad\\', aggressiveness=4, language=\\'spanish\\')\\ninp = \"Weather is ok here, I can go outside without much more than a coat\"prompt = tagging_prompt.invoke({\"input\": inp})structured_llm.invoke(prompt)\\nClassification(sentiment=\\'happy\\', aggressiveness=1, language=\\'english\\')\\nThe LangSmith trace lets us peek under the hood:\\n\\nGoing deeper\\u200b\\n\\nYou can use the metadata tagger document transformer to extract metadata from a LangChain Document.\\nThis covers the same basic functionality as the tagging chain, only applied to a LangChain Document.\\nEdit this pagePreviousBuild an AgentNextBuild a Retrieval Augmented Generation (RAG) App: Part 1OverviewQuickstartFiner controlGoing deeperCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/extraction/', 'title': 'Build an Extraction Chain | ü¶úÔ∏èüîó LangChain', 'description': 'In this tutorial, we will use tool-calling features of chat models to extract structured information from unstructured text. We will also demonstrate how to use few-shot prompting in this context to improve performance.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild an Extraction Chain | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild an Extraction ChainOn this pageBuild an Extraction Chain\\nIn this tutorial, we will use tool-calling features of chat models to extract structured information from unstructured text. We will also demonstrate how to use few-shot prompting in this context to improve performance.\\nimportantThis tutorial requires langchain-core>=0.3.20 and will only work with models that support tool calling.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install --upgrade langchain-coreconda install langchain-core -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nThe Schema\\u200b\\nFirst, we need to describe what information we want to extract from the text.\\nWe\\'ll use Pydantic to define an example schema  to extract personal information.\\nfrom typing import Optionalfrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    # ^ Doc-string for the entity Person.    # This doc-string is sent to the LLM as the description of the schema Person,    # and it can help to improve extraction results.    # Note that:    # 1. Each field is an `optional` -- this allows the model to decline to extract it!    # 2. Each field has a `description` -- this description is used by the LLM.    # Having a good description can help improve extraction results.    name: Optional[str] = Field(default=None, description=\"The name of the person\")    hair_color: Optional[str] = Field(        default=None, description=\"The color of the person\\'s hair if known\"    )    height_in_meters: Optional[str] = Field(        default=None, description=\"Height measured in meters\"    )\\nThere are two best practices when defining schema:\\n\\nDocument the attributes and the schema itself: This information is sent to the LLM and is used to improve the quality of information extraction.\\nDo not force the LLM to make up information! Above we used Optional for the attributes allowing the LLM to output None if it doesn\\'t know the answer.\\n\\nimportantFor best performance, document the schema well and make sure the model isn\\'t forced to return results if there\\'s no information to be extracted in the text.\\nThe Extractor\\u200b\\nLet\\'s create an information extractor using the schema we defined above.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder# Define a custom prompt to provide instructions and any additional context.# 1) You can add examples into the prompt template to improve extraction quality# 2) Introduce additional parameters to take context into account (e.g., include metadata#    about the document from which the text was extracted.)prompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are an expert extraction algorithm. \"            \"Only extract relevant information from the text. \"            \"If you do not know the value of an attribute asked to extract, \"            \"return null for the attribute\\'s value.\",        ),        # Please see the how-to about improving performance with        # reference examples.        # MessagesPlaceholder(\\'examples\\'),        (\"human\", \"{text}\"),    ])API Reference:ChatPromptTemplate | MessagesPlaceholder\\nWe need to use a model that supports function/tool calling.\\nPlease review the documentation for all models that can be used with this API.\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nstructured_llm = llm.with_structured_output(schema=Person)\\nLet\\'s test it out:\\ntext = \"Alan Smith is 6 feet tall and has blond hair.\"prompt = prompt_template.invoke({\"text\": text})structured_llm.invoke(prompt)\\nPerson(name=\\'Alan Smith\\', hair_color=\\'blond\\', height_in_meters=\\'1.83\\')\\nimportantExtraction is Generative ü§ØLLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters\\neven though it was provided in feet!\\nWe can see the LangSmith trace here. Note that the chat model portion of the trace reveals the exact sequence of messages sent to the model, tools invoked, and other metadata.\\nMultiple Entities\\u200b\\nIn most cases, you should be extracting a list of entities rather than a single entity.\\nThis can be easily achieved using pydantic by nesting models inside one another.\\nfrom typing import List, Optionalfrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    # ^ Doc-string for the entity Person.    # This doc-string is sent to the LLM as the description of the schema Person,    # and it can help to improve extraction results.    # Note that:    # 1. Each field is an `optional` -- this allows the model to decline to extract it!    # 2. Each field has a `description` -- this description is used by the LLM.    # Having a good description can help improve extraction results.    name: Optional[str] = Field(default=None, description=\"The name of the person\")    hair_color: Optional[str] = Field(        default=None, description=\"The color of the person\\'s hair if known\"    )    height_in_meters: Optional[str] = Field(        default=None, description=\"Height measured in meters\"    )class Data(BaseModel):    \"\"\"Extracted data about people.\"\"\"    # Creates a model so that we can extract multiple entities.    people: List[Person]\\nimportantExtraction results might not be perfect here. Read on to see how to use Reference Examples to improve the quality of extraction, and check out our extraction how-to guides for more detail.\\nstructured_llm = llm.with_structured_output(schema=Data)text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"prompt = prompt_template.invoke({\"text\": text})structured_llm.invoke(prompt)\\nData(people=[Person(name=\\'Jeff\\', hair_color=\\'black\\', height_in_meters=\\'1.83\\'), Person(name=\\'Anna\\', hair_color=\\'black\\', height_in_meters=None)])\\ntipWhen the schema accommodates the extraction of multiple entities, it also allows the model to extract no entities if no relevant information\\nis in the text by providing an empty list.This is usually a good thing! It allows specifying required attributes on an entity without necessarily forcing the model to detect this entity.\\nWe can see the LangSmith trace here.\\nReference examples\\u200b\\nThe behavior of LLM applications can be steered using few-shot prompting. For chat models, this can take the form of a sequence of pairs of input and response messages demonstrating desired behaviors.\\nFor example, we can convey the meaning of a symbol with alternating user and assistant messages:\\nmessages = [    {\"role\": \"user\", \"content\": \"2 ü¶ú 2\"},    {\"role\": \"assistant\", \"content\": \"4\"},    {\"role\": \"user\", \"content\": \"2 ü¶ú 3\"},    {\"role\": \"assistant\", \"content\": \"5\"},    {\"role\": \"user\", \"content\": \"3 ü¶ú 4\"},]response = llm.invoke(messages)print(response.content)\\n7\\nStructured output often uses tool calling under-the-hood. This typically involves the generation of AI messages containing tool calls, as well as tool messages containing the results of tool calls. What should a sequence of messages look like in this case?\\nDifferent chat model providers impose different requirements for valid message sequences. Some will accept a (repeating) message sequence of the form:\\n\\nUser message\\nAI message with tool call\\nTool message with result\\n\\nOthers require a final AI message containing some sort of response.\\nLangChain includes a utility function tool_example_to_messages that will generate a valid sequence for most model providers. It simplifies the generation of structured few-shot examples by just requiring Pydantic representations of the corresponding tool calls.\\nLet\\'s try this out. We can convert pairs of input strings and desired Pydantic objects to a sequence of messages that can be provided to a chat model. Under the hood, LangChain will format the tool calls to each provider\\'s required format.\\nNote: this version of tool_example_to_messages requires langchain-core>=0.3.20.\\nfrom langchain_core.utils.function_calling import tool_example_to_messagesexamples = [    (        \"The ocean is vast and blue. It\\'s more than 20,000 feet deep.\",        Data(people=[]),    ),    (        \"Fiona traveled far from France to Spain.\",        Data(people=[Person(name=\"Fiona\", height_in_meters=None, hair_color=None)]),    ),]messages = []for txt, tool_call in examples:    if tool_call.people:        # This final message is optional for some providers        ai_response = \"Detected people.\"    else:        ai_response = \"Detected no people.\"    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))API Reference:tool_example_to_messages\\nInspecting the result, we see these two example pairs generated eight messages:\\nfor message in messages:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================The ocean is vast and blue. It\\'s more than 20,000 feet deep.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  Data (d8f2e054-7fb9-417f-b28f-0447a775b2c3) Call ID: d8f2e054-7fb9-417f-b28f-0447a775b2c3  Args:    people: []=================================\\x1b[1m Tool Message \\x1b[0m=================================You have correctly called this tool.==================================\\x1b[1m Ai Message \\x1b[0m==================================Detected no people.================================\\x1b[1m Human Message \\x1b[0m=================================Fiona traveled far from France to Spain.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  Data (0178939e-a4b1-4d2a-a93e-b87f665cdfd6) Call ID: 0178939e-a4b1-4d2a-a93e-b87f665cdfd6  Args:    people: [{\\'name\\': \\'Fiona\\', \\'hair_color\\': None, \\'height_in_meters\\': None}]=================================\\x1b[1m Tool Message \\x1b[0m=================================You have correctly called this tool.==================================\\x1b[1m Ai Message \\x1b[0m==================================Detected people.\\nLet\\'s compare performance with and without these messages. For example, let\\'s pass a message for which we intend no people to be extracted:\\nmessage_no_extraction = {    \"role\": \"user\",    \"content\": \"The solar system is large, but earth has only 1 moon.\",}structured_llm = llm.with_structured_output(schema=Data)structured_llm.invoke([message_no_extraction])\\nData(people=[Person(name=\\'Earth\\', hair_color=\\'None\\', height_in_meters=\\'0.00\\')])\\nIn this example, the model is liable to erroneously generate records of people.\\nBecause our few-shot examples contain examples of \"negatives\", we encourage the model to behave correctly in this case:\\nstructured_llm.invoke(messages + [message_no_extraction])\\nData(people=[])\\ntipThe LangSmith trace for the run reveals the exact sequence of messages sent to the chat model, tool calls generated, latency, token counts, and other metadata.\\nSee this guide for more detail on extraction workflows with reference examples, including how to incorporate prompt templates and customize the generation of example messages.\\nNext steps\\u200b\\nNow that you understand the basics of extraction with LangChain, you\\'re ready to proceed to the rest of the how-to guides:\\n\\nAdd Examples: More detail on using reference examples to improve performance.\\nHandle Long Text: What should you do if the text does not fit into the context window of the LLM?\\nUse a Parsing Approach: Use a prompt based approach to extract with models that do not support tool/function calling.\\nEdit this pagePreviousBuild a Retrieval Augmented Generation (RAG) App: Part 2NextBuild an AgentSetupJupyter NotebookInstallationLangSmithThe SchemaThe ExtractorMultiple EntitiesReference examplesNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | ü¶úÔ∏èüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Chatbot | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a ChatbotOn this pageBuild a Chatbot\\nnoteThis tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nOverview\\u200b\\nWe\\'ll go over an example of how to design and implement an LLM-powered chatbot.\\nThis chatbot will be able to have a conversation and remember previous interactions with a chat model.\\nNote that this chatbot that we build will only use the language model to have a conversation.\\nThere are several other related concepts that you may be looking for:\\n\\nConversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nThis tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nFor this tutorial we will need langchain-core and langgraph. This guide requires langgraph >= 0.2.28.\\n\\nPipCondapip install langchain-core langgraph>0.2.27conda install langchain-core langgraph>0.2.27 -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, (you\\'ll need to create an API key from the Settings -> API Keys page on the LangSmith website), make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nQuickstart\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hi! I\\'m Bob\")])API Reference:HumanMessage\\nAIMessage(content=\\'Hi Bob! How can I assist you today?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 10, \\'prompt_tokens\\': 11, \\'total_tokens\\': 21, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 10, \\'total_tokens\\': 21, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nThe model on its own does not have any concept of state. For example, if you ask a followup question:\\nmodel.invoke([HumanMessage(content=\"What\\'s my name?\")])\\nAIMessage(content=\"I\\'m sorry, but I don\\'t have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 34, \\'prompt_tokens\\': 11, \\'total_tokens\\': 45, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-a2d13a18-7022-4784-b54f-f85c097d1075-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 34, \\'total_tokens\\': 45, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nLet\\'s take a look at the example LangSmith trace\\nWe can see that it doesn\\'t take the previous conversation turn into context, and cannot answer the question.\\nThis makes for a terrible chatbot experience!\\nTo get around this, we need to pass the entire conversation history into the model. Let\\'s see what happens when we do that:\\nfrom langchain_core.messages import AIMessagemodel.invoke(    [        HumanMessage(content=\"Hi! I\\'m Bob\"),        AIMessage(content=\"Hello Bob! How can I assist you today?\"),        HumanMessage(content=\"What\\'s my name?\"),    ])API Reference:AIMessage\\nAIMessage(content=\\'Your name is Bob! How can I help you today, Bob?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 14, \\'prompt_tokens\\': 33, \\'total_tokens\\': 47, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-34bcccb3-446e-42f2-b1de-52c09936c02c-0\\', usage_metadata={\\'input_tokens\\': 33, \\'output_tokens\\': 14, \\'total_tokens\\': 47, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nAnd now we can see that we get a good response!\\nThis is the basic idea underpinning a chatbot\\'s ability to interact conversationally.\\nSo how do we best implement this?\\nMessage persistence\\u200b\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\nfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define a new graphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    response = model.invoke(state[\"messages\"])    return {\"messages\": response}# Define the (single) node in the graphworkflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)# Add memorymemory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:MemorySaver | StateGraph\\nWe now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like:\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\\nThis enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\\nWe can then invoke the application:\\nquery = \"Hi! I\\'m Bob.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()  # output contains all messages in state\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Hi Bob! How can I assist you today?\\nquery = \"What\\'s my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob! How can I help you today, Bob?\\nGreat! Our chatbot now remembers things about us. If we change the config to reference a different thread_id, we can see that it starts the conversation fresh.\\nconfig = {\"configurable\": {\"thread_id\": \"abc234\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================I\\'m sorry, but I don\\'t have access to personal information about you unless you\\'ve shared it in this conversation. How can I assist you today?\\nHowever, we can always go back to the original conversation (since we are persisting it in a database)\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob. What would you like to discuss today?\\nThis is how we can support a chatbot having conversations with many users!\\ntipFor async support, update the call_model node to be an async function and use .ainvoke when invoking the application:# Async function for node:async def call_model(state: MessagesState):    response = await model.ainvoke(state[\"messages\"])    return {\"messages\": response}# Define graph as before:workflow = StateGraph(state_schema=MessagesState)workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)app = workflow.compile(checkpointer=MemorySaver())# Async invocation:output = await app.ainvoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\nRight now, all we\\'ve done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.\\nPrompt templates\\u200b\\nPrompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let\\'s now make that a bit more complicated. First, let\\'s add in a system message with some custom instructions (but still taking messages as input). Next, we\\'ll add in more input besides just the messages.\\nTo add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You talk like a pirate. Answer all questions to the best of your ability.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])API Reference:ChatPromptTemplate | MessagesPlaceholder\\nWe can now update our application to incorporate this template:\\nworkflow = StateGraph(state_schema=MessagesState)def call_model(state: MessagesState):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": response}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nWe invoke the application in the same way:\\nconfig = {\"configurable\": {\"thread_id\": \"abc345\"}}query = \"Hi! I\\'m Jim.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ahoy there, Jim! What brings ye to these waters today? Be ye seekin\\' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!\\nAwesome! Let\\'s now make our prompt a little bit more complicated. Let\\'s assume that the prompt template now looks something like this:\\nprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])\\nNote that we have added a new language input to the prompt. Our application now has two parameters-- the input messages and language. We should update our application\\'s state to reflect this:\\nfrom typing import Sequencefrom langchain_core.messages import BaseMessagefrom langgraph.graph.message import add_messagesfrom typing_extensions import Annotated, TypedDictclass State(TypedDict):    messages: Annotated[Sequence[BaseMessage], add_messages]    language: strworkflow = StateGraph(state_schema=State)def call_model(state: State):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:BaseMessage | add_messages\\nconfig = {\"configurable\": {\"thread_id\": \"abc456\"}}query = \"Hi! I\\'m Bob.\"language = \"Spanish\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================¬°Hola, Bob! ¬øC√≥mo puedo ayudarte hoy?\\nNote that the entire state is persisted, so we can omit parameters like language if no changes are desired:\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Tu nombre es Bob. ¬øHay algo m√°s en lo que pueda ayudarte?\\nTo help you understand what\\'s happening internally, check out this LangSmith trace.\\nManaging Conversation History\\u200b\\nOne important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\\nImportantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\\nWe can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the Message History class.\\nLangChain comes with a few built-in helpers for managing a list of messages. In this case we\\'ll use the trim_messages helper to reduce how many messages we\\'re sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:\\nfrom langchain_core.messages import SystemMessage, trim_messagestrimmer = trim_messages(    max_tokens=65,    strategy=\"last\",    token_counter=model,    include_system=True,    allow_partial=False,    start_on=\"human\",)messages = [    SystemMessage(content=\"you\\'re a good assistant\"),    HumanMessage(content=\"hi! I\\'m bob\"),    AIMessage(content=\"hi!\"),    HumanMessage(content=\"I like vanilla ice cream\"),    AIMessage(content=\"nice\"),    HumanMessage(content=\"whats 2 + 2\"),    AIMessage(content=\"4\"),    HumanMessage(content=\"thanks\"),    AIMessage(content=\"no problem!\"),    HumanMessage(content=\"having fun?\"),    AIMessage(content=\"yes!\"),]trimmer.invoke(messages)API Reference:SystemMessage | trim_messages\\n[SystemMessage(content=\"you\\'re a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'whats 2 + 2\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'4\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'thanks\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'no problem!\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'having fun?\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'yes!\\', additional_kwargs={}, response_metadata={})]\\nTo  use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\\nworkflow = StateGraph(state_schema=State)def call_model(state: State):    print(f\"Messages before trimming: {len(state[\\'messages\\'])}\")    trimmed_messages = trimmer.invoke(state[\"messages\"])    print(f\"Messages after trimming: {len(trimmed_messages)}\")    print(\"Remaining messages:\")    for msg in trimmed_messages:        print(f\"  {type(msg).__name__}: {msg.content}\")    prompt = prompt_template.invoke(        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}    )    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nNow if we try asking the model our name, it won\\'t know it since we trimmed that part of the chat history. (By defining our trim stragegy as \\'last\\', we are only keeping the most recent messages that fit within the max_tokens.)\\nconfig = {\"configurable\": {\"thread_id\": \"abc567\"}}query = \"What is my name?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\nMessages before trimming: 12Messages after trimming: 8Remaining messages:  SystemMessage: you\\'re a good assistant  HumanMessage: whats 2 + 2  AIMessage: 4  HumanMessage: thanks  AIMessage: no problem!  HumanMessage: having fun?  AIMessage: yes!  HumanMessage: What is my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================I don\\'t know your name. If you\\'d like to share it, feel free!\\nBut if we ask about information that is within the last few messages, it remembers:\\nconfig = {\"configurable\": {\"thread_id\": \"abc678\"}}query = \"What math problem was asked?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\nMessages before trimming: 12Messages after trimming: 8Remaining messages:  SystemMessage: you\\'re a good assistant  HumanMessage: whats 2 + 2  AIMessage: 4  HumanMessage: thanks  AIMessage: no problem!  HumanMessage: having fun?  AIMessage: yes!  HumanMessage: What math problem was asked?==================================\\x1b[1m Ai Message \\x1b[0m==================================The math problem that was asked was \"what\\'s 2 + 2.\"\\nIf you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace.\\nStreaming\\u200b\\nNow we\\'ve got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\\nIt\\'s actually super easy to do this!\\nBy default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead:\\nconfig = {\"configurable\": {\"thread_id\": \"abc789\"}}query = \"Hi I\\'m Todd, please tell me a joke.\"language = \"English\"input_messages = [HumanMessage(query)]for chunk, metadata in app.stream(    {\"messages\": input_messages, \"language\": language},    config,    stream_mode=\"messages\",):    if isinstance(chunk, AIMessage):  # Filter to just model responses        print(chunk.content, end=\"|\")\\n|Hi| Todd|!| Here|‚Äôs| a| joke| for| you|:|Why| don\\'t| scientists| trust| atoms|?|Because| they| make| up| everything|!||\\nNext Steps\\u200b\\nNow that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:\\n\\nConversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nIf you want to dive deeper on specifics, some things worth checking out are:\\n\\nStreaming: streaming is crucial for chat applications\\nHow to add message history: for a deeper dive into all things related to message history\\nHow to manage large message history: more techniques for managing a large chat history\\nLangGraph main docs: for more detail on building with LangGraph\\nEdit this pagePreviousBuild a simple LLM application with chat models and prompt templatesNextBuild a Retrieval Augmented Generation (RAG) App: Part 2OverviewSetupJupyter NotebookInstallationLangSmithQuickstartMessage persistencePrompt templatesManaging Conversation HistoryStreamingNext StepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/agents/', 'title': 'Build an Agent | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain supports the creation of agents, or systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild an Agent | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild an AgentOn this pageBuild an Agent\\nLangChain supports the creation of agents, or systems that use LLMs as reasoning engines to determine which actions to take and the inputs necessary to perform the action.\\nAfter executing actions, the results can be fed back into the LLM to determine whether more actions are needed, or whether it is okay to finish. This is often achieved via tool-calling.\\nIn this tutorial we will build an agent that can interact with a search engine. You will be able to ask this agent questions, watch it call the search tool, and have conversations with it.\\nEnd-to-end agent\\u200b\\nThe code snippet below represents a fully functional agent that uses an LLM to decide which tools to use. It is equipped with a generic search tool. It has conversational memory - meaning that it can be used as a multi-turn chatbot.\\nIn the rest of the guide, we will walk through the individual components and what each part does - but if you want to just grab some code and get started, feel free to use this!\\n# Import relevant functionalityfrom langchain.chat_models import init_chat_modelfrom langchain_tavily import TavilySearchfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.prebuilt import create_react_agent# Create the agentmemory = MemorySaver()model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")search = TavilySearch(max_results=2)tools = [search]agent_executor = create_react_agent(model, tools, checkpointer=memory)API Reference:MemorySaver | create_react_agent\\n# Use the agentconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_message = {    \"role\": \"user\",    \"content\": \"Hi, I\\'m Bob and I live in SF.\",}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi, I\\'m Bob and I live in SF.==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello Bob! I notice you\\'ve introduced yourself and mentioned you live in SF (San Francisco), but you haven\\'t asked a specific question or made a request that requires the use of any tools. Is there something specific you\\'d like to know about San Francisco or any other topic? I\\'d be happy to help you find information using the available search tools.\\ninput_message = {    \"role\": \"user\",    \"content\": \"What\\'s the weather where I live?\",}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s the weather where I live?==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \\'Let me search for current weather information in San Francisco.\\', \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_011kSdheoJp8THURoLmeLtZo\\', \\'input\\': {\\'query\\': \\'current weather San Francisco CA\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_011kSdheoJp8THURoLmeLtZo) Call ID: toolu_011kSdheoJp8THURoLmeLtZo  Args:    query: current weather San Francisco CA=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco CA\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.944705, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.86441374, \"raw_content\": null}], \"response_time\": 2.34}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9 milesThis is quite typical weather for San Francisco, with the characteristic fog that the city is known for. Would you like to know anything else about the weather or San Francisco in general?\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect interactive environments for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc), and observing these cases is a great way to better understand building with LLMs.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n%pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nTavily\\u200b\\nWe will be using Tavily (a search engine) as a tool.\\nIn order to use it, you will need to get and set an API key:\\nexport TAVILY_API_KEY=\"...\"\\nOr, if in a notebook, you can set it with:\\nimport getpassimport osos.environ[\"TAVILY_API_KEY\"] = getpass.getpass()\\nDefine tools\\u200b\\nWe first need to create the tools we want to use. Our main tool of choice will be Tavily - a search engine. We can use the dedicated langchain-tavily integration package to easily use Tavily search engine as tool with LangChain.\\nfrom langchain_tavily import TavilySearchsearch = TavilySearch(max_results=2)search_results = search.invoke(\"What is the weather in SF\")print(search_results)# If we want, we can create other tools.# Once we have all the tools we want, we can put them in a list that we will reference later.tools = [search]\\n{\\'query\\': \\'What is the weather in SF\\', \\'follow_up_questions\\': None, \\'answer\\': None, \\'images\\': [], \\'results\\': [{\\'title\\': \\'Weather in San Francisco, CA\\', \\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \\'score\\': 0.9185379, \\'raw_content\\': None}, {\\'title\\': \\'Weather in San Francisco in June 2025\\', \\'url\\': \\'https://world-weather.info/forecast/usa/san_francisco/june-2025/\\', \\'content\\': \"Weather in San Francisco in June 2025 (California) - Detailed Weather Forecast for a Month *   Weather in San Francisco Weather in San Francisco in June 2025 *   1 +63¬∞ +55¬∞ *   2 +66¬∞ +54¬∞ *   3 +66¬∞ +55¬∞ *   4 +66¬∞ +54¬∞ *   5 +66¬∞ +55¬∞ *   6 +66¬∞ +57¬∞ *   7 +64¬∞ +55¬∞ *   8 +63¬∞ +55¬∞ *   9 +63¬∞ +54¬∞ *   10 +59¬∞ +54¬∞ *   11 +59¬∞ +54¬∞ *   12 +61¬∞ +54¬∞ Weather in Washington, D.C.**+68¬∞** Sacramento**+81¬∞** Pleasanton**+72¬∞** Redwood City**+68¬∞** San Leandro**+61¬∞** San Mateo**+64¬∞** San Rafael**+70¬∞** San Ramon**+64¬∞** South San Francisco**+61¬∞** Daly City**+59¬∞** Wilder**+66¬∞** Woodacre**+70¬∞** world\\'s temperature today Colchani day+50¬∞F night+16¬∞F Az Zubayr day+124¬∞F night+93¬∞F Weather forecast on your site Install _San Francisco_ +61¬∞ Temperature units\", \\'score\\': 0.7978881, \\'raw_content\\': None}], \\'response_time\\': 2.62}\\ntipIn many applications, you may want to define custom tools. LangChain supports custom\\ntool creation via Python functions and other means. Refer to the\\nHow to create tools guide for details.\\nUsing Language Models\\u200b\\nNext, let\\'s learn how to use a language model to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nYou can call the language model by passing in a list of messages. By default, the response is a content string.\\nquery = \"Hi!\"response = model.invoke([{\"role\": \"user\", \"content\": query}])response.text()\\n\\'Hello! How can I help you today?\\'\\nWe can now see what it is like to enable this model to do tool calling. In order to enable that we use .bind_tools to give the language model knowledge of these tools\\nmodel_with_tools = model.bind_tools(tools)\\nWe can now call the model. Let\\'s first call it with a normal message, and see how it responds. We can look at both the content field as well as the tool_calls field.\\nquery = \"Hi!\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\\\n\")print(f\"Tool calls: {response.tool_calls}\")\\nMessage content: Hello! I\\'m here to help you. I have access to a powerful search tool that can help answer questions and find information about various topics. What would you like to know about?Feel free to ask any question or request information, and I\\'ll do my best to assist you using the available tools.Tool calls: []\\nNow, let\\'s try calling it with some input that would expect a tool to be called.\\nquery = \"Search for the weather in SF\"response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])print(f\"Message content: {response.text()}\\\\n\")print(f\"Tool calls: {response.tool_calls}\")\\nMessage content: I\\'ll help you search for information about the weather in San Francisco.Tool calls: [{\\'name\\': \\'tavily_search\\', \\'args\\': {\\'query\\': \\'current weather San Francisco\\'}, \\'id\\': \\'toolu_015gdPn1jbB2Z21DmN2RAnti\\', \\'type\\': \\'tool_call\\'}]\\nWe can see that there\\'s now no text content, but there is a tool call! It wants us to call the Tavily Search tool.\\nThis isn\\'t calling that tool yet - it\\'s just telling us to. In order to actually call it, we\\'ll want to create our agent.\\nCreate the agent\\u200b\\nNow that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent.\\nCurrently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.\\nNow, we can initialize the agent with the LLM and the tools.\\nNote that we are passing in the model, not model_with_tools. That is because create_react_agent will call .bind_tools for us under the hood.\\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(model, tools)API Reference:create_react_agent\\nRun the agent\\u200b\\nWe can now run the agent with a few queries! Note that for now, these are all stateless queries (it won\\'t remember previous interactions). Note that the agent will return the final state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs).\\nFirst up, let\\'s see how it responds when there\\'s no need to call a tool:\\ninput_message = {\"role\": \"user\", \"content\": \"Hi!\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi!==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello! I\\'m here to help you with your questions using the available search tools. Please feel free to ask any question, and I\\'ll do my best to find relevant and accurate information for you.\\nIn order to see exactly what is happening under the hood (and to make sure it\\'s not calling a tool) we can take a look at the LangSmith trace\\nLet\\'s now try it out on an example where it should be invoking the tool\\ninput_message = {\"role\": \"user\", \"content\": \"Search for the weather in SF\"}response = agent_executor.invoke({\"messages\": [input_message]})for message in response[\"messages\"]:    message.pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Search for the weather in SF==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \"I\\'ll help you search for weather information in San Francisco. Let me use the search engine to find current weather conditions.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01WWcXGnArosybujpKzdmARZ\\', \\'input\\': {\\'query\\': \\'current weather San Francisco SF\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_01WWcXGnArosybujpKzdmARZ) Call ID: toolu_01WWcXGnArosybujpKzdmARZ  Args:    query: current weather San Francisco SF=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco SF\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, CA\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168606, \\'localtime\\': \\'2025-06-17 06:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.885373, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.8830044, \"raw_content\": null}], \"response_time\": 2.6}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Conditions: Foggy- Wind: 4.0 mph from the SW- Humidity: 86%- Visibility: 9.0 milesThe weather appears to be typical for San Francisco, with morning fog and mild temperatures. The \"feels like\" temperature is 52.4¬∞F (11.3¬∞C).\\nWe can check out the LangSmith trace to make sure it\\'s calling the search tool effectively.\\nStreaming Messages\\u200b\\nWe\\'ve seen how the agent can be called with .invoke to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\\nfor step in agent_executor.stream({\"messages\": [input_message]}, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Search for the weather in SF==================================\\x1b[1m Ai Message \\x1b[0m==================================[{\\'text\\': \"I\\'ll help you search for information about the weather in San Francisco.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01DCPnJES53Fcr7YWnZ47kDG\\', \\'input\\': {\\'query\\': \\'current weather San Francisco\\'}, \\'name\\': \\'tavily_search\\', \\'type\\': \\'tool_use\\'}]Tool Calls:  tavily_search (toolu_01DCPnJES53Fcr7YWnZ47kDG) Call ID: toolu_01DCPnJES53Fcr7YWnZ47kDG  Args:    query: current weather San Francisco=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: tavily_search{\"query\": \"current weather San Francisco\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1750168506, \\'localtime\\': \\'2025-06-17 06:55\\'}, \\'current\\': {\\'last_updated_epoch\\': 1750167900, \\'last_updated\\': \\'2025-06-17 06:45\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Fog\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/248.png\\', \\'code\\': 1135}, \\'wind_mph\\': 4.0, \\'wind_kph\\': 6.5, \\'wind_degree\\': 215, \\'wind_dir\\': \\'SW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 86, \\'cloud\\': 0, \\'feelslike_c\\': 11.3, \\'feelslike_f\\': 52.4, \\'windchill_c\\': 8.7, \\'windchill_f\\': 47.7, \\'heatindex_c\\': 9.8, \\'heatindex_f\\': 49.7, \\'dewpoint_c\\': 9.6, \\'dewpoint_f\\': 49.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 6.3, \\'gust_kph\\': 10.2}}\", \"score\": 0.9542825, \"raw_content\": null}, {\"title\": \"Weather in San Francisco in June 2025\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2025/\", \"content\": \"Detailed ‚ö° San Francisco Weather Forecast for June 2025 - day/night üå°Ô∏è temperatures, precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; Weather Widget ¬∞F. World; United States; California; Weather in San Francisco; ... 17 +64¬∞ +54¬∞ 18 +61¬∞ +54¬∞ 19\", \"score\": 0.8638634, \"raw_content\": null}], \"response_time\": 2.57}==================================\\x1b[1m Ai Message \\x1b[0m==================================Based on the search results, here\\'s the current weather in San Francisco:- Temperature: 53.1¬∞F (11.7¬∞C)- Condition: Foggy- Wind: 4.0 mph from the Southwest- Humidity: 86%- Visibility: 9.0 miles- Feels like: 52.4¬∞F (11.3¬∞C)This is quite typical weather for San Francisco, which is known for its fog, especially during the morning hours. The city\\'s proximity to the ocean and unique geographical features often result in mild temperatures and foggy conditions.\\nStreaming tokens\\u200b\\nIn addition to streaming back messages, it is also useful to stream back tokens.\\nWe can do this by specifying stream_mode=\"messages\".\\n::: note\\nBelow we use message.text(), which requires langchain-core>=0.3.37.\\n:::\\nfor step, metadata in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"messages\"):    if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()):        print(text, end=\"|\")\\nI|\\'ll help you search for information| about the weather in San Francisco.|Base|d on the search results, here|\\'s the current weather in| San Francisco:-| Temperature: 53.1¬∞F (|11.7¬∞C)-| Condition: Foggy- Wind:| 4.0 mph from| the Southwest- Humidity|: 86%|- Visibility: 9|.0 miles- Pressure: |30.02 in|HgThe weather| is characteristic of San Francisco, with| foggy conditions and mild temperatures|. The \"feels like\" temperature is slightly| lower at 52.4|¬∞F (11.|3¬∞C)| due to the wind chill effect|.|\\nAdding in memory\\u200b\\nAs mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a thread_id when invoking the agent (so it knows which thread/conversation to resume from).\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()API Reference:MemorySaver\\nagent_executor = create_react_agent(model, tools, checkpointer=memory)config = {\"configurable\": {\"thread_id\": \"abc123\"}}\\ninput_message = {\"role\": \"user\", \"content\": \"Hi, I\\'m Bob!\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hi, I\\'m Bob!==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello Bob! I\\'m an AI assistant who can help you search for information using specialized search tools. Is there anything specific you\\'d like to know about or search for? I\\'m happy to help you find accurate and up-to-date information on various topics.\\ninput_message = {\"role\": \"user\", \"content\": \"What\\'s my name?\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob, as you introduced yourself earlier. I can remember information shared within our conversation without needing to search for it.\\nExample LangSmith trace\\nIf you want to start a new conversation, all you have to do is change the thread_id used\\nconfig = {\"configurable\": {\"thread_id\": \"xyz123\"}}input_message = {\"role\": \"user\", \"content\": \"What\\'s my name?\"}for step in agent_executor.stream(    {\"messages\": [input_message]}, config, stream_mode=\"values\"):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What\\'s my name?==================================\\x1b[1m Ai Message \\x1b[0m==================================I apologize, but I don\\'t have access to any tools that would tell me your name. I can only assist you with searching for publicly available information using the tavily_search function. I don\\'t have access to personal information about users. If you\\'d like to tell me your name, I\\'ll be happy to address you by it.\\nConclusion\\u200b\\nThat\\'s a wrap! In this quick start we covered how to create a simple agent.\\nWe\\'ve then shown how to stream back a response - not only with the intermediate steps, but also tokens!\\nWe\\'ve also added in memory so you can have a conversation with them.\\nAgents are a complex topic with lots to learn!\\nFor more information on Agents, please check out the LangGraph documentation. This has it\\'s own set of concepts, tutorials, and how-to guides.Edit this pagePreviousBuild an Extraction ChainNextTaggingEnd-to-end agentSetupJupyter NotebookInstallationLangSmithTavilyDefine toolsUsing Language ModelsCreate the agentRun the agentStreaming MessagesStreaming tokensAdding in memoryConclusionCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 1On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 1\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis is a multi-part tutorial:\\n\\nPart 1 (this guide) introduces RAG and walks through a minimal implementation.\\nPart 2 extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\\n\\nThis tutorial will show how to build a simple Q&A application\\nover a text data source. Along the way we‚Äôll go over a typical Q&A\\narchitecture and highlight additional resources for more advanced Q&A techniques. We‚Äôll also see\\nhow LangSmith can help us trace and understand our application.\\nLangSmith will become increasingly helpful as our application grows in\\ncomplexity.\\nIf you\\'re already familiar with basic retrieval, you might also be interested in\\nthis high-level overview of different retrieval techniques.\\nNote: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.\\nOverview\\u200b\\nA typical RAG application has two main components:\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\\nRetrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\nNote: the indexing portion of this tutorial will largely follow the semantic search tutorial.\\nThe most common full sequence from raw data to answer looks like:\\nIndexing\\u200b\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\\'t fit in a model\\'s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\nRetrieval and generation\\u200b\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nOnce we\\'ve indexed our data, we will use LangGraph as our orchestration framework to implement the retrieval and generation steps.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nThis tutorial requires these langchain dependencies:\\n\\nPipConda%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraphconda install langchain-text-splitters langchain-community langgraph -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nComponents\\u200b\\nWe will need to select three components from LangChain\\'s suite of integrations.\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nPreview\\u200b\\nIn this guide we‚Äôll build an app that answers questions about the website\\'s content. The specific website we will use is the LLM Powered Autonomous\\nAgents blog post\\nby Lilian Weng, which allows us to ask questions about the contents of\\nthe post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~50\\nlines of code.\\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Index chunks_ = vector_store.add_documents(documents=all_splits)# Define prompt for question-answering# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    context: List[Document]    answer: str# Define application stepsdef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:Document | StateGraph\\nresponse = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(response[\"answer\"])\\nTask Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model\\'s thinking process.\\nCheck out the LangSmith\\ntrace.\\nDetailed walkthrough\\u200b\\nLet‚Äôs go through the above code step-by-step to really understand what‚Äôs\\ngoing on.\\n1. Indexing\\u200b\\nnoteThis section is an abbreviated version of the content in the semantic search tutorial.\\nIf you\\'re comfortable with document loaders, embeddings, and vector stores,\\nfeel free to skip to the next section on retrieval and generation.\\nLoading documents\\u200b\\nWe need to first load the blog post contents. We can use\\nDocumentLoaders\\nfor this, which are objects that load in data from a source and return a\\nlist of\\nDocument\\nobjects.\\nIn this case we‚Äôll use the\\nWebBaseLoader,\\nwhich uses urllib to load HTML from web URLs and BeautifulSoup to\\nparse it to text. We can customize the HTML -> text parsing by passing\\nin parameters into the BeautifulSoup parser via bs_kwargs (see\\nBeautifulSoup\\ndocs).\\nIn this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or\\n‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nimport bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()assert len(docs) == 1print(f\"Total characters: {len(docs[0].page_content)}\")\\nTotal characters: 43131\\nprint(docs[0].page_content[:500])\\n      LLM Powered Autonomous Agents    Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In\\nGo deeper\\u200b\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nDocs:\\nDetailed documentation on how to use DocumentLoaders.\\nIntegrations: 160+\\nintegrations to choose from.\\nInterface:\\nAPI reference for the base interface.\\n\\nSplitting documents\\u200b\\nOur loaded document is over 42k characters which is too long to fit\\ninto the context window of many models. Even for those models that could\\nfit the full post in their context window, models can struggle to find\\ninformation in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and\\nvector storage. This should help us retrieve only the most relevant parts\\nof the blog post at run time.\\nAs in the semantic search tutorial, we use a\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000,  # chunk size (characters)    chunk_overlap=200,  # chunk overlap (characters)    add_start_index=True,  # track index in original document)all_splits = text_splitter.split_documents(docs)print(f\"Split blog post into {len(all_splits)} sub-documents.\")\\nSplit blog post into 66 sub-documents.\\nGo deeper\\u200b\\nTextSplitter: Object that splits a list of Documents into smaller\\nchunks. Subclass of DocumentTransformers.\\n\\nLearn more about splitting text using different methods by reading the how-to docs\\nCode (py or js)\\nScientific papers\\nInterface: API reference for the base interface.\\n\\nDocumentTransformer: Object that performs a transformation on a list\\nof Document objects.\\n\\nDocs: Detailed documentation on how to use DocumentTransformers\\nIntegrations\\nInterface: API reference for the base interface.\\n\\nStoring documents\\u200b\\nNow we need to index our 66 text chunks so that we can search over them\\nat runtime. Following the semantic search tutorial,\\nour approach is to embed the contents of each document split and insert these embeddings\\ninto a vector store. Given an input query, we can then use\\nvector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command\\nusing the vector store and embeddings model selected at the start of the tutorial.\\ndocument_ids = vector_store.add_documents(documents=all_splits)print(document_ids[:3])\\n[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\nGo deeper\\u200b\\nEmbeddings: Wrapper around a text embedding model, used for converting\\ntext to embeddings.\\n\\nDocs: Detailed documentation on how to use embeddings.\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and\\nquerying embeddings.\\n\\nDocs: Detailed documentation on how to use vector stores.\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point\\nwe have a query-able vector store containing the chunked contents of our\\nblog post. Given a user question, we should ideally be able to return\\nthe snippets of the blog post that answer the question.\\n2. Retrieval and Generation\\u200b\\nNow let‚Äôs write the actual application logic. We want to create a simple\\napplication that takes a user question, searches for documents relevant\\nto that question, passes the retrieved documents and initial question to\\na model, and returns an answer.\\nFor generation, we will use the chat model selected at the start of the tutorial.\\nWe‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub\\n(here).\\nfrom langchain import hub# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke(    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}).to_messages()assert len(example_messages) == 1print(example_messages[0].content)\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.Question: (question goes here) Context: (context goes here) Answer:\\nWe\\'ll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\\n\\nWe can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.\\nWe get streamlined deployments via LangGraph Platform.\\nLangSmith will automatically trace the steps of our application together.\\nWe can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes.\\n\\nTo use LangGraph, we need to define three things:\\n\\nThe state of our application;\\nThe nodes of our application (i.e., application steps);\\nThe \"control flow\" of our application (e.g., the ordering of the steps).\\n\\nState:\\u200b\\nThe state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\\nFor a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer:\\nfrom langchain_core.documents import Documentfrom typing_extensions import List, TypedDictclass State(TypedDict):    question: str    context: List[Document]    answer: strAPI Reference:Document\\nNodes (application steps)\\u200b\\nLet\\'s start with a simple sequence of two steps: retrieval and generation.\\ndef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}\\nOur retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.\\nControl flow\\u200b\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the retrieval and generation steps into a single sequence.\\nfrom langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:StateGraph\\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nDo I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:question = \"...\"retrieved_docs = vector_store.similarity_search(question)docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)prompt = prompt.invoke({\"question\": question, \"context\": docs_content})answer = llm.invoke(prompt)The benefits of LangGraph include:\\nSupport for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\\nAutomatic support for tracing via LangSmith and deployments via LangGraph Platform;\\nSupport for persistence, human-in-the-loop, and other features.\\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in Part 2 of the tutorial, LangGraph\\'s management and persistence of state simplifies these applications enormously.\\nUsage\\u200b\\nLet\\'s test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\\nInvoke:\\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f\"Context: {result[\\'context\\']}\\\\n\\\\n\")print(f\"Answer: {result[\\'answer\\']}\")\\nContext: [Document(id=\\'a42dc78b-8f76-472a-9e25-180508af74f3\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}, page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\'), Document(id=\\'c0e45887-d0b0-483d-821a-bb5d8316d51d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}, page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\'), Document(id=\\'4cc7f318-35f5-440f-a4a4-145b5f0b918d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 29630}, page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\'), Document(id=\\'f621ade4-9b0d-471f-a522-44eb5feeba0c\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]Answer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs.\\nStream steps:\\nfor step in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"):    print(f\"{step}\\\\n\\\\n----------------\\\\n\")\\n{\\'retrieve\\': {\\'context\\': [Document(id=\\'a42dc78b-8f76-472a-9e25-180508af74f3\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}, page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\'), Document(id=\\'c0e45887-d0b0-483d-821a-bb5d8316d51d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}, page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\'), Document(id=\\'4cc7f318-35f5-440f-a4a4-145b5f0b918d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 29630}, page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\'), Document(id=\\'f621ade4-9b0d-471f-a522-44eb5feeba0c\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}----------------{\\'generate\\': {\\'answer\\': \\'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.\\'}}----------------\\nStream tokens:\\nfor message, metadata in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"):    print(message.content, end=\"|\")\\n|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|\\'s| reasoning| process|.||\\ntipFor async invocations, use:result = await graph.ainvoke(...)andasync for step in graph.astream(...):\\nReturning sources\\u200b\\nNote that by storing the retrieved context in the state of the graph, we recover sources for the model\\'s generated answer in the \"context\" field of the state. See this guide on returning sources for more detail.\\nGo deeper\\u200b\\nChat models take in a sequence of messages and return a message.\\n\\nDocs\\nIntegrations: 25+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nCustomizing the prompt\\nAs shown above, we can load prompts (e.g., this RAG\\nprompt) from the prompt\\nhub. The prompt can also be easily customized. For example:\\nfrom langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)API Reference:PromptTemplate\\nQuery analysis\\u200b\\nSo far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:\\n\\nIn addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\");\\nThe model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.\\n\\nQuery analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let\\'s add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.\\ntotal_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"all_splits[0].metadata\\n{\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 8, \\'section\\': \\'beginning\\'}\\nWe will need to update the documents in our vector store. We will use a simple InMemoryVectorStore for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store integration documentation for relevant features of your chosen vector store.\\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)API Reference:InMemoryVectorStore\\nLet\\'s next define a schema for our search query. We will use structured output for this purpose. Here we define a query as containing a string query and a document section (either \"beginning\", \"middle\", or \"end\"), but this can be defined however you like.\\nfrom typing import Literalfrom typing_extensions import Annotatedclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]\\nFinally, we add a step to our LangGraph application to generate a query from the user\\'s raw input:\\nclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()\\nFull Code:from typing import Literalimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import Annotated, List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Update metadata (illustration purposes)total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"# Index chunksvector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)# Define schema for searchclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()API Reference:Document | InMemoryVectorStore | StateGraph\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nWe can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.\\nfor step in graph.stream(    {\"question\": \"What does the end of the post say about Task Decomposition?\"},    stream_mode=\"updates\",):    print(f\"{step}\\\\n\\\\n----------------\\\\n\")\\n{\\'analyze_query\\': {\\'query\\': {\\'query\\': \\'Task Decomposition\\', \\'section\\': \\'end\\'}}}----------------{\\'retrieve\\': {\\'context\\': [Document(id=\\'d6cef137-e1e8-4ddc-91dc-b62bd33c6020\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 39221, \\'section\\': \\'end\\'}, page_content=\\'Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\\\n\\\\n\\\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\'), Document(id=\\'d1834ae1-eb6a-43d7-a023-08dfa5028799\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 39086, \\'section\\': \\'end\\'}, page_content=\\'}\\\\n]\\\\nChallenges#\\\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\'), Document(id=\\'ca7f06e4-2c2e-4788-9a81-2418d82213d9\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 32942, \\'section\\': \\'end\\'}, page_content=\\'}\\\\n]\\\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\\\nSystem message:\\'), Document(id=\\'1fcc2736-30f4-4ef6-90f2-c64af92118cb\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 35127, \\'section\\': \\'end\\'}, page_content=\\'\"content\": \"You will get instructions for code to write.\\\\\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\\\\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\\\`\\\\`\\\\`LANG\\\\\\\\nCODE\\\\\\\\n\\\\`\\\\`\\\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\"entrypoint\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease\\')]}}----------------{\\'generate\\': {\\'answer\\': \\'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.\\'}}----------------\\nIn both the streamed steps and the LangSmith trace, we can now observe the structured query that was fed into the retrieval step.\\nQuery Analysis is a rich problem with a wide range of approaches. Refer to the how-to guides for more examples.\\nNext steps\\u200b\\nWe\\'ve covered the steps to build a basic Q&A app over data:\\n\\nLoading data with a Document Loader\\nChunking the indexed data with a Text Splitter to make it more easily usable by a model\\nEmbedding the data and storing the data in a vectorstore\\nRetrieving the previously stored chunks in response to incoming questions\\nGenerating an answer using the retrieved chunks as context.\\n\\nIn Part 2 of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.\\nFurther reading:\\n\\nReturn sources: Learn how to return source documents\\nStreaming: Learn how to stream outputs and intermediate steps\\nAdd chat history: Learn how to add chat history to your app\\nRetrieval conceptual guide: A high-level overview of specific retrieval techniques\\nEdit this pagePreviousTaggingNextBuild a semantic search engineOverviewIndexingRetrieval and generationSetupJupyter NotebookInstallationLangSmithComponentsPreviewDetailed walkthrough1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationQuery analysisNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain', 'description': 'In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Retrieval Augmented Generation (RAG) App: Part 2 | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 2On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 2\\nIn many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\\nThis is the second part of a multi-part tutorial:\\n\\nPart 1 introduces RAG and walks through a minimal implementation.\\nPart 2 (this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\\n\\nHere we focus on adding logic for incorporating historical messages. This involves the management of a chat history.\\nWe will cover two approaches:\\n\\nChains, in which we execute at most one retrieval step;\\nAgents, in which we give an LLM discretion to execute multiple retrieval steps.\\n\\nnoteThe methods presented here leverage tool-calling capabilities in modern chat models. See this page for a table of models supporting tool calling features.\\nFor the external knowledge source, we will use the same LLM Powered Autonomous Agents blog post by Lilian Weng from the Part 1 of the RAG tutorial.\\nSetup\\u200b\\nComponents\\u200b\\nWe will need to select three components from LangChain\\'s suite of integrations.\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nDependencies\\u200b\\nIn addition, we\\'ll use the following packages:\\n%%capture --no-stderr%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"if not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nChains\\u200b\\nLet\\'s first revisit the vector store we built in Part 1, which indexes an LLM Powered Autonomous Agents blog post by Lilian Weng.\\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)API Reference:Document\\n# Index chunks_ = vector_store.add_documents(documents=all_splits)\\nIn the Part 1 of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of messages. In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via tool messages. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have\\n\\nUser input as a HumanMessage;\\nVector store query as an AIMessage with tool calls;\\nRetrieved documents as a ToolMessage;\\nFinal response as a AIMessage.\\n\\nThis model for state is so versatile that LangGraph offers a built-in version for convenience:\\nfrom langgraph.graph import MessagesState, StateGraphgraph_builder = StateGraph(MessagesState)API Reference:StateGraph\\nLeveraging tool-calling to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:\\n\\nHuman: \"What is Task Decomposition?\"\\nAI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\\nHuman: \"What are common ways of doing it?\"\\n\\nIn this scenario, a model could generate a query such as \"common approaches to task decomposition\". Tool-calling facilitates this naturally. As in the query analysis section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user).\\nLet\\'s turn our retrieval step into a tool:\\nfrom langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def retrieve(query: str):    \"\"\"Retrieve information related to a query.\"\"\"    retrieved_docs = vector_store.similarity_search(query, k=2)    serialized = \"\\\\n\\\\n\".join(        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")        for doc in retrieved_docs    )    return serialized, retrieved_docsAPI Reference:tool\\nSee this guide for more detail on creating tools.\\nOur graph will consist of three nodes:\\n\\nA node that fields the user input, either generating a query for the retriever or responding directly;\\nA node for the retriever tool that executes the retrieval step;\\nA node that generates the final response using the retrieved context.\\n\\nWe build them below. Note that we leverage another pre-built LangGraph component, ToolNode, that executes the tool and adds the result as a ToolMessage to the state.\\nfrom langchain_core.messages import SystemMessagefrom langgraph.prebuilt import ToolNode# Step 1: Generate an AIMessage that may include a tool-call to be sent.def query_or_respond(state: MessagesState):    \"\"\"Generate tool call for retrieval or respond.\"\"\"    llm_with_tools = llm.bind_tools([retrieve])    response = llm_with_tools.invoke(state[\"messages\"])    # MessagesState appends messages to state instead of overwriting    return {\"messages\": [response]}# Step 2: Execute the retrieval.tools = ToolNode([retrieve])# Step 3: Generate a response using the retrieved content.def generate(state: MessagesState):    \"\"\"Generate answer.\"\"\"    # Get generated ToolMessages    recent_tool_messages = []    for message in reversed(state[\"messages\"]):        if message.type == \"tool\":            recent_tool_messages.append(message)        else:            break    tool_messages = recent_tool_messages[::-1]    # Format into prompt    docs_content = \"\\\\n\\\\n\".join(doc.content for doc in tool_messages)    system_message_content = (        \"You are an assistant for question-answering tasks. \"        \"Use the following pieces of retrieved context to answer \"        \"the question. If you don\\'t know the answer, say that you \"        \"don\\'t know. Use three sentences maximum and keep the \"        \"answer concise.\"        \"\\\\n\\\\n\"        f\"{docs_content}\"    )    conversation_messages = [        message        for message in state[\"messages\"]        if message.type in (\"human\", \"system\")        or (message.type == \"ai\" and not message.tool_calls)    ]    prompt = [SystemMessage(system_message_content)] + conversation_messages    # Run    response = llm.invoke(prompt)    return {\"messages\": [response]}API Reference:SystemMessage | ToolNode\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the steps into a sequence. We also allow the first query_or_respond step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\\nfrom langgraph.graph import ENDfrom langgraph.prebuilt import ToolNode, tools_conditiongraph_builder.add_node(query_or_respond)graph_builder.add_node(tools)graph_builder.add_node(generate)graph_builder.set_entry_point(\"query_or_respond\")graph_builder.add_conditional_edges(    \"query_or_respond\",    tools_condition,    {END: END, \"tools\": \"tools\"},)graph_builder.add_edge(\"tools\", \"generate\")graph_builder.add_edge(\"generate\", END)graph = graph_builder.compile()API Reference:ToolNode | tools_condition\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nLet\\'s test our application.\\nNote that it responds appropriately to messages that do not require an additional retrieval step:\\ninput_message = \"Hello\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hello==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello! How can I assist you today?\\nAnd when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:\\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is Task Decomposition?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_dLjB3rkMoxZZxwUGXi33UBeh) Call ID: call_dLjB3rkMoxZZxwUGXi33UBeh  Args:    query: Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\\x1b[1m Ai Message \\x1b[0m==================================Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often involves techniques like Chain of Thought (CoT), which encourages models to think step by step, enhancing performance on complex tasks. This approach allows for a clearer understanding of the task and aids in structuring the problem-solving process.\\nCheck out the LangSmith trace here.\\nStateful management of chat history\\u200b\\nnoteThis section of the tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nIn production, the Q&A application will usually persist the chat history into a database, and be able to read and update it appropriately.\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nTo manage multiple conversational turns and threads, all we have to do is specify a checkpointer when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\nFor a detailed walkthrough of how to manage message history, head to the How to add message history (memory) guide.\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory)# Specify an ID for the threadconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}API Reference:MemorySaver\\nWe can now invoke similar to before:\\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is Task Decomposition?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_JZb6GLD812bW2mQsJ5EJQDnN) Call ID: call_JZb6GLD812bW2mQsJ5EJQDnN  Args:    query: Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\\x1b[1m Ai Message \\x1b[0m==================================Task Decomposition is a technique used to break down complicated tasks into smaller, manageable steps. It involves using methods like Chain of Thought (CoT) prompting, which encourages the model to think step by step, enhancing performance on complex tasks. This process helps to clarify the model\\'s reasoning and makes it easier to tackle difficult problems.\\ninput_message = \"Can you look up some common ways of doing it?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Can you look up some common ways of doing it?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_kjRI4Y5cJOiB73yvd7dmb6ux) Call ID: call_kjRI4Y5cJOiB73yvd7dmb6ux  Args:    query: common methods of task decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================Common ways of performing Task Decomposition include: (1) using Large Language Models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", (2) employing task-specific instructions such as \"Write a story outline\" for specific tasks, and (3) incorporating human inputs to guide the decomposition process.\\nNote that the query generated by the model in the second question incorporates the conversational context.\\nThe LangSmith trace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step.\\nAgents\\u200b\\nAgents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.\\nBelow we assemble a minimal RAG agent. Using LangGraph\\'s pre-built ReAct agent constructor, we can do this in one line.\\ntipCheck out LangGraph\\'s Agentic RAG tutorial for more advanced formulations.\\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)API Reference:create_react_agent\\nLet\\'s inspect the graph:\\ndisplay(Image(agent_executor.get_graph().draw_mermaid_png()))\\n\\nThe key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.\\nLet\\'s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nconfig = {\"configurable\": {\"thread_id\": \"def234\"}}input_message = (    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"    \"Once you get the answer, look up common extensions of that method.\")for event in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    event[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is the standard method for Task Decomposition?Once you get the answer, look up common extensions of that method.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_Y3YaIzL71B83Cjqa8d2G0O8N) Call ID: call_Y3YaIzL71B83Cjqa8d2G0O8N  Args:    query: standard method for Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_2JntP1x4XQMWwgVpYurE12ff) Call ID: call_2JntP1x4XQMWwgVpYurE12ff  Args:    query: common extensions of Task Decomposition methods=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================The standard method for task decomposition involves using techniques such as Chain of Thought (CoT), where a model is instructed to \"think step by step\" to break down complex tasks into smaller, more manageable components. This approach enhances model performance by allowing for more thorough reasoning and planning. Task decomposition can be accomplished through various means, including:1. Simple prompting (e.g., asking for steps to achieve a goal).2. Task-specific instructions (e.g., asking for a story outline).3. Human inputs to guide the decomposition process.### Common Extensions of Task Decomposition Methods:1. **Tree of Thoughts**: This extension builds on CoT by not only decomposing the problem into thought steps but also generating multiple thoughts at each step, creating a tree structure. The search process can employ breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.These extensions aim to enhance reasoning capabilities and improve the effectiveness of task decomposition in various contexts.\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nNext steps\\u200b\\nWe\\'ve covered the steps to build a basic conversational Q&A application:\\n\\nWe used chains to build a predictable application that generates at most one query per user input;\\nWe used agents to build an application that can iterate on a sequence of queries.\\n\\nTo explore different types of retrievers and retrieval strategies, visit the retrievers section of the how-to guides.\\nFor a detailed walkthrough of LangChain\\'s conversation memory abstractions, visit the How to add message history (memory) guide.\\nTo learn more about agents, check out the conceptual guide and LangGraph agent architectures page.Edit this pagePreviousBuild a ChatbotNextBuild an Extraction ChainSetupComponentsDependenciesLangSmithChainsStateful management of chat historyAgentsNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/sql_qa/', 'title': 'Build a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Question/Answering system over SQL data | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Question/Answering system over SQL dataOn this pageBuild a Question/Answering system over SQL data\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nTools\\nAgents\\nLangGraph\\n\\nEnabling a LLM system to query structured data can be qualitatively different from unstructured text data. Whereas in the latter it is common to generate text that can be searched against a vector database, the approach for structured data is often for the LLM to write and execute queries in a DSL, such as SQL. In this guide we\\'ll go over the basic ways to create a Q&A system over tabular data in databases. We will cover implementations using both chains and agents. These systems will allow us to ask a question about the data in a database and get back a natural language answer. The main difference between the two is that our agent can query the database in a loop as many times as it needs to answer the question.\\n‚ö†Ô∏è Security note ‚ö†Ô∏è\\u200b\\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent\\'s needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see here.\\nArchitecture\\u200b\\nAt a high-level, the steps of these systems are:\\n\\nConvert question to SQL query: Model converts user input to a SQL query.\\nExecute SQL query: Execute the query.\\nAnswer the question: Model responds to user input using the query results.\\n\\nNote that querying data in CSVs can follow a similar approach. See our how-to guide on question-answering over CSV data for more detail.\\n\\nSetup\\u200b\\nFirst, get required packages and set environment variables:\\n%%capture --no-stderr%pip install --upgrade --quiet langchain-community langgraph\\n# Comment out the below to opt-out of using LangSmith in this notebook. Not required.if not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()    os.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nSample data\\u200b\\nThe below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow these installation steps to create Chinook.db in the same directory as this notebook. You can also download and build the database via the command line:\\ncurl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db\\nNow, Chinook.db is in our directory and we can interface with it using the SQLAlchemy-driven SQLDatabase class:\\nfrom langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")print(db.dialect)print(db.get_usable_table_names())db.run(\"SELECT * FROM Artist LIMIT 10;\")\\nsqlite[\\'Album\\', \\'Artist\\', \\'Customer\\', \\'Employee\\', \\'Genre\\', \\'Invoice\\', \\'InvoiceLine\\', \\'MediaType\\', \\'Playlist\\', \\'PlaylistTrack\\', \\'Track\\']\\n\"[(1, \\'AC/DC\\'), (2, \\'Accept\\'), (3, \\'Aerosmith\\'), (4, \\'Alanis Morissette\\'), (5, \\'Alice In Chains\\'), (6, \\'Ant√¥nio Carlos Jobim\\'), (7, \\'Apocalyptica\\'), (8, \\'Audioslave\\'), (9, \\'BackBeat\\'), (10, \\'Billy Cobham\\')]\"\\nGreat! We\\'ve got a SQL database that we can query. Now let\\'s try hooking it up to an LLM.\\nChains\\u200b\\nChains are compositions of predictable steps. In LangGraph, we can represent a chain via simple sequence of nodes. Let\\'s create a sequence of steps that, given a question, does the following:\\n\\nconverts the question into a SQL query;\\nexecutes the query;\\nuses the result to answer the original question.\\n\\nThere are scenarios not supported by this arrangement. For example, this system will execute a SQL query for any user input-- even \"hello\". Importantly, as we\\'ll see below, some questions require more than one query to answer. We will address these scenarios in the Agents section.\\nApplication state\\u200b\\nThe LangGraph state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\\nFor this application, we can just keep track of the input question, generated query, query result, and generated answer:\\nfrom typing_extensions import TypedDictclass State(TypedDict):    question: str    query: str    result: str    answer: str\\nNow we just need functions that operate on this state and populate its contents.\\nConvert question to SQL query\\u200b\\nThe first step is to take the user input and convert it to a SQL query. To reliably obtain SQL queries (absent markdown formatting and explanations or clarifications), we will make use of LangChain\\'s structured output abstraction.\\nLet\\'s select a chat model for our application:\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nLet\\'s provide some instructions for our model:\\nfrom langchain_core.prompts import ChatPromptTemplatesystem_message = \"\"\"Given an input question, create a syntactically correct {dialect} query torun to help find the answer. Unless the user specifies in his question aspecific number of examples they wish to obtain, always limit your query toat most {top_k} results. You can order the results by a relevant column toreturn the most interesting examples in the database.Never query for all the columns from a specific table, only ask for a thefew relevant columns given the question.Pay attention to use only the column names that you can see in the schemadescription. Be careful to not query for columns that do not exist. Also,pay attention to which column is in which table.Only use the following tables:{table_info}\"\"\"user_prompt = \"Question: {input}\"query_prompt_template = ChatPromptTemplate(    [(\"system\", system_message), (\"user\", user_prompt)])for message in query_prompt_template.messages:    message.pretty_print()API Reference:ChatPromptTemplate\\n================================\\x1b[1m System Message \\x1b[0m================================Given an input question, create a syntactically correct \\x1b[33;1m\\x1b[1;3m{dialect}\\x1b[0m query torun to help find the answer. Unless the user specifies in his question aspecific number of examples they wish to obtain, always limit your query toat most \\x1b[33;1m\\x1b[1;3m{top_k}\\x1b[0m results. You can order the results by a relevant column toreturn the most interesting examples in the database.Never query for all the columns from a specific table, only ask for a thefew relevant columns given the question.Pay attention to use only the column names that you can see in the schemadescription. Be careful to not query for columns that do not exist. Also,pay attention to which column is in which table.Only use the following tables:\\x1b[33;1m\\x1b[1;3m{table_info}\\x1b[0m================================\\x1b[1m Human Message \\x1b[0m=================================Question: \\x1b[33;1m\\x1b[1;3m{input}\\x1b[0m\\nThe prompt includes several parameters we will need to populate, such as the SQL dialect and table schemas. LangChain\\'s SQLDatabase object includes methods to help with this. Our write_query step will just populate these parameters and prompt a model to generate the SQL query:\\nfrom typing_extensions import Annotatedclass QueryOutput(TypedDict):    \"\"\"Generated SQL query.\"\"\"    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]def write_query(state: State):    \"\"\"Generate SQL query to fetch information.\"\"\"    prompt = query_prompt_template.invoke(        {            \"dialect\": db.dialect,            \"top_k\": 10,            \"table_info\": db.get_table_info(),            \"input\": state[\"question\"],        }    )    structured_llm = llm.with_structured_output(QueryOutput)    result = structured_llm.invoke(prompt)    return {\"query\": result[\"query\"]}\\nLet\\'s test it out:\\nwrite_query({\"question\": \"How many Employees are there?\"})\\n{\\'query\\': \\'SELECT COUNT(*) as employee_count FROM Employee;\\'}\\nExecute query\\u200b\\nThis is the most dangerous part of creating a SQL chain. Consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution (see below).\\nTo execute the query, we will load a tool from langchain-community. Our execute_query node will just wrap this tool:\\nfrom langchain_community.tools.sql_database.tool import QuerySQLDatabaseTooldef execute_query(state: State):    \"\"\"Execute SQL query.\"\"\"    execute_query_tool = QuerySQLDatabaseTool(db=db)    return {\"result\": execute_query_tool.invoke(state[\"query\"])}\\nTesting this step:\\nexecute_query({\"query\": \"SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;\"})\\n{\\'result\\': \\'[(8,)]\\'}\\nGenerate answer\\u200b\\nFinally, our last step generates an answer to the question given the information pulled from the database:\\ndef generate_answer(state: State):    \"\"\"Answer question using retrieved information as context.\"\"\"    prompt = (        \"Given the following user question, corresponding SQL query, \"        \"and SQL result, answer the user question.\\\\n\\\\n\"        f\"Question: {state[\\'question\\']}\\\\n\"        f\"SQL Query: {state[\\'query\\']}\\\\n\"        f\"SQL Result: {state[\\'result\\']}\"    )    response = llm.invoke(prompt)    return {\"answer\": response.content}\\nOrchestrating with LangGraph\\u200b\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the three steps into a single sequence.\\nfrom langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence(    [write_query, execute_query, generate_answer])graph_builder.add_edge(START, \"write_query\")graph = graph_builder.compile()API Reference:StateGraph\\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nLet\\'s test our application! Note that we can stream the results of individual steps:\\nfor step in graph.stream(    {\"question\": \"How many employees are there?\"}, stream_mode=\"updates\"):    print(step)\\n{\\'write_query\\': {\\'query\\': \\'SELECT COUNT(*) as employee_count FROM Employee;\\'}}{\\'execute_query\\': {\\'result\\': \\'[(8,)]\\'}}{\\'generate_answer\\': {\\'answer\\': \\'There are 8 employees in total.\\'}}\\nCheck out the LangSmith trace.\\nHuman-in-the-loop\\u200b\\nLangGraph supports a number of features that can be useful for this workflow. One of them is human-in-the-loop: we can interrupt our application before sensitive steps (such as the execution of a SQL query) for human review. This is enabled by LangGraph\\'s persistence layer, which saves run progress to your storage of choice. Below, we specify storage in-memory:\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory, interrupt_before=[\"execute_query\"])# Now that we\\'re using persistence, we need to specify a thread ID# so that we can continue the run after review.config = {\"configurable\": {\"thread_id\": \"1\"}}API Reference:MemorySaver\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nLet\\'s repeat the same run, adding in a simple yes/no approval step:\\nfor step in graph.stream(    {\"question\": \"How many employees are there?\"},    config,    stream_mode=\"updates\",):    print(step)try:    user_approval = input(\"Do you want to go to execute query? (yes/no): \")except Exception:    user_approval = \"no\"if user_approval.lower() == \"yes\":    # If approved, continue the graph execution    for step in graph.stream(None, config, stream_mode=\"updates\"):        print(step)else:    print(\"Operation cancelled by user.\")\\n{\\'write_query\\': {\\'query\\': \\'SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;\\'}}{\\'__interrupt__\\': ()}``````outputDo you want to go to execute query? (yes/no):  yes``````output{\\'execute_query\\': {\\'result\\': \\'[(8,)]\\'}}{\\'generate_answer\\': {\\'answer\\': \\'There are 8 employees.\\'}}\\nSee this LangGraph guide for more detail and examples.\\nNext steps\\u200b\\nFor more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:\\n\\nPrompting strategies: Advanced prompt engineering techniques.\\nQuery checking: Add query validation and error handling.\\nLarge databases: Techniques for working with large databases.\\n\\nAgents\\u200b\\nAgents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the query generation and execution process. Although their behavior is less predictable than the above \"chain\", they feature some advantages:\\n\\nThey can query the database as many times as needed to answer the user question.\\nThey can recover from errors by running a generated query, catching the traceback and regenerating it correctly.\\nThey can answer questions based on the databases\\' schema as well as on the databases\\' content (like describing a specific table).\\n\\nBelow we assemble a minimal SQL agent. We will equip it with a set of tools using LangChain\\'s SQLDatabaseToolkit. Using LangGraph\\'s pre-built ReAct agent constructor, we can do this in one line.\\ntipCheck out LangGraph\\'s SQL Agent Tutorial for a more advanced formulation of a SQL agent.\\nThe SQLDatabaseToolkit includes tools that can:\\n\\nCreate and execute queries\\nCheck query syntax\\nRetrieve table descriptions\\n... and more\\n\\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkittoolkit = SQLDatabaseToolkit(db=db, llm=llm)tools = toolkit.get_tools()tools\\n[QuerySQLDatabaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column \\'xxxx\\' in \\'field list\\', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), InfoSQLDatabaseTool(description=\\'Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\\', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>), QuerySQLCheckerTool(description=\\'Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\\', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name=\\'gpt-4o\\', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=[\\'dialect\\', \\'query\\'], input_types={}, partial_variables={}, template=\\'\\\\n{query}\\\\nDouble check the {dialect} query above for common mistakes, including:\\\\n- Using NOT IN with NULL values\\\\n- Using UNION when UNION ALL should have been used\\\\n- Using BETWEEN for exclusive ranges\\\\n- Data type mismatch in predicates\\\\n- Properly quoting identifiers\\\\n- Using the correct number of arguments for functions\\\\n- Casting to the correct data type\\\\n- Using the proper columns for joins\\\\n\\\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\\\n\\\\nOutput the final SQL query only.\\\\n\\\\nSQL Query: \\'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name=\\'gpt-4o\\', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')), output_parser=StrOutputParser(), llm_kwargs={}))]\\nSystem Prompt\\u200b\\nWe will also want to load a system prompt for our agent. This will consist of instructions for how to behave. Note that the prompt below has several parameters, which we assign below.\\nsystem_message = \"\"\"You are an agent designed to interact with a SQL database.Given an input question, create a syntactically correct {dialect} query to run,then look at the results of the query and return the answer. Unless the userspecifies a specific number of examples they wish to obtain, always limit yourquery to at most {top_k} results.You can order the results by a relevant column to return the most interestingexamples in the database. Never query for all the columns from a specific table,only ask for the relevant columns given the question.You MUST double check your query before executing it. If you get an error whileexecuting a query, rewrite the query and try again.DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to thedatabase.To start you should ALWAYS look at the tables in the database to see what youcan query. Do NOT skip this step.Then you should query the schema of the most relevant tables.\"\"\".format(    dialect=\"SQLite\",    top_k=5,)\\nInitializing agent\\u200b\\nWe will use a prebuilt LangGraph agent to build our agent\\nfrom langchain_core.messages import HumanMessagefrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, tools, prompt=system_message)API Reference:HumanMessage | create_react_agent\\nConsider how the agent responds to the below question:\\nquestion = \"Which country\\'s customers spent the most?\"for step in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Which country\\'s customers spent the most?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_list_tables (call_tFp7HYD6sAAmCShgeqkVZH6Q) Call ID: call_tFp7HYD6sAAmCShgeqkVZH6Q  Args:=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_list_tablesAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_schema (call_KJZ1Jx6JazyDdJa0uH1UeiOz) Call ID: call_KJZ1Jx6JazyDdJa0uH1UeiOz  Args:    table_names: Customer, Invoice=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_schemaCREATE TABLE \"Customer\" (\\t\"CustomerId\" INTEGER NOT NULL, \\t\"FirstName\" NVARCHAR(40) NOT NULL, \\t\"LastName\" NVARCHAR(20) NOT NULL, \\t\"Company\" NVARCHAR(80), \\t\"Address\" NVARCHAR(70), \\t\"City\" NVARCHAR(40), \\t\"State\" NVARCHAR(40), \\t\"Country\" NVARCHAR(40), \\t\"PostalCode\" NVARCHAR(10), \\t\"Phone\" NVARCHAR(24), \\t\"Fax\" NVARCHAR(24), \\t\"Email\" NVARCHAR(60) NOT NULL, \\t\"SupportRepId\" INTEGER, \\tPRIMARY KEY (\"CustomerId\"), \\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\"))/*3 rows from Customer table:CustomerId\\tFirstName\\tLastName\\tCompany\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\tSupportRepId1\\tLu√≠s\\tGon√ßalves\\tEmbraer - Empresa Brasileira de Aeron√°utica S.A.\\tAv. Brigadeiro Faria Lima, 2170\\tS√£o Jos√© dos Campos\\tSP\\tBrazil\\t12227-000\\t+55 (12) 3923-5555\\t+55 (12) 3923-5566\\tluisg@embraer.com.br\\t32\\tLeonie\\tK√∂hler\\tNone\\tTheodor-Heuss-Stra√üe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t+49 0711 2842222\\tNone\\tleonekohler@surfeu.de\\t53\\tFran√ßois\\tTremblay\\tNone\\t1498 rue B√©langer\\tMontr√©al\\tQC\\tCanada\\tH2G 1A7\\t+1 (514) 721-4711\\tNone\\tftremblay@gmail.com\\t3*/CREATE TABLE \"Invoice\" (\\t\"InvoiceId\" INTEGER NOT NULL, \\t\"CustomerId\" INTEGER NOT NULL, \\t\"InvoiceDate\" DATETIME NOT NULL, \\t\"BillingAddress\" NVARCHAR(70), \\t\"BillingCity\" NVARCHAR(40), \\t\"BillingState\" NVARCHAR(40), \\t\"BillingCountry\" NVARCHAR(40), \\t\"BillingPostalCode\" NVARCHAR(10), \\t\"Total\" NUMERIC(10, 2) NOT NULL, \\tPRIMARY KEY (\"InvoiceId\"), \\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\"))/*3 rows from Invoice table:InvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal1\\t2\\t2021-01-01 00:00:00\\tTheodor-Heuss-Stra√üe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.982\\t4\\t2021-01-02 00:00:00\\tUllev√•lsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.963\\t8\\t2021-01-03 00:00:00\\tGr√©trystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94*/==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_query_checker (call_AQuTGbgH63u4gPgyV723yrjX) Call ID: call_AQuTGbgH63u4gPgyV723yrjX  Args:    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_query_checker\\\\`\\\\`\\\\`sqlSELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;\\\\`\\\\`\\\\`==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_query (call_B88EwU44nwwpQL5M9nlcemSU) Call ID: call_B88EwU44nwwpQL5M9nlcemSU  Args:    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_query[(\\'USA\\', 523.06)]==================================\\x1b[1m Ai Message \\x1b[0m==================================The country whose customers spent the most is the USA, with a total spending of 523.06.\\nYou can also use the LangSmith trace to visualize these steps and associated metadata.\\nNote that the agent executes multiple queries until it has the information it needs:\\n\\nList available tables;\\nRetrieves the schema for three tables;\\nQueries multiple of the tables via a join operation.\\n\\nThe agent is then able to use the result of the final query to generate an answer to the original question.\\nThe agent can similarly handle qualitative questions:\\nquestion = \"Describe the playlisttrack table\"for step in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Describe the playlisttrack table==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_list_tables (call_fMF8eTmX5TJDJjc3Mhdg52TI) Call ID: call_fMF8eTmX5TJDJjc3Mhdg52TI  Args:=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_list_tablesAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_schema (call_W8Vkk4NEodkAAIg8nexAszUH) Call ID: call_W8Vkk4NEodkAAIg8nexAszUH  Args:    table_names: PlaylistTrack=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_schemaCREATE TABLE \"PlaylistTrack\" (\\t\"PlaylistId\" INTEGER NOT NULL, \\t\"TrackId\" INTEGER NOT NULL, \\tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\"))/*3 rows from PlaylistTrack table:PlaylistId\\tTrackId1\\t34021\\t33891\\t3390*/==================================\\x1b[1m Ai Message \\x1b[0m==================================The `PlaylistTrack` table is designed to associate tracks with playlists. It has the following structure:- **PlaylistId**: An integer that serves as a foreign key referencing the `Playlist` table. It is part of the composite primary key.- **TrackId**: An integer that serves as a foreign key referencing the `Track` table. It is also part of the composite primary key.The primary key for this table is a composite key consisting of both `PlaylistId` and `TrackId`, ensuring that each track can be uniquely associated with a playlist. The table enforces referential integrity by linking to the `Track` and `Playlist` tables through foreign keys.\\nDealing with high-cardinality columns\\u200b\\nIn order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.\\nWe can achieve this by creating a vector store with all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.\\nFirst we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:\\nimport astimport redef query_as_list(db, query):    res = db.run(query)    res = [el for sub in ast.literal_eval(res) for el in sub if el]    res = [re.sub(r\"\\\\b\\\\d+\\\\b\", \"\", string).strip() for string in res]    return list(set(res))artists = query_as_list(db, \"SELECT Name FROM Artist\")albums = query_as_list(db, \"SELECT Title FROM Album\")albums[:5]\\n[\\'In Through The Out Door\\', \\'Transmission\\', \\'Battlestar Galactica (Classic), Season\\', \\'A Copland Celebration, Vol. I\\', \\'Quiet Songs\\']\\nUsing this function, we can create a retriever tool that the agent can execute at its discretion.\\nLet\\'s select an embeddings model and vector store for this step:\\nSelect an embedding model:\\n\\nSelect embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakeOCIGenAIEmbeddingspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nSelect a vector store:\\n\\nSelect vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nWe can now construct a retrieval tool that can search over relevant proper nouns in the database:\\nfrom langchain.agents.agent_toolkits import create_retriever_tool_ = vector_store.add_texts(artists + albums)retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})description = (    \"Use to look up values to filter on. Input is an approximate spelling \"    \"of the proper noun, output is valid proper nouns. Use the noun most \"    \"similar to the search.\")retriever_tool = create_retriever_tool(    retriever,    name=\"search_proper_nouns\",    description=description,)\\nLet\\'s try it out:\\nprint(retriever_tool.invoke(\"Alice Chains\"))\\nAlice In ChainsAlanis MorissettePearl JamPearl JamAudioslave\\nThis way, if the agent determines it needs to write a filter based on an artist along the lines of \"Alice Chains\", it can first use the retriever tool to observe relevant values of a column.\\nPutting this together:\\n# Add to system messagesuffix = (    \"If you need to filter on a proper noun like a Name, you must ALWAYS first look up \"    \"the filter value using the \\'search_proper_nouns\\' tool! Do not try to \"    \"guess at the proper name - use this function to find similar ones.\")system = f\"{system_message}\\\\n\\\\n{suffix}\"tools.append(retriever_tool)agent = create_react_agent(llm, tools, prompt=system)\\nquestion = \"How many albums does alis in chain have?\"for step in agent.stream(    {\"messages\": [{\"role\": \"user\", \"content\": question}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================How many albums does alis in chain have?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  search_proper_nouns (call_8ryjsRPLAr79mM3Qvnq6gTOH) Call ID: call_8ryjsRPLAr79mM3Qvnq6gTOH  Args:    query: alis in chain=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: search_proper_nounsAlice In ChainsAisha DuoXisDa Lama Ao CaosA-Sides==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_list_tables (call_NJjtCpU89MBMplssjn1z0xzq) Call ID: call_NJjtCpU89MBMplssjn1z0xzq  Args:  search_proper_nouns (call_1BfrueC9koSIyi4OfMu2Ao8q) Call ID: call_1BfrueC9koSIyi4OfMu2Ao8q  Args:    query: Alice In Chains=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: search_proper_nounsAlice In ChainsPearl JamPearl JamFoo FightersSoundgarden==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_schema (call_Kn09w9jd9swcNzIZ1b5MlKID) Call ID: call_Kn09w9jd9swcNzIZ1b5MlKID  Args:    table_names: Album, Artist=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_schemaCREATE TABLE \"Album\" (\\t\"AlbumId\" INTEGER NOT NULL, \\t\"Title\" NVARCHAR(160) NOT NULL, \\t\"ArtistId\" INTEGER NOT NULL, \\tPRIMARY KEY (\"AlbumId\"), \\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\"))/*3 rows from Album table:AlbumId\\tTitle\\tArtistId1\\tFor Those About To Rock We Salute You\\t12\\tBalls to the Wall\\t23\\tRestless and Wild\\t2*/CREATE TABLE \"Artist\" (\\t\"ArtistId\" INTEGER NOT NULL, \\t\"Name\" NVARCHAR(120), \\tPRIMARY KEY (\"ArtistId\"))/*3 rows from Artist table:ArtistId\\tName1\\tAC/DC2\\tAccept3\\tAerosmith*/==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  sql_db_query (call_WkHRiPcBoGN9bc58MIupRHKP) Call ID: call_WkHRiPcBoGN9bc58MIupRHKP  Args:    query: SELECT COUNT(*) FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = \\'Alice In Chains\\')=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: sql_db_query[(1,)]==================================\\x1b[1m Ai Message \\x1b[0m==================================Alice In Chains has released 1 album in the database.\\nAs we can see, both in the streamed steps and in the LangSmith trace, the agent used the search_proper_nouns tool in order to check how to correctly query the database for this specific artist.Edit this pagePreviousBuild a semantic search engineNextSummarize Text‚ö†Ô∏è Security note ‚ö†Ô∏èArchitectureSetupSample dataChainsApplication stateConvert question to SQL queryExecute queryGenerate answerOrchestrating with LangGraphHuman-in-the-loopNext stepsAgentsSystem PromptInitializing agentDealing with high-cardinality columnsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/summarization/', 'title': 'Summarize Text | ü¶úÔ∏èüîó LangChain', 'description': 'This tutorial demonstrates text summarization using built-in chains and LangGraph.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nSummarize Text | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsSummarize TextOn this pageSummarize Text\\ninfoThis tutorial demonstrates text summarization using built-in chains and LangGraph.A previous version of this page showcased the legacy chains StuffDocumentsChain, MapReduceDocumentsChain, and RefineDocumentsChain. See here for information on using those abstractions and a comparison with the methods demonstrated in this tutorial.\\nSuppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content.\\nLLMs are a great tool for this given their proficiency in understanding and synthesizing text.\\nIn the context of retrieval-augmented generation, summarizing text can help distill the information in a large number of retrieved documents to provide context for a LLM.\\nIn this walkthrough we\\'ll go over how to summarize content from multiple documents using LLMs.\\n\\nConcepts\\u200b\\nConcepts we will cover are:\\n\\n\\nUsing language models.\\n\\n\\nUsing document loaders, specifically the WebBaseLoader to load content from an HTML webpage.\\n\\n\\nTwo ways to summarize or otherwise combine documents.\\n\\nStuff, which simply concatenates documents into a prompt;\\nMap-reduce, for larger sets of documents. This splits documents into batches, summarizes those, and then summarizes the summaries.\\n\\n\\n\\nShorter, targeted guides on these strategies and others, including iterative refinement, can be found in the how-to guides.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nOverview\\u200b\\nA central question for building a summarizer is how to pass your documents into the LLM\\'s context window. Two common approaches for this are:\\n\\n\\nStuff: Simply \"stuff\" all your documents into a single prompt. This is the simplest approach (see here for more on the create_stuff_documents_chain constructor, which is used for this method).\\n\\n\\nMap-reduce: Summarize each document on its own in a \"map\" step and then \"reduce\" the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method).\\n\\n\\nNote that map-reduce is especially effective when understanding of a sub-document does not rely on preceding context. For example, when summarizing a corpus of many, shorter documents. In other cases, such as summarizing a novel or body of text with an inherent sequence, iterative refinement may be more effective.\\n\\nSetup\\u200b\\nFirst set environment variables and install packages:\\n%pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4 langchain-community# Set env var OPENAI_API_KEY or load from a .env file# import dotenv# dotenv.load_dotenv()\\nimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nFirst we load in our documents. We will use WebBaseLoader to load a blog post:\\nfrom langchain_community.document_loaders import WebBaseLoaderloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")docs = loader.load()\\nLet\\'s next select a LLM:\\n\\nSelect chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexityDeepSeekChatOCIGenAIpip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\\nStuff: summarize in a single LLM call\\u200b\\nWe can use create_stuff_documents_chain, especially if using larger context window models such as:\\n\\n128k token OpenAI gpt-4o\\n200k token Anthropic claude-3-5-sonnet-latest\\n\\nThe chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:\\nfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain.chains.llm import LLMChainfrom langchain_core.prompts import ChatPromptTemplate# Define promptprompt = ChatPromptTemplate.from_messages(    [(\"system\", \"Write a concise summary of the following:\\\\\\\\n\\\\\\\\n{context}\")])# Instantiate chainchain = create_stuff_documents_chain(llm, prompt)# Invoke chainresult = chain.invoke({\"context\": docs})print(result)API Reference:ChatPromptTemplate\\nThe article \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture that includes three main components: Planning, Memory, and Tool Use. 1. **Planning** involves task decomposition, where complex tasks are broken down into manageable subgoals, and self-reflection, allowing agents to learn from past actions to improve future performance. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for enhancing reasoning and planning.2. **Memory** is categorized into short-term and long-term memory, with mechanisms for fast retrieval using Maximum Inner Product Search (MIPS) algorithms. This allows agents to retain and recall information effectively.3. **Tool Use** enables agents to interact with external APIs and tools, enhancing their capabilities beyond the limitations of their training data. Examples include MRKL systems and frameworks like HuggingGPT, which facilitate task planning and execution.The article also addresses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes with case studies demonstrating the practical applications of these concepts in scientific discovery and interactive simulations. Overall, the article emphasizes the potential of LLMs as powerful problem solvers in autonomous agent systems.\\nStreaming\\u200b\\nNote that we can also stream the result token-by-token:\\nfor token in chain.stream({\"context\": docs}):    print(token, end=\"|\")\\n|The| article| \"|LL|M| Powered| Autonomous| Agents|\"| by| Lil|ian| W|eng| discusses| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| outlines| a| system| architecture| that| includes| three| main| components|:| Planning|,| Memory|,| and| Tool| Use|.| |1|.| **|Planning|**| involves| task| decomposition|,| where| complex| tasks| are| broken| down| into| manageable| sub|go|als|,| and| self|-ref|lection|,| allowing| agents| to| learn| from| past| actions| to| improve| future| performance|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| enhancing| reasoning| and| planning|.|2|.| **|Memory|**| is| categorized| into| short|-term| and| long|-term| memory|,| with| mechanisms| for| fast| retrieval| using| Maximum| Inner| Product| Search| (|M|IPS|)| algorithms|.| This| allows| agents| to| retain| and| recall| information| effectively|.|3|.| **|Tool| Use|**| emphasizes| the| integration| of| external| APIs| and| tools| to| extend| the| capabilities| of| L|LM|s|,| enabling| them| to| perform| tasks| beyond| their| inherent| limitations|.| Examples| include| MR|KL| systems| and| frameworks| like| Hug|ging|GPT|,| which| facilitate| task| planning| and| execution|.|The| article| also| addresses| challenges| such| as| finite| context| length|,| difficulties| in| long|-term| planning|,| and| the| reliability| of| natural| language| interfaces|.| It| concludes| with| case| studies| demonstrating| the| practical| applications| of| L|LM|-powered| agents| in| scientific| discovery| and| interactive| simulations|.| Overall|,| the| piece| illustrates| the| potential| of| L|LM|s| as| general| problem| sol|vers| and| their| evolving| role| in| autonomous| systems|.||\\nGo deeper\\u200b\\n\\nYou can easily customize the prompt.\\nYou can easily try different LLMs, (e.g., Claude) via the llm parameter.\\n\\nMap-Reduce: summarize long texts via parallelization\\u200b\\nLet\\'s unpack the map reduce approach. For this, we\\'ll first map each document to an individual summary using an LLM. Then we\\'ll reduce or consolidate those summaries into a single global summary.\\nNote that the map step is typically parallelized over the input documents.\\nLangGraph, built on top of langchain-core, supports map-reduce workflows and is well-suited to this problem:\\n\\nLangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;\\nLangGraph\\'s checkpointing supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.\\nThe LangGraph implementation is straightforward to modify and extend, as we will see below.\\n\\nMap\\u200b\\nLet\\'s first define the prompt associated with the map step. We can use the same summarization prompt as in the stuff approach, above:\\nfrom langchain_core.prompts import ChatPromptTemplatemap_prompt = ChatPromptTemplate.from_messages(    [(\"system\", \"Write a concise summary of the following:\\\\\\\\n\\\\\\\\n{context}\")])API Reference:ChatPromptTemplate\\nWe can also use the Prompt Hub to store and fetch prompts.\\nThis will work with your LangSmith API key.\\nFor example, see the map prompt here.\\nfrom langchain import hubmap_prompt = hub.pull(\"rlm/map-prompt\")\\nReduce\\u200b\\nWe also define a prompt that takes the document mapping results and reduces them into a single output.\\n# Also available via the hub: `hub.pull(\"rlm/reduce-prompt\")`reduce_template = \"\"\"The following is a set of summaries:{docs}Take these and distill it into a final, consolidated summaryof the main themes.\"\"\"reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\\nOrchestration via LangGraph\\u200b\\nBelow we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.\\nMap-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model\\'s context window size. Here we implement a recursive \"collapsing\" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.\\nFirst we chunk the blog post into smaller \"sub documents\" to be mapped:\\nfrom langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    chunk_size=1000, chunk_overlap=0)split_docs = text_splitter.split_documents(docs)print(f\"Generated {len(split_docs)} documents.\")\\nCreated a chunk of size 1003, which is longer than the specified 1000``````outputGenerated 14 documents.\\nNext, we define our graph. Note that we define an artificially low maximum token length of 1,000 tokens to illustrate the \"collapsing\" step.\\nimport operatorfrom typing import Annotated, List, Literal, TypedDictfrom langchain.chains.combine_documents.reduce import (    acollapse_docs,    split_list_of_docs,)from langchain_core.documents import Documentfrom langgraph.constants import Sendfrom langgraph.graph import END, START, StateGraphtoken_max = 1000def length_function(documents: List[Document]) -> int:    \"\"\"Get number of tokens for input contents.\"\"\"    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)# This will be the overall state of the main graph.# It will contain the input document contents, corresponding# summaries, and a final summary.class OverallState(TypedDict):    # Notice here we use the operator.add    # This is because we want combine all the summaries we generate    # from individual nodes back into one list - this is essentially    # the \"reduce\" part    contents: List[str]    summaries: Annotated[list, operator.add]    collapsed_summaries: List[Document]    final_summary: str# This will be the state of the node that we will \"map\" all# documents to in order to generate summariesclass SummaryState(TypedDict):    content: str# Here we generate a summary, given a documentasync def generate_summary(state: SummaryState):    prompt = map_prompt.invoke(state[\"content\"])    response = await llm.ainvoke(prompt)    return {\"summaries\": [response.content]}# Here we define the logic to map out over the documents# We will use this an edge in the graphdef map_summaries(state: OverallState):    # We will return a list of `Send` objects    # Each `Send` object consists of the name of a node in the graph    # as well as the state to send to that node    return [        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]    ]def collect_summaries(state: OverallState):    return {        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]    }async def _reduce(input: dict) -> str:    prompt = reduce_prompt.invoke(input)    response = await llm.ainvoke(prompt)    return response.content# Add node to collapse summariesasync def collapse_summaries(state: OverallState):    doc_lists = split_list_of_docs(        state[\"collapsed_summaries\"], length_function, token_max    )    results = []    for doc_list in doc_lists:        results.append(await acollapse_docs(doc_list, _reduce))    return {\"collapsed_summaries\": results}# This represents a conditional edge in the graph that determines# if we should collapse the summaries or notdef should_collapse(    state: OverallState,) -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:    num_tokens = length_function(state[\"collapsed_summaries\"])    if num_tokens > token_max:        return \"collapse_summaries\"    else:        return \"generate_final_summary\"# Here we will generate the final summaryasync def generate_final_summary(state: OverallState):    response = await _reduce(state[\"collapsed_summaries\"])    return {\"final_summary\": response}# Construct the graph# Nodes:graph = StateGraph(OverallState)graph.add_node(\"generate_summary\", generate_summary)  # same as beforegraph.add_node(\"collect_summaries\", collect_summaries)graph.add_node(\"collapse_summaries\", collapse_summaries)graph.add_node(\"generate_final_summary\", generate_final_summary)# Edges:graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])graph.add_edge(\"generate_summary\", \"collect_summaries\")graph.add_conditional_edges(\"collect_summaries\", should_collapse)graph.add_conditional_edges(\"collapse_summaries\", should_collapse)graph.add_edge(\"generate_final_summary\", END)app = graph.compile()API Reference:Document | Send | StateGraph\\nLangGraph allows the graph structure to be plotted to help visualize its function:\\nfrom IPython.display import ImageImage(app.get_graph().draw_mermaid_png())\\n\\nWhen running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.\\nNote that because we have a loop in the graph, it can be helpful to specify a recursion_limit on its execution. This will raise a specific error when the specified limit is exceeded.\\nasync for step in app.astream(    {\"contents\": [doc.page_content for doc in split_docs]},    {\"recursion_limit\": 10},):    print(list(step.keys()))\\n[\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'generate_summary\\'][\\'collect_summaries\\'][\\'collapse_summaries\\'][\\'collapse_summaries\\'][\\'generate_final_summary\\']\\nprint(step)\\n{\\'generate_final_summary\\': {\\'final_summary\\': \\'The consolidated summary of the main themes from the provided documents is as follows:\\\\n\\\\n1. **Integration of Large Language Models (LLMs) in Autonomous Agents**: The documents explore the evolving role of LLMs in autonomous systems, emphasizing their enhanced reasoning and acting capabilities through methodologies that incorporate structured planning, memory systems, and tool use.\\\\n\\\\n2. **Core Components of Autonomous Agents**:\\\\n   - **Planning**: Techniques like task decomposition (e.g., Chain of Thought) and external classical planners are utilized to facilitate long-term planning by breaking down complex tasks.\\\\n   - **Memory**: The memory system is divided into short-term (in-context learning) and long-term memory, with parallels drawn between human memory and machine learning to improve agent performance.\\\\n   - **Tool Use**: Agents utilize external APIs and algorithms to enhance problem-solving abilities, exemplified by frameworks like HuggingGPT that manage task workflows.\\\\n\\\\n3. **Neuro-Symbolic Architectures**: The integration of MRKL (Modular Reasoning, Knowledge, and Language) systems combines neural and symbolic expert modules with LLMs, addressing challenges in tasks such as verbal math problem-solving.\\\\n\\\\n4. **Specialized Applications**: Case studies, such as ChemCrow and projects in anticancer drug discovery, demonstrate the advantages of LLMs augmented with expert tools in specialized domains.\\\\n\\\\n5. **Challenges and Limitations**: The documents highlight challenges such as hallucination in model outputs and the finite context length of LLMs, which affects their ability to incorporate historical information and perform self-reflection. Techniques like Chain of Hindsight and Algorithm Distillation are discussed to enhance model performance through iterative learning.\\\\n\\\\n6. **Structured Software Development**: A systematic approach to creating Python software projects is emphasized, focusing on defining core components, managing dependencies, and adhering to best practices for documentation.\\\\n\\\\nOverall, the integration of structured planning, memory systems, and advanced tool use aims to enhance the capabilities of LLM-powered autonomous agents while addressing the challenges and limitations these technologies face in real-world applications.\\'}}\\nIn the corresponding LangSmith trace we can see the individual LLM calls, grouped under their respective nodes.\\nGo deeper\\u200b\\nCustomization\\n\\nAs shown above, you can customize the LLMs and prompts for map and reduce stages.\\n\\nReal-world use-case\\n\\nSee this blog post case-study on analyzing user interactions (questions about LangChain documentation)!\\nThe blog post and associated repo also introduce clustering as a means of summarization.\\nThis opens up another path beyond the stuff or map-reduce approaches that is worth considering.\\n\\n\\nNext steps\\u200b\\nWe encourage you to check out the how-to guides for more detail on:\\n\\nOther summarization strategies, such as iterative refinement\\nBuilt-in document loaders and text-splitters\\nIntegrating various combine-document chains into a RAG application\\nIncorporating retrieval into a chatbot\\n\\nand other concepts.Edit this pagePreviousBuild a Question/Answering system over SQL dataNextHow-to guidesConceptsSetupJupyter NotebookInstallationLangSmithOverviewSetupStuff: summarize in a single LLM callStreamingGo deeperMap-Reduce: summarize long texts via parallelizationMapReduceOrchestration via LangGraphGo deeperNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/graph/', 'title': 'Build a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain', 'description': \"In this guide we'll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer. First, we will show a simple out-of-the-box option and then implement a more sophisticated version with LangGraph.\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Question Answering application over a Graph Database | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow to deal with high-cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with LangChainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Question Answering application over a Graph DatabaseOn this pageBuild a Question Answering application over a Graph Database\\nIn this guide we\\'ll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer. First, we will show a simple out-of-the-box option and then implement a more sophisticated version with LangGraph.\\n‚ö†Ô∏è Security note ‚ö†Ô∏è\\u200b\\nBuilding Q&A systems of graph databases requires executing model-generated graph queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent\\'s needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see here.\\nArchitecture\\u200b\\nAt a high-level, the steps of most graph chains are:\\n\\nConvert question to a graph database query: Model converts user input to a graph database query (e.g. Cypher).\\nExecute graph database query: Execute the graph database query.\\nAnswer the question: Model responds to user input using the query results.\\n\\n\\nSetup\\u200b\\nFirst, get required packages and set environment variables.\\nIn this example, we will be using Neo4j graph database.\\n%pip install --upgrade --quiet langchain langchain-neo4j langchain-openai langgraph\\nWe default to OpenAI models in this guide.\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")# Uncomment the below to use LangSmith. Not required.# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nEnter your OpenAI API key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\\nNext, we need to define Neo4j credentials.\\nFollow these installation steps to set up a Neo4j database.\\nos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"os.environ[\"NEO4J_PASSWORD\"] = \"password\"\\nThe below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.\\nfrom langchain_neo4j import Neo4jGraphgraph = Neo4jGraph()# Import movie informationmovies_query = \"\"\"LOAD CSV WITH HEADERS FROM \\'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv\\'AS rowMERGE (m:Movie {id:row.movieId})SET m.released = date(row.released),    m.title = row.title,    m.imdbRating = toFloat(row.imdbRating)FOREACH (director in split(row.director, \\'|\\') |     MERGE (p:Person {name:trim(director)})    MERGE (p)-[:DIRECTED]->(m))FOREACH (actor in split(row.actors, \\'|\\') |     MERGE (p:Person {name:trim(actor)})    MERGE (p)-[:ACTED_IN]->(m))FOREACH (genre in split(row.genres, \\'|\\') |     MERGE (g:Genre {name:trim(genre)})    MERGE (m)-[:IN_GENRE]->(g))\"\"\"graph.query(movies_query)\\n[]\\nGraph schema\\u200b\\nIn order for an LLM to be able to generate a Cypher statement, it needs information about the graph schema. When you instantiate a graph object, it retrieves the information about the graph schema. If you later make any changes to the graph, you can run the refresh_schema method to refresh the schema information.\\ngraph.refresh_schema()print(graph.schema)\\nNode properties:Person {name: STRING}Movie {id: STRING, released: DATE, title: STRING, imdbRating: FLOAT}Genre {name: STRING}Chunk {id: STRING, embedding: LIST, text: STRING, question: STRING, query: STRING}Relationship properties:The relationships:(:Person)-[:DIRECTED]->(:Movie)(:Person)-[:ACTED_IN]->(:Movie)(:Movie)-[:IN_GENRE]->(:Genre)\\nFor more involved schema information, you can use enhanced_schema option.\\nenhanced_graph = Neo4jGraph(enhanced_schema=True)print(enhanced_graph.schema)\\nReceived notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. (\\'config\\' used by \\'apoc.meta.graphSample\\' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, \\'type\\'), count: apoc.any.property(rel, \\'count\\')}] AS relationships\"``````outputNode properties:- **Person**  - `name`: STRING Example: \"John Lasseter\"- **Movie**  - `id`: STRING Example: \"1\"  - `released`: DATE Min: 1964-12-16, Max: 1996-09-15  - `title`: STRING Example: \"Toy Story\"  - `imdbRating`: FLOAT Min: 2.4, Max: 9.3- **Genre**  - `name`: STRING Example: \"Adventure\"- **Chunk**  - `id`: STRING Available options: [\\'d66006059fd78d63f3df90cc1059639a\\', \\'0e3dcb4502853979d12357690a95ec17\\', \\'c438c6bcdcf8e4fab227f29f8e7ff204\\', \\'97fe701ec38057594464beaa2df0710e\\', \\'b54f9286e684373498c4504b4edd9910\\', \\'5b50a72c3a4954b0ff7a0421be4f99b9\\', \\'fb28d41771e717255f0d8f6c799ede32\\', \\'58e6f14dd2e6c6702cf333f2335c499c\\']  - `text`: STRING Available options: [\\'How many artists are there?\\', \\'Which actors played in the movie Casino?\\', \\'How many movies has Tom Hanks acted in?\\', \"List all the genres of the movie Schindler\\'s List\", \\'Which actors have worked in movies from both the c\\', \\'Which directors have made movies with at least thr\\', \\'Identify movies where directors also played a role\\', \\'Find the actor with the highest number of movies i\\']  - `question`: STRING Available options: [\\'How many artists are there?\\', \\'Which actors played in the movie Casino?\\', \\'How many movies has Tom Hanks acted in?\\', \"List all the genres of the movie Schindler\\'s List\", \\'Which actors have worked in movies from both the c\\', \\'Which directors have made movies with at least thr\\', \\'Identify movies where directors also played a role\\', \\'Find the actor with the highest number of movies i\\']  - `query`: STRING Available options: [\\'MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN coun\\', \"MATCH (m:Movie {title: \\'Casino\\'})<-[:ACTED_IN]-(a)\", \"MATCH (a:Person {name: \\'Tom Hanks\\'})-[:ACTED_IN]->\", \"MATCH (m:Movie {title: \\'Schindler\\'s List\\'})-[:IN_G\", \\'MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]\\', \\'MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_I\\', \\'MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACT\\', \\'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.na\\']Relationship properties:The relationships:(:Person)-[:DIRECTED]->(:Movie)(:Person)-[:ACTED_IN]->(:Movie)(:Movie)-[:IN_GENRE]->(:Genre)\\nThe enhanced_schema option enriches property information by including details such as minimum and maximum values for floats and dates, as well as example values for string properties. This additional context helps guide the LLM toward generating more accurate and effective queries.\\nGreat! We\\'ve got a graph database that we can query. Now let\\'s try hooking it up to an LLM.\\nGraphQACypherChain\\u200b\\nLet\\'s use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question.\\n\\nLangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: GraphCypherQAChain\\nfrom langchain_neo4j import GraphCypherQAChainfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)chain = GraphCypherQAChain.from_llm(    graph=enhanced_graph, llm=llm, verbose=True, allow_dangerous_requests=True)response = chain.invoke({\"query\": \"What was the cast of the Casino?\"})response\\n\\x1b[1m> Entering new GraphCypherQAChain chain...\\x1b[0mGenerated Cypher:\\x1b[32;1m\\x1b[1;3mcypherMATCH (p:Person)-[:ACTED_IN]->(m:Movie {title: \"Casino\"})RETURN p.name\\x1b[0mFull Context:\\x1b[32;1m\\x1b[1;3m[{\\'p.name\\': \\'Robert De Niro\\'}, {\\'p.name\\': \\'Joe Pesci\\'}, {\\'p.name\\': \\'Sharon Stone\\'}, {\\'p.name\\': \\'James Woods\\'}]\\x1b[0m\\x1b[1m> Finished chain.\\x1b[0m\\n{\\'query\\': \\'What was the cast of the Casino?\\', \\'result\\': \\'Robert De Niro, Joe Pesci, Sharon Stone, and James Woods were the cast of Casino.\\'}\\nAdvanced implementation with LangGraph\\u200b\\nWhile the GraphCypherQAChain is effective for quick demonstrations, it may face challenges in production environments. Transitioning to LangGraph can enhance the workflow, but implementing natural language to query flows in production remains a complex task. Nevertheless, there are several strategies to significantly improve accuracy and reliability, which we will explore next.\\nHere is the visualized LangGraph flow we will implement:\\n\\nWe will begin by defining the Input, Output, and Overall state of the LangGraph application.\\nfrom operator import addfrom typing import Annotated, Listfrom typing_extensions import TypedDictclass InputState(TypedDict):    question: strclass OverallState(TypedDict):    question: str    next_action: str    cypher_statement: str    cypher_errors: List[str]    database_records: List[dict]    steps: Annotated[List[str], add]class OutputState(TypedDict):    answer: str    steps: List[str]    cypher_statement: str\\nThe first step is a simple guardrails step, where we validate whether the question pertains to movies or their cast. If it doesn\\'t, we notify the user that we cannot answer any other questions. Otherwise, we move on to the Cypher generation step.\\nfrom typing import Literalfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldguardrails_system = \"\"\"As an intelligent assistant, your primary objective is to decide whether a given question is related to movies or not. If the question is related to movies, output \"movie\". Otherwise, output \"end\".To make this decision, assess the content of the question and determine if it refers to any movie, actor, director, film industry, or related topics. Provide only the specified output: \"movie\" or \"end\".\"\"\"guardrails_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            guardrails_system,        ),        (            \"human\",            (\"{question}\"),        ),    ])class GuardrailsOutput(BaseModel):    decision: Literal[\"movie\", \"end\"] = Field(        description=\"Decision on whether the question is related to movies\"    )guardrails_chain = guardrails_prompt | llm.with_structured_output(GuardrailsOutput)def guardrails(state: InputState) -> OverallState:    \"\"\"    Decides if the question is related to movies or not.    \"\"\"    guardrails_output = guardrails_chain.invoke({\"question\": state.get(\"question\")})    database_records = None    if guardrails_output.decision == \"end\":        database_records = \"This questions is not about movies or their cast. Therefore I cannot answer this question.\"    return {        \"next_action\": guardrails_output.decision,        \"database_records\": database_records,        \"steps\": [\"guardrail\"],    }API Reference:ChatPromptTemplate\\nFew-shot prompting\\u200b\\nConverting natural language into accurate queries is challenging. One way to enhance this process is by providing relevant few-shot examples to guide the LLM in query generation. To achieve this, we will use the SemanticSimilarityExampleSelector to dynamically select the most relevant examples.\\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_neo4j import Neo4jVectorfrom langchain_openai import OpenAIEmbeddingsexamples = [    {        \"question\": \"How many artists are there?\",        \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)\",    },    {        \"question\": \"Which actors played in the movie Casino?\",        \"query\": \"MATCH (m:Movie {title: \\'Casino\\'})<-[:ACTED_IN]-(a) RETURN a.name\",    },    {        \"question\": \"How many movies has Tom Hanks acted in?\",        \"query\": \"MATCH (a:Person {name: \\'Tom Hanks\\'})-[:ACTED_IN]->(m:Movie) RETURN count(m)\",    },    {        \"question\": \"List all the genres of the movie Schindler\\'s List\",        \"query\": \"MATCH (m:Movie {title: \\'Schindler\\'s List\\'})-[:IN_GENRE]->(g:Genre) RETURN g.name\",    },    {        \"question\": \"Which actors have worked in movies from both the comedy and action genres?\",        \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = \\'Comedy\\' AND g2.name = \\'Action\\' RETURN DISTINCT a.name\",    },    {        \"question\": \"Which directors have made movies with at least three different actors named \\'John\\'?\",        \"query\": \"MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH \\'John\\' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name\",    },    {        \"question\": \"Identify movies where directors also played a role in the film.\",        \"query\": \"MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACTED_IN]->(m) RETURN m.title, p.name\",    },    {        \"question\": \"Find the actor with the highest number of movies in the database.\",        \"query\": \"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1\",    },]example_selector = SemanticSimilarityExampleSelector.from_examples(    examples, OpenAIEmbeddings(), Neo4jVector, k=5, input_keys=[\"question\"])API Reference:SemanticSimilarityExampleSelector\\nNext, we implement the Cypher generation chain, also known as text2cypher. The prompt includes an enhanced graph schema, dynamically selected few-shot examples, and the user‚Äôs question. This combination enables the generation of a Cypher query to retrieve relevant information from the database.\\nfrom langchain_core.output_parsers import StrOutputParsertext2cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            (                \"Given an input question, convert it to a Cypher query. No pre-amble.\"                \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"            ),        ),        (            \"human\",            (                \"\"\"You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!Here is the schema information{schema}Below are a number of examples of questions and their corresponding Cypher queries.{fewshot_examples}User input: {question}Cypher query:\"\"\"            ),        ),    ])text2cypher_chain = text2cypher_prompt | llm | StrOutputParser()def generate_cypher(state: OverallState) -> OverallState:    \"\"\"    Generates a cypher statement based on the provided schema and user input    \"\"\"    NL = \"\\\\n\"    fewshot_examples = (NL * 2).join(        [            f\"Question: {el[\\'question\\']}{NL}Cypher:{el[\\'query\\']}\"            for el in example_selector.select_examples(                {\"question\": state.get(\"question\")}            )        ]    )    generated_cypher = text2cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"fewshot_examples\": fewshot_examples,            \"schema\": enhanced_graph.schema,        }    )    return {\"cypher_statement\": generated_cypher, \"steps\": [\"generate_cypher\"]}API Reference:StrOutputParser\\nQuery validation\\u200b\\nThe next step is to validate the generated Cypher statement and ensuring that all property values are accurate. While numbers and dates typically don‚Äôt require validation, strings such as movie titles or people‚Äôs names do. In this example, we‚Äôll use a basic CONTAINS clause for validation, though more advanced mapping and validation techniques can be implemented if needed.\\nFirst, we will create a chain that detects any errors in the Cypher statement and extracts the property values it references.\\nfrom typing import List, Optionalvalidate_cypher_system = \"\"\"You are a Cypher expert reviewing a statement written by a junior developer.\"\"\"validate_cypher_user = \"\"\"You must check the following:* Are there any syntax errors in the Cypher statement?* Are there any missing or undefined variables in the Cypher statement?* Are any node labels missing from the schema?* Are any relationship types missing from the schema?* Are any of the properties not included in the schema?* Does the Cypher statement include enough information to answer the question?Examples of good errors:* Label (:Foo) does not exist, did you mean (:Bar)?* Property bar does not exist for label Foo, did you mean baz?* Relationship FOO does not exist, did you mean FOO_BAR?Schema:{schema}The question is:{question}The Cypher statement is:{cypher}Make sure you don\\'t make any mistakes!\"\"\"validate_cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            validate_cypher_system,        ),        (            \"human\",            (validate_cypher_user),        ),    ])class Property(BaseModel):    \"\"\"    Represents a filter condition based on a specific node property in a graph in a Cypher statement.    \"\"\"    node_label: str = Field(        description=\"The label of the node to which this property belongs.\"    )    property_key: str = Field(description=\"The key of the property being filtered.\")    property_value: str = Field(        description=\"The value that the property is being matched against.\"    )class ValidateCypherOutput(BaseModel):    \"\"\"    Represents the validation result of a Cypher query\\'s output,    including any errors and applied filters.    \"\"\"    errors: Optional[List[str]] = Field(        description=\"A list of syntax or semantical errors in the Cypher statement. Always explain the discrepancy between schema and Cypher statement\"    )    filters: Optional[List[Property]] = Field(        description=\"A list of property-based filters applied in the Cypher statement.\"    )validate_cypher_chain = validate_cypher_prompt | llm.with_structured_output(    ValidateCypherOutput)\\nLLMs often struggle with correctly determining relationship directions in generated Cypher statements. Since we have access to the schema, we can deterministically correct these directions using the CypherQueryCorrector.\\nNote: The CypherQueryCorrector is an experimental feature and doesn\\'t support all the newest Cypher syntax.\\nfrom langchain_neo4j.chains.graph_qa.cypher_utils import CypherQueryCorrector, Schema# Cypher query corrector is experimentalcorrector_schema = [    Schema(el[\"start\"], el[\"type\"], el[\"end\"])    for el in enhanced_graph.structured_schema.get(\"relationships\")]cypher_query_corrector = CypherQueryCorrector(corrector_schema)\\nNow we can implement the Cypher validation step. First, we use the EXPLAIN method to detect any syntax errors. Next, we leverage the LLM to identify potential issues and extract the properties used for filtering. For string properties, we validate them against the database using a simple CONTAINS clause.\\nBased on the validation results, the process can take the following paths:\\n\\nIf value mapping fails, we end the conversation and inform the user that we couldn\\'t identify a specific property value (e.g., a person or movie title).\\nIf errors are found, we route the query for correction.\\nIf no issues are detected, we proceed to the Cypher execution step.\\n\\nfrom neo4j.exceptions import CypherSyntaxErrordef validate_cypher(state: OverallState) -> OverallState:    \"\"\"    Validates the Cypher statements and maps any property values to the database.    \"\"\"    errors = []    mapping_errors = []    # Check for syntax errors    try:        enhanced_graph.query(f\"EXPLAIN {state.get(\\'cypher_statement\\')}\")    except CypherSyntaxError as e:        errors.append(e.message)    # Experimental feature for correcting relationship directions    corrected_cypher = cypher_query_corrector(state.get(\"cypher_statement\"))    if not corrected_cypher:        errors.append(\"The generated Cypher statement doesn\\'t fit the graph schema\")    if not corrected_cypher == state.get(\"cypher_statement\"):        print(\"Relationship direction was corrected\")    # Use LLM to find additional potential errors and get the mapping for values    llm_output = validate_cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"schema\": enhanced_graph.schema,            \"cypher\": state.get(\"cypher_statement\"),        }    )    if llm_output.errors:        errors.extend(llm_output.errors)    if llm_output.filters:        for filter in llm_output.filters:            # Do mapping only for string values            if (                not [                    prop                    for prop in enhanced_graph.structured_schema[\"node_props\"][                        filter.node_label                    ]                    if prop[\"property\"] == filter.property_key                ][0][\"type\"]                == \"STRING\"            ):                continue            mapping = enhanced_graph.query(                f\"MATCH (n:{filter.node_label}) WHERE toLower(n.`{filter.property_key}`) = toLower($value) RETURN \\'yes\\' LIMIT 1\",                {\"value\": filter.property_value},            )            if not mapping:                print(                    f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\"                )                mapping_errors.append(                    f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\"                )    if mapping_errors:        next_action = \"end\"    elif errors:        next_action = \"correct_cypher\"    else:        next_action = \"execute_cypher\"    return {        \"next_action\": next_action,        \"cypher_statement\": corrected_cypher,        \"cypher_errors\": errors,        \"steps\": [\"validate_cypher\"],    }\\nThe Cypher correction step takes the existing Cypher statement, any identified errors, and the original question to generate a corrected version of the query.\\ncorrect_cypher_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            (                \"You are a Cypher expert reviewing a statement written by a junior developer. \"                \"You need to correct the Cypher statement based on the provided errors. No pre-amble.\"                \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"            ),        ),        (            \"human\",            (                \"\"\"Check for invalid syntax or semantics and return a corrected Cypher statement.Schema:{schema}Note: Do not include any explanations or apologies in your responses.Do not wrap the response in any backticks or anything else.Respond with a Cypher statement only!Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.The question is:{question}The Cypher statement is:{cypher}The errors are:{errors}Corrected Cypher statement: \"\"\"            ),        ),    ])correct_cypher_chain = correct_cypher_prompt | llm | StrOutputParser()def correct_cypher(state: OverallState) -> OverallState:    \"\"\"    Correct the Cypher statement based on the provided errors.    \"\"\"    corrected_cypher = correct_cypher_chain.invoke(        {            \"question\": state.get(\"question\"),            \"errors\": state.get(\"cypher_errors\"),            \"cypher\": state.get(\"cypher_statement\"),            \"schema\": enhanced_graph.schema,        }    )    return {        \"next_action\": \"validate_cypher\",        \"cypher_statement\": corrected_cypher,        \"steps\": [\"correct_cypher\"],    }\\nWe need to add a step that executes the given Cypher statement. If no results are returned, we should explicitly handle this scenario, as leaving the context empty can sometimes lead to LLM hallucinations.\\nno_results = \"I couldn\\'t find any relevant information in the database\"def execute_cypher(state: OverallState) -> OverallState:    \"\"\"    Executes the given Cypher statement.    \"\"\"    records = enhanced_graph.query(state.get(\"cypher_statement\"))    return {        \"database_records\": records if records else no_results,        \"next_action\": \"end\",        \"steps\": [\"execute_cypher\"],    }\\nThe final step is to generate the answer. This involves combining the initial question with the database output to produce a relevant response.\\ngenerate_final_prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant\",        ),        (            \"human\",            (                \"\"\"Use the following results retrieved from a database to providea succinct, definitive answer to the user\\'s question.Respond as if you are answering the question directly.Results: {results}Question: {question}\"\"\"            ),        ),    ])generate_final_chain = generate_final_prompt | llm | StrOutputParser()def generate_final_answer(state: OverallState) -> OutputState:    \"\"\"    Decides if the question is related to movies.    \"\"\"    final_answer = generate_final_chain.invoke(        {\"question\": state.get(\"question\"), \"results\": state.get(\"database_records\")}    )    return {\"answer\": final_answer, \"steps\": [\"generate_final_answer\"]}\\nNext, we will implement the LangGraph workflow, starting with defining the conditional edge functions.\\ndef guardrails_condition(    state: OverallState,) -> Literal[\"generate_cypher\", \"generate_final_answer\"]:    if state.get(\"next_action\") == \"end\":        return \"generate_final_answer\"    elif state.get(\"next_action\") == \"movie\":        return \"generate_cypher\"def validate_cypher_condition(    state: OverallState,) -> Literal[\"generate_final_answer\", \"correct_cypher\", \"execute_cypher\"]:    if state.get(\"next_action\") == \"end\":        return \"generate_final_answer\"    elif state.get(\"next_action\") == \"correct_cypher\":        return \"correct_cypher\"    elif state.get(\"next_action\") == \"execute_cypher\":        return \"execute_cypher\"\\nLet\\'s put it all together now.\\nfrom IPython.display import Image, displayfrom langgraph.graph import END, START, StateGraphlanggraph = StateGraph(OverallState, input=InputState, output=OutputState)langgraph.add_node(guardrails)langgraph.add_node(generate_cypher)langgraph.add_node(validate_cypher)langgraph.add_node(correct_cypher)langgraph.add_node(execute_cypher)langgraph.add_node(generate_final_answer)langgraph.add_edge(START, \"guardrails\")langgraph.add_conditional_edges(    \"guardrails\",    guardrails_condition,)langgraph.add_edge(\"generate_cypher\", \"validate_cypher\")langgraph.add_conditional_edges(    \"validate_cypher\",    validate_cypher_condition,)langgraph.add_edge(\"execute_cypher\", \"generate_final_answer\")langgraph.add_edge(\"correct_cypher\", \"validate_cypher\")langgraph.add_edge(\"generate_final_answer\", END)langgraph = langgraph.compile()# Viewdisplay(Image(langgraph.get_graph().draw_mermaid_png()))API Reference:StateGraph\\n\\nWe can now test the application by asking an irrelevant question.\\nlanggraph.invoke({\"question\": \"What\\'s the weather in Spain?\"})\\n{\\'answer\\': \"I\\'m sorry, but I cannot provide current weather information. Please check a reliable weather website or app for the latest updates on the weather in Spain.\", \\'steps\\': [\\'guardrail\\', \\'generate_final_answer\\']}\\nLet\\'s now ask something relevant about the movies.\\nlanggraph.invoke({\"question\": \"What was the cast of the Casino?\"})\\n{\\'answer\\': \\'The cast of \"Casino\" includes Robert De Niro, Joe Pesci, Sharon Stone, and James Woods.\\', \\'steps\\': [\\'guardrail\\',  \\'generate_cypher\\',  \\'validate_cypher\\',  \\'execute_cypher\\',  \\'generate_final_answer\\'], \\'cypher_statement\\': \"MATCH (m:Movie {title: \\'Casino\\'})<-[:ACTED_IN]-(a:Person) RETURN a.name\"}\\nNext steps\\u200b\\nFor other graph techniques like this and more check out:\\n\\nSemantic layer: Techniques for implementing semantic layers.\\nConstructing graphs: Techniques for constructing knowledge graphs.\\nEdit this pagePreviousTutorialsNextTutorials‚ö†Ô∏è Security note ‚ö†Ô∏èArchitectureSetupGraph schemaGraphQACypherChainAdvanced implementation with LangGraphFew-shot promptingQuery validationNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_langgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ae86bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list_langgraph=[item for sublist in docs_langgraph for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66c1fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_split_langgraph=text_spitter.split_documents(doc_list_langgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fa0e08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_split_langgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e5b47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_langgraph=FAISS.from_documents(\n",
    "    documents=doc_split_langchain,\n",
    "    embedding=embeddings\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa00da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "langgraph_retriever=vector_store_langgraph.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0921c1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='9aacbcfc-b513-4949-aa86-62627bab6121', metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Automatic support for tracing via LangSmith and deployments via LangGraph Platform;\\nSupport for persistence, human-in-the-loop, and other features.\\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in Part 2 of the tutorial, LangGraph\\'s management and persistence of state simplifies these applications enormously.\\nUsage\\u200b\\nLet\\'s test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\\nInvoke:\\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f\"Context: {result[\\'context\\']}\\\\n\\\\n\")print(f\"Answer: {result[\\'answer\\']}\")'),\n",
       " Document(id='00d0cf02-6c77-485b-b486-87a70f0f6dcc', metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:question = \"...\"retrieved_docs = vector_store.similarity_search(question)docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)prompt = prompt.invoke({\"question\": question, \"context\": docs_content})answer = llm.invoke(prompt)The benefits of LangGraph include:\\nSupport for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\\nAutomatic support for tracing via LangSmith and deployments via LangGraph Platform;\\nSupport for persistence, human-in-the-loop, and other features.'),\n",
       " Document(id='e954abcc-2a51-4154-87c2-e23455a2f0ed', metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='We can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.\\nWe get streamlined deployments via LangGraph Platform.\\nLangSmith will automatically trace the steps of our application together.\\nWe can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes.\\n\\nTo use LangGraph, we need to define three things:\\n\\nThe state of our application;\\nThe nodes of our application (i.e., application steps);\\nThe \"control flow\" of our application (e.g., the ordering of the steps).'),\n",
       " Document(id='565af78f-2d1d-4e8f-8111-e5340f571349', metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Tutorials | ü¶úÔ∏èüîó LangChain', 'description': 'New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.', 'language': 'en'}, page_content='LangSmith\\u200b\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse LangSmith tutorials here.\\nEvaluation\\u200b\\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\\n\\nEvaluate your LLM application\\nEdit this pagePreviousIntroductionNextBuild a Question Answering application over a Graph DatabaseGet startedOrchestrationLangSmithEvaluationCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langgraph_retriever.invoke('what is langgraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "509fbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retriever To Retriever Tools\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool_langgraph=create_retriever_tool(\n",
    "    langgraph_retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search and run information about Langgraph\"\n",
    ")\n",
    "\n",
    "retriever_tool_langchain=create_retriever_tool(\n",
    "    langchain_retriever,\n",
    "    \"retriever_vector_langchain_blog\",\n",
    "    \"Search and run information about Langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b82a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool_langchain,retriever_tool_langgraph]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0583b5",
   "metadata": {},
   "source": [
    "# Langgraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cae06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state\n",
    "\n",
    "from typing import TypedDict,Literal,Sequence,Annotated\n",
    "from langchain_core.messages import BaseMessage,SystemMessage,AIMessage,HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5c2fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model='llama-3.1-8b-instant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c44c3bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state:State)->State:\n",
    "    ''' Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "        '''\n",
    "\n",
    "    print('---Agent Call---')\n",
    "    model=llm.bind_tools(tools)\n",
    "    message=state['messages']\n",
    "    response=model.invoke(message)\n",
    "    return {\n",
    "        'messages':[response]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "095eb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data model for grading\n",
    "from pydantic import Field,BaseModel\n",
    "class grade(BaseModel):\n",
    "    binary_score:Literal['yes','no']=Field(description='Score yes or no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7781102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_yes_no=llm.with_structured_output(grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2239ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "def classify_documents(state:State)->Literal[\"generate\", \"rewrite\"]:\n",
    "    '''\n",
    "    Determine wheter the document is relavant or not\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not'''\n",
    "    \n",
    "    print('---Check Relavane---')\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    #chain\n",
    "    chain=prompt | llm_yes_no\n",
    "\n",
    "    messages = state[\"messages\"] # get messages\n",
    "    last_message = messages[-1] # get last message\n",
    "\n",
    "    question = messages[0].content # get user question\n",
    "    docs = last_message.content # get docs\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "    score = scored_result.binary_score\n",
    "    \n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0a65590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fcb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    print(prompt)\n",
    "    \n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "136ac55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "403e898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node('agent',agent)\n",
    "workflow.add_node('retrieve',ToolNode(tools))\n",
    "workflow.add_node('generate',generate)\n",
    "workflow.add_node('rewrite',rewrite)\n",
    "\n",
    "workflow.add_edge(START,'agent')\n",
    "workflow.add_conditional_edges(\n",
    "    'agent',\n",
    "    tools_condition,\n",
    "    {\n",
    "        'tools':'retrieve',\n",
    "        END:END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    'retrieve',\n",
    "    classify_documents\n",
    ")\n",
    "workflow.add_edge('generate',END)\n",
    "workflow.add_edge('rewrite','agent')\n",
    "\n",
    "graph=workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ddbbfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000270F0476BA0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee15e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Agent Call---\n",
      "---Check Relavane---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n",
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is langgraph', additional_kwargs={}, response_metadata={}, id='ab381ae5-0581-4a88-aed1-fae5a629c3d6'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'fadg9h1bb', 'function': {'arguments': '{\"query\":\"langgraph\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 309, 'total_tokens': 328, 'completion_time': 0.016815311, 'prompt_time': 0.018632726, 'queue_time': 0.054493503, 'total_time': 0.035448037}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c54ae48ece', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--72655dbf-8b84-4596-8059-c459f3644bb6-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'langgraph'}, 'id': 'fadg9h1bb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 309, 'output_tokens': 19, 'total_tokens': 328}),\n",
       "  ToolMessage(content='LangGraph allows the graph structure to be plotted to help visualize its function:\\nfrom IPython.display import ImageImage(app.get_graph().draw_mermaid_png())\\n\\nSee this LangGraph guide for more detail and examples.\\nNext steps\\u200b\\nFor more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:\\n\\nAdvanced implementation with LangGraph\\u200b\\nWhile the GraphCypherQAChain is effective for quick demonstrations, it may face challenges in production environments. Transitioning to LangGraph can enhance the workflow, but implementing natural language to query flows in production remains a complex task. Nevertheless, there are several strategies to significantly improve accuracy and reliability, which we will explore next.\\nHere is the visualized LangGraph flow we will implement:\\n\\nSemantic layer: Techniques for implementing semantic layers.\\nConstructing graphs: Techniques for constructing knowledge graphs.\\nEdit this pagePreviousTutorialsNextTutorials‚ö†Ô∏è Security note ‚ö†Ô∏èArchitectureSetupGraph schemaGraphQACypherChainAdvanced implementation with LangGraphFew-shot promptingQuery validationNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.', name='retriever_vector_db_blog', id='7f773de6-4503-43f6-8901-fac8893f3437', tool_call_id='fadg9h1bb'),\n",
       "  HumanMessage(content='LangGraph is a tool used to plot the graph structure and help visualize its function. It can be used to create complex query-generation and query-checking steps. LangGraph is mentioned as an alternative to GraphCypherQAChain for more complex tasks in production environments.', additional_kwargs={}, response_metadata={}, id='08229a10-1136-4b56-9d99-9d08b8ab9837')]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\n",
    "    'messages':'what is langgraph'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff05f1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Agent Call---\n",
      "---Check Relavane---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---Agent Call---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is MAchine learning', additional_kwargs={}, response_metadata={}, id='80fab887-5f28-4911-8dca-1b8705775677'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '2pd42y8c8', 'function': {'arguments': '{\"query\":\"Machine learning definition\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 310, 'total_tokens': 331, 'completion_time': 0.03468298, 'prompt_time': 0.018918745, 'queue_time': 0.413441038, 'total_time': 0.053601725}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_8a13dfab2b', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c3f510d1-7215-4752-8574-b06bbbe9c0b1-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'Machine learning definition'}, 'id': '2pd42y8c8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 310, 'output_tokens': 21, 'total_tokens': 331}),\n",
       "  ToolMessage(content='interact with external APIs and tools, enhancing their capabilities beyond the limitations of their training data. Examples include MRKL systems and frameworks like HuggingGPT, which facilitate task planning and execution.The article also addresses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes with case studies demonstrating the practical applications of these concepts in scientific discovery and interactive simulations. Overall, the article emphasizes the potential of LLMs as powerful problem solvers in autonomous agent systems.\\n\\nThe article \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture that includes three main components: Planning, Memory, and Tool Use. 1. **Planning** involves task decomposition, where complex tasks are broken down into manageable subgoals, and self-reflection, allowing agents to learn from past actions to improve future performance. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for enhancing reasoning and planning.2. **Memory** is categorized into short-term and long-term memory, with mechanisms for fast retrieval using Maximum Inner Product Search (MIPS) algorithms. This allows agents to retain and recall information effectively.3. **Tool Use** enables agents to interact with external APIs and tools, enhancing their capabilities beyond the limitations of their training data. Examples include MRKL systems and\\n\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis is a multi-part tutorial:\\n\\n|The| article| \"|LL|M| Powered| Autonomous| Agents|\"| by| Lil|ian| W|eng| discusses| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| outlines| a| system| architecture| that| includes| three| main| components|:| Planning|,| Memory|,| and| Tool| Use|.| |1|.| **|Planning|**| involves| task| decomposition|,| where| complex| tasks| are| broken| down| into| manageable| sub|go|als|,| and| self|-ref|lection|,| allowing| agents| to| learn| from| past| actions| to| improve| future| performance|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| enhancing| reasoning| and| planning|.|2|.| **|Memory|**| is| categorized| into| short|-term| and| long|-term| memory|,| with| mechanisms| for| fast| retrieval| using| Maximum| Inner| Product| Search| (|M|IPS|)| algorithms|.| This| allows| agents| to| retain| and| recall| information| effectively|.|3|.| **|Tool| Use|**| emphasizes|', name='retriever_vector_langchain_blog', id='16d308bd-22f0-46af-9281-1dae44c02a6f', tool_call_id='2pd42y8c8'),\n",
       "  AIMessage(content='Based on the initial question, I would formulate an improved question as follows:\\n\\n\"What are the fundamental concepts, key techniques, and applications of Machine Learning, a subfield of Artificial Intelligence that enables systems to learn from data and make predictions or decisions without being explicitly programmed?\"\\n\\nThis improved question:\\n\\n1. Clarifies the scope and focus of the question\\n2. Identifies the key area of interest (Machine Learning)\\n3. Specifies the broader context (Artificial Intelligence)\\n4. Highlights the core capabilities and goals of Machine Learning (learning from data and making predictions or decisions)\\n\\nThis improved question allows for a more nuanced and informed response that addresses the underlying semantic intent and meaning of the initial query.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 139, 'prompt_tokens': 81, 'total_tokens': 220, 'completion_time': 0.231119618, 'prompt_time': 0.020269567, 'queue_time': 0.210388627, 'total_time': 0.251389185}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c54ae48ece', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--834022f2-b56b-4164-bee5-845c34cc82e3-0', usage_metadata={'input_tokens': 81, 'output_tokens': 139, 'total_tokens': 220}),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 1140, 'total_tokens': 1141, 'completion_time': 0.003668268, 'prompt_time': 0.104758801, 'queue_time': None, 'total_time': 0.108427069}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_8804b970d6', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--275eadcb-a4e9-46c5-af21-d062ae668fb3-0', usage_metadata={'input_tokens': 1140, 'output_tokens': 1, 'total_tokens': 1141})]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\n",
    "    'messages':'what is MAchine learning'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7419fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
